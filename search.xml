<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Bert模型MRPC任务从本地到云端部署踩坑记录</title>
      <link href="/2023/02/13/Bert/"/>
      <url>/2023/02/13/Bert/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><h2 id="1-项目的最开始（踩坑）"><a href="#1-项目的最开始（踩坑）" class="headerlink" title="1.项目的最开始（踩坑）**"></a>1.项目的最开始（踩坑）**</h2><p>研究路线：<a href="https://github.com/macanv/BERT-BiLSTM-CRF-NER">macanv&#x2F;BERT-BiLSTM-CRF-NER：NER 任务的张量流解决方案 使用 BiLSTM-CRF 模型与 Google BERT 微调和私有服务器服务 (github.com)</a></p><p>Paper with code网站，查找部署数据集的方法</p><p> 有一篇London University的论文可供参考</p><p>使用conda创建新环境</p><p>conda create –name BERT python&#x3D;3.8</p><p>Cd D:\code\BERT-BiLSTM-CRF-NER-master</p><p>D:python setup.py install</p><p><img src="https://alan-obs.obs.cn-south-1.myhuaweicloud.com/picgo/wps1.jpg?AccessKeyId=8OZWHBFPIQNNBK85CIY5&Expires=1676306131&Signature=NpUg5Wd0kiMUEfYD0Z0upm2WBxI="></p><p>尝试输入：bert-base-ner-train -help</p><p>结果报错：</p><p>2023-02-08 23:49:08.886024: W tensorflow&#x2F;stream_executor&#x2F;platform&#x2F;default&#x2F;dso_loader.cc:55] Could not load dynamic library ‘cudart64_101.dll’; dlerror: cudart64_101.dll not found</p><p>2023-02-08 23:49:08.886177: I tensorflow&#x2F;stream_executor&#x2F;cuda&#x2F;cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.</p><p><img src="https://alan-obs.obs.cn-south-1.myhuaweicloud.com/picgo/wps2-16762170464621.jpg?AccessKeyId=8OZWHBFPIQNNBK85CIY5&Expires=1676306180&Signature=trItdvFCONtZIdW0EWJziPUDU28="></p><p>于是参考：<a href="https://blog.csdn.net/weixin_43786241/article/details/109203995">(3条消息) 解决：Could not load dynamic library ‘cudart64_101.dll‘； dlerror: cudart64_101.dll not found_狂奔的菜鸡的博客-CSDN博客</a></p><h2 id=""><a href="#" class="headerlink" title=""></a></h2><p>将<em><strong>*CUDART64_101.DLL*</strong></em><em><strong>*放到我们的C:\Windows\System32中即可。然后第一个报错解决了。*</strong></em></p><p><em><strong>*这时又有了第二个错误：*</strong></em></p><p> <img src="https://alan-obs.obs.cn-south-1.myhuaweicloud.com/picgo/wps3-16762170464632.jpg?AccessKeyId=8OZWHBFPIQNNBK85CIY5&Expires=1676306256&Signature=PLE8FXzrnnfx2qyZHFMpIKP3lhE="></p><p>ModuleNotFoundError: No module named ‘google.protobuf’</p><p>参考<a href="https://blog.csdn.net/weixin_45753881/article/details/114314802">(3条消息) ModuleNotFoundError: No module named google.protobuf 解决方法_就是叫这个名字的博客-CSDN博客</a></p><p>使用pip install protobuf</p><p> <img src="https://alan-obs.obs.cn-south-1.myhuaweicloud.com/picgo/wps4-16762170464633.jpg?AccessKeyId=8OZWHBFPIQNNBK85CIY5&Expires=1676306291&Signature=/PEUAat7LkArBhmx9Y5JOtYkQ2Y="></p><p>继续报错</p><p>TypeError: Descriptors cannot not be created directly.</p><p>If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc &gt;&#x3D; 3.19.0.</p><p>If you cannot immediately regenerate your protos, some other possible workarounds are:</p><p> \1. Downgrade the protobuf package to 3.20.x or lower.</p><p> \2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION&#x3D;python (but this will use pure-Python parsing and will be much slower).</p><p> <img src="https://alan-obs.obs.cn-south-1.myhuaweicloud.com/picgo/wps5-16762170464636.jpg?AccessKeyId=8OZWHBFPIQNNBK85CIY5&Expires=1676306312&Signature=uR42/BeyGDw2f66XeLiYxmlj1E4="></p><p>解决方法：<a href="https://blog.csdn.net/qq_42951560/article/details/124997453">(3条消息) TypeError: Descriptors cannot not be created directly. If this call came from a _pb2.py file……_Xavier Jiezou的博客-CSDN博客</a></p><p>pip uninstall protobuf</p><p>pip install protobuf&#x3D;&#x3D;3.20.1</p><p>这时候又出现新报错了</p><p> <img src="https://alan-obs.obs.cn-south-1.myhuaweicloud.com/picgo/wps6-16762170464635.jpg?AccessKeyId=8OZWHBFPIQNNBK85CIY5&Expires=1676306335&Signature=v1/u/3sINwAG4VZJJOGhHCEgMpw="></p><p>ModuleNotFoundError: No module named ‘absl’</p><p>解决：<a href="https://blog.csdn.net/m13526413031/article/details/116892836">(3条消息) ModuleNotFoundError: No module named ‘absl‘_糖尛果的博客-CSDN博客</a></p><p>pip install absl-py</p><p><img src="https://alan-obs.obs.cn-south-1.myhuaweicloud.com/picgo/wps7-16762170464637.jpg?AccessKeyId=8OZWHBFPIQNNBK85CIY5&Expires=1676306362&Signature=Dd04/2LnlaohrDDIlmgLuhN2TyA="></p><p>报错：ModuleNotFoundError: No module named ‘wrapt’</p><p>参考：[(3条消息) No module named ‘wrapt‘错误解决方法_三角粽的博客-CSDN博客](<a href="https://blog.csdn.net/weixin_47594795/article/details/117284425?ops_request_misc=%7B&quot;request_id&quot;:&quot;167587238416800180646569&quot;,&quot;scm&quot;:&quot;20140713.130102334..&quot;%7D&amp;request_id=167587238416800180646569&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-1-117284425-null-null.142%5Ev73%5Einsert_down3,201%5Ev4%5Eadd_ask,239%5Ev1%5Econtrol&amp;utm_term=ModuleNotFoundError">https://blog.csdn.net/weixin_47594795/article/details/117284425?ops_request_misc={&quot;request_id&quot;%3A&quot;167587238416800180646569&quot;%2C&quot;scm&quot;%3A&quot;20140713.130102334..&quot;}&amp;request_id=167587238416800180646569&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-1-117284425-null-null.142^v73^insert_down3,201^v4^add_ask,239^v1^control&amp;utm_term=ModuleNotFoundError</a>: No module named wrapt&amp;spm&#x3D;1018.2226.3001.4187)</p><p>pip install wrapt</p><p> <img src="https://alan-obs.obs.cn-south-1.myhuaweicloud.com/picgo/wps8-16762170464634.jpg?AccessKeyId=8OZWHBFPIQNNBK85CIY5&Expires=1676306440&Signature=dELoNgd32oWEYl+MZcoxjcoeUYA="></p><p>出现新报错：AttributeError: module ‘numpy’ has no attribute ‘object’.</p><p><a href="https://alan-obs.obs.cn-south-1.myhuaweicloud.com/picgo/wps9-167621704646311.jpg?AccessKeyId=8OZWHBFPIQNNBK85CIY5&amp;Expires=1676306463&amp;Signature=j2zqmmQQkutYHUUtbI+jhWY2U30=">https://alan-obs.obs.cn-south-1.myhuaweicloud.com:443/picgo/wps9-167621704646311.jpg?AccessKeyId=8OZWHBFPIQNNBK85CIY5&amp;Expires=1676306463&amp;Signature=j2zqmmQQkutYHUUtbI%2BjhWY2U30%3D</a> </p><p>解决方案：如下</p><p>ERROR: pip’s dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.</p><p>tensorflow-gpu 2.2.0 requires astunparse&#x3D;&#x3D;1.6.3, which is not installed.</p><p>tensorflow-gpu 2.2.0 requires gast&#x3D;&#x3D;0.3.3, which is not installed.</p><p>tensorflow-gpu 2.2.0 requires google-pasta&gt;&#x3D;0.1.8, which is not installed.</p><p>tensorflow-gpu 2.2.0 requires grpcio&gt;&#x3D;1.8.6, which is not installed.</p><p>tensorflow-gpu 2.2.0 requires h5py&lt;2.11.0,&gt;&#x3D;2.10.0, which is not installed.</p><p>tensorflow-gpu 2.2.0 requires keras-preprocessing&gt;&#x3D;1.1.0, which is not installed.</p><p>tensorflow-gpu 2.2.0 requires opt-einsum&gt;&#x3D;2.3.2, which is not installed.</p><p>tensorflow-gpu 2.2.0 requires scipy&#x3D;&#x3D;1.4.1; python_version &gt;&#x3D; “3”, which is not installed.</p><p>tensorflow-gpu 2.2.0 requires tensorflow-gpu-estimator&lt;2.3.0,&gt;&#x3D;2.2.0, which is not installed.</p><p>tensorboard 2.2.2 requires google-auth-oauthlib&lt;0.5,&gt;&#x3D;0.4.1, which is not installed.</p><p>tensorboard 2.2.2 requires grpcio&gt;&#x3D;1.24.3, which is not installed.</p><p>tensorboard 2.2.2 requires markdown&gt;&#x3D;2.6.8, which is not installed.</p><p>tensorboard 2.2.2 requires requests&lt;3,&gt;&#x3D;2.21.0, which is not installed.</p><p>tensorboard 2.2.2 requires tensorboard-plugin-wit&gt;&#x3D;1.6.0, which is not installed.</p><p>tensorboard 2.2.2 requires werkzeug&gt;&#x3D;0.11.15, which is not installed.</p><p><a href="https://blog.csdn.net/can903154417/article/details/120254479">(2条消息) tensorflow和numpy 兼容版本_can903154417的博客-CSDN博客_tensorflow和numpy对应版本</a></p><p> pip intall numpy&#x3D;&#x3D;1.19.5</p><p>tensorflow-gpu 2.2.0 requires h5py&lt;2.11.0,&gt;&#x3D;2.10.0, which is not installed.</p><p>tensorflow-gpu 2.2.0 requires keras-preprocessing&gt;&#x3D;1.1.0, which is not installed.</p><p>tensorflow-gpu 2.2.0 requires opt-einsum&gt;&#x3D;2.3.2, which is not installed.</p><p>tensorflow-gpu 2.2.0 requires scipy&#x3D;&#x3D;1.4.1; python_version &gt;&#x3D; “3”, which is not installed.</p><p>tensorflow-gpu 2.2.0 requires tensorflow-gpu-estimator&lt;2.3.0,&gt;&#x3D;2.2.0, which is not installed.</p><p>2023-02-10 00:16:08.839501: I tensorflow&#x2F;stream_executor&#x2F;platform&#x2F;default&#x2F;dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll</p><p>Traceback (most recent call last):</p><p> File “C:\Users\14508.conda\envs\BERT\Scripts\bert-base-ner-train-script.py”, line 33, in <module></p><p>  sys.exit(load_entry_point(‘bert-base&#x3D;&#x3D;0.0.9’, ‘console_scripts’, ‘bert-base-ner-train’)())</p><p> File “C:\Users\14508.conda\envs\BERT\lib\site-packages\bert_base-0.0.9-py3.8.egg\bert_base\runs_<em>init</em>_.py”, line 29, in train_ner</p><p>  from bert_base.train.bert_lstm_ner import train</p><p> File “C:\Users\14508.conda\envs\BERT\lib\site-packages\bert_base-0.0.9-py3.8.egg\bert_base\train\bert_lstm_ner.py”, line 24, in <module></p><p>  from bert_base.bert import optimization</p><p> File “C:\Users\14508.conda\envs\BERT\lib\site-packages\bert_base-0.0.9-py3.8.egg\bert_base\bert\optimization.py”, line 84, in <module></p><p>  class AdamWeightDecayOptimizer(tf.train.Optimizer):</p><p>AttributeError: module ‘tensorflow._api.v2.train’ has no attribute ‘Optimizer’</p><p><em><strong>*解决方案：等待解决</strong></em>*</p><p>对于报错：AttributeError: module ‘tensorflow._api.v2.train’ has no attribute ‘Optimizer’</p><p>发现原来是TensorFlow版本装高了，于是</p><p><a href="https://stackoverflow.com/questions/61250311/error-importing-bert-module-tensorflow-api-v2-train-has-no-attribute-optimi">python - 导入 BERT 时出错：模块“tensorflow._api.v2.train”没有属性“优化器” - 堆栈溢出 (stackoverflow.com)</a></p><p>pip install tensorflow-gpu&#x3D;&#x3D;1.15.0</p><p>发现py38无法安装这个TensorFlow-gpu&#x3D;&#x3D;1.15.0</p><p>则采取“极端”做法—–卸载之前的环境并安装第二个cuda</p><p><em><strong>*con*</strong></em><em><strong>*da remove*</strong></em> -n BERT –all</p><p>安装CUDA10以及CUDNN-7.4</p><p> <img src="https://alan-obs.obs.cn-south-1.myhuaweicloud.com/picgo/wps10-167621704646310.jpg?AccessKeyId=8OZWHBFPIQNNBK85CIY5&Expires=1676306486&Signature=/9m3Q6xViQW0Mo6L8hfMirrLFws="></p><p>然后等待其注入完成</p><p> <img src="https://alan-obs.obs.cn-south-1.myhuaweicloud.com/picgo/wps11-16762170464639.jpg?AccessKeyId=8OZWHBFPIQNNBK85CIY5&Expires=1676306517&Signature=6pBZae0iMmFWI4K30cFNpHIEhAw="></p><p>下一步继续安装下去。</p><p> <img src="https://alan-obs.obs.cn-south-1.myhuaweicloud.com/picgo/wps12-16762170464638.jpg?AccessKeyId=8OZWHBFPIQNNBK85CIY5&Expires=1676306539&Signature=0ItDKMayJ2OgiDqAUIWTUcb4dt4="></p><p> <img src="https://alan-obs.obs.cn-south-1.myhuaweicloud.com/picgo/wps13-167621704646414.jpg?AccessKeyId=8OZWHBFPIQNNBK85CIY5&Expires=1676306567&Signature=cYAGgUJRCgh4atJ06albY80rI5k="></p><p>安装cuda成功以后我们还需要安装cudnn</p><p>进入网站<a href="https://developer.nvidia.cn/zh-cn/cudnn">CUDA 深度神经网络库 (cuDNN) | NVIDIA 开发者</a></p><p>对下载的cuDNN压缩包解压后出现如下三个文件夹子，</p><p>然后找到cuda的安装路径，我的安装路径如下：</p><p>E:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0</p><p>分别将cuDNN三个文件夹的内容分别复制到cuda对应的文件夹里面。</p><p>为cuDNN添加环境变量：<br>找到环境变量-系统变量-path，分别将如下三个变量添加进去，完成安装。</p><p>参考<a href="https://blog.csdn.net/jhsignal/article/details/111401628">Cuda和cuDNN安装教程(超级详细)_jhsignal的博客-CSDN博客_cudnn安装</a></p><p>和<a href="https://blog.csdn.net/jhsignal/article/details/111398427">Windows10检查Cuda和cuDNN是否安装成功？_jhsignal的博客-CSDN博客_检验cuda是否安装成功</a></p><p>对于出现的闪屏问题：——进入到cuda的安装路径，C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.1\extras\demo_suite，找到如下两个.exe文件：</p><p>首先执行：deviceQuery.exe，查看是否出现如下界面：这个过程中如若出现闪屏，那么我们可以这样做：先打开cmd，把exe文件拖进去，按住shift之后敲回车，就不会闪退了，实测有效。</p><p> <img src="https://alan-obs.obs.cn-south-1.myhuaweicloud.com/picgo/wps14-167621704646413.jpg?AccessKeyId=8OZWHBFPIQNNBK85CIY5&Expires=1676306593&Signature=x6Iy9KbtxzRE5WDs0TF8eiqk6ZU="></p><p>然后执行bandwidthTest.exe，出现如下界面，则代代表cuDNN也安装成功。</p><p> <img src="https://alan-obs.obs.cn-south-1.myhuaweicloud.com/picgo/wps15-167621704646412.jpg?AccessKeyId=8OZWHBFPIQNNBK85CIY5&Expires=1676306615&Signature=kFv63oII+2jCEDlSIaRAR2Ky9mU="></p><p>关于如何安装多个版本的cuda参考这篇blog即可。</p><p><a href="https://blog.csdn.net/m0_46579864/article/details/122887343?ops_request_misc=&request_id=&biz_id=102&utm_term=%E5%A6%82%E4%BD%95%E5%90%8C%E6%97%B6%E5%AE%89%E8%A3%85%E4%B8%A4%E4%B8%AAcuda&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-4-122887343.142%5Ev73%5Einsert_down3,201%5Ev4%5Eadd_ask,239%5Ev1%5Econtrol&spm=1018.2226.3001.4187">详细讲解如何在win10系统上安装多个版本的CUDA_Al Hg的博客-CSDN博客_cuda可以安装多个版本吗</a></p><p>具体而言其实就是说如果要使用cuda11.1那么把cuda10.0的禁用即可，也就是把环境变量给它换成其他的就行。</p><p>关于各种版本对照的问题，要参考这张图所示的内容。</p><p> <img src="https://alan-obs.obs.cn-south-1.myhuaweicloud.com/picgo/wps16-167621704646416.jpg?AccessKeyId=8OZWHBFPIQNNBK85CIY5&Expires=1676306634&Signature=wfbCxQwuXLMUyQa7nH5Bde3J280="></p><p>因此我们安装python3.6即可。</p><p>conda create –name BERT python&#x3D;3.6</p><p>Conda activate BERT</p><p>pip install tensorflow-gpu&#x3D;&#x3D;1.15</p><p> <img src="https://alan-obs.obs.cn-south-1.myhuaweicloud.com/picgo/wps17-167621704646415.jpg?AccessKeyId=8OZWHBFPIQNNBK85CIY5&Expires=1676306652&Signature=UKU1Al/wQ0tXAyHs1u8zegR6C9k="></p><p>安装这个pip install keras-applications&gt;&#x3D;1.0.8的时候发生了报错</p><p>那么我们需要做的就是</p><p>进入官网直接下载这个包即可</p><p>cd E:\AnacondaLib</p><p>(BERT) PS E:\AnacondaLib&gt; pip install Keras_Applications-1.0.8-py3-none-any.whl</p><p> <img src="https://alan-obs.obs.cn-south-1.myhuaweicloud.com/picgo/wps18-167621704646419.jpg?AccessKeyId=8OZWHBFPIQNNBK85CIY5&Expires=1676306675&Signature=iqBbPTuwPmbrvctNGg7Gd/wEcp0="></p><p>Tensorflow安装成功</p><p>下面我们参照github的文档说明python setup.py install</p><p>cd D:\code\BERT-BiLSTM-CRF-NER-master</p><p>然后python setup.py install</p><p> <img src="https://alan-obs.obs.cn-south-1.myhuaweicloud.com/picgo/wps19-167621704646418.jpg?AccessKeyId=8OZWHBFPIQNNBK85CIY5&Expires=1676306696&Signature=Gn1zl5sjp2xvp0S34AxxJbjxutA="></p><p>Using c:\users\14508.conda\envs\bert\lib\site-packages</p><p>Searching for numpy&#x3D;&#x3D;1.19.5</p><p>Best match: numpy 1.19.5</p><p>Adding numpy 1.19.5 to easy-install.pth file</p><p>Installing f2py-script.py script to C:\Users\14508.conda\envs\BERT\Scripts</p><p>Installing f2py.exe script to C:\Users\14508.conda\envs\BERT\Scripts</p><p>Using c:\users\14508.conda\envs\bert\lib\site-packages</p><p>Finished processing dependencies for bert-base&#x3D;&#x3D;0.0.9</p><p>代码依赖处理成功</p><p>下面我们继续安装github上面的说明进行下一步操作</p><p>可以使用 -help 查看训练命名实体识别模型的相关参数，其中必须指定data_dir、bert_config_file、output_dir、init_checkpoint vocab_file。</p><p>我们输入命令：bert-base-ner-train -help</p><p> <img src="https://alan-obs.obs.cn-south-1.myhuaweicloud.com/picgo/wps20-167621704646417.jpg?AccessKeyId=8OZWHBFPIQNNBK85CIY5&Expires=1676306714&Signature=TARlLOk0WQQJVGyMQOWGXB8tF28="></p><p>下一步将继续研究<a href="https://github.com/macanv/BERT-BiLSTM-CRF-NER">macanv&#x2F;BERT-BiLSTM-CRF-NER: Tensorflow solution of NER task Using BiLSTM-CRF model with Google BERT Fine-tuning And private Server services (github.com)</a></p><p>这其中有这么一句话：</p><p>注意：“X”、“[CLS]”、“[SEP]”这三个是必需的，您只需将数据标签替换为此返回列表即可。<br>或者你可以使用最后的代码让程序自动从训练数据中获取标签</p><h2 id="2-Bert常用的代码格式整理（仅供参考，存在错误，如需运行请修改）"><a href="#2-Bert常用的代码格式整理（仅供参考，存在错误，如需运行请修改）" class="headerlink" title="2.Bert常用的代码格式整理（仅供参考，存在错误，如需运行请修改）"></a><strong>2.Bert常用的代码格式整理（仅供参考，存在错误，如需运行请修改）</strong></h2><p>python fc.py </p><p>  -data_dir {your dataset dir}\</p><p>  -output_dir {training output dir}\</p><p>  -init_checkpoint {Google BERT model dir}\</p><p>  -bert_config_file {bert_config.json under the Google BERT model dir} \</p><p>-vocab_file {vocab.txt under the Google BERT model dir}</p><p>python3 fc.py –train&#x3D;false –test&#x3D;true –input_size&#x3D;1536 –num_units&#x3D;128 –batch_size&#x3D;8 –epoches&#x3D;4 –learning_rate&#x3D;0.0001</p><p>bert-base-serving-start \</p><p>  -model_dir C:\workspace\python\BERT_Base\output\ner2 \</p><p>  -bert_model_dir F:\chinese_L-12_H-768_A-12</p><p>  -model_pb_dir C:\workspace\python\BERT_Base\model_pb_dir</p><p>  -mode CLASS</p><p>  -max_seq_len 202</p><p>bert-base-serving-start \</p><p>  -model_dir C:\workspace\python\BERT_Base\output\ner2 \</p><p>  -bert_model_dir F:\chinese_L-12_H-768_A-12</p><p>  -model_pb_dir C:\workspace\python\BERT_Base\model_pb_dir</p><p>  -mode CLASS</p><p>  -max_seq_len 202</p><p> python bert_lstm_ner.py  –task_name&#x3D;”NER”  –do_train&#x3D;True –do_eval&#x3D;True  –do_predict&#x3D;True –data_dir&#x3D;NERdata –vocab_file&#x3D;checkpoint&#x2F;vocab.txt –bert_config_file&#x3D;checkpoint&#x2F;bert_config.json –init_checkpoint&#x3D;checkpoint&#x2F;bert_model.ckpt –max_seq_length&#x3D;128 –train_batch_size&#x3D;32 –learning_rate&#x3D;2e-5 –num_train_epochs&#x3D;3.0 –output_dir&#x3D;.&#x2F;output&#x2F;result_dir&#x2F; </p><p>成功运行run_clssifier.py</p><p> python run_classifier.py  –task_name&#x3D;MRPC –do_train&#x3D;True –do_eval&#x3D;True –data_dir&#x3D;D:&#x2F;code&#x2F;BERT-open&#x2F;GLUE&#x2F;glue_data\MRPC  –vocab_file&#x3D;D:&#x2F;code&#x2F;BERT-open&#x2F;GLUE&#x2F;BERT_BASE_DIR&#x2F;chinese_L-12_H-768_A-12&#x2F;vocab.txt  –bert_config_file&#x3D;D:&#x2F;code&#x2F;BERT-open&#x2F;GLUE&#x2F;BERT_BASE_DIR&#x2F;chinese_L-12_H-768_A-12&#x2F;bert_config.json –init_checkpoint&#x3D;D:&#x2F;code&#x2F;BERT-open&#x2F;GLUE&#x2F;BERT_BASE_DIR&#x2F;chinese_L-12_H-768_A-12&#x2F;bert_model.ckpt  –max_seq_length&#x3D;128  –train_batch_size&#x3D;8  –learning_rate&#x3D;2e-5 –num_train_epochs&#x3D;3.0  –output_dir&#x3D;D:&#x2F;code&#x2F;BERT-open&#x2F;GLUE&#x2F;output</p><p><strong>原来的</strong></p><p>python run_classifier.py –task_name&#x3D;MRPC –do_train&#x3D;true –do_eval&#x3D;true –data_dir&#x3D;$GLUE_DIR&#x2F;MRPC –vocab_file&#x3D;$BERT_BASE_DIR&#x2F;vocab.txt –bert_config_file&#x3D;$BERT_BASE_DIR&#x2F;bert_config.json </p><p> –init_checkpoint&#x3D;$BERT_BASE_DIR&#x2F;bert_model.ckpt –max_seq_length&#x3D;128 –train_batch_size&#x3D;32 –learning_rate&#x3D;2e-5 –num_train_epochs&#x3D;3.0 –output_dir&#x3D;&#x2F;tmp&#x2F;mrpc_output&#x2F;</p><p><strong>修改之后的</strong></p><p>python run_classifier.py –task_name&#x3D;MRPC –do_train&#x3D;true –do_eval&#x3D;true –data_dir&#x3D;D:&#x2F;code&#x2F;BERT-open&#x2F;GLUE&#x2F;glue_data\MRPC –vocab_file&#x3D;D:&#x2F;code&#x2F;BERT-open&#x2F;GLUE&#x2F;BERT_BASE_DIR&#x2F;chinese_L-12_H-768_A-12&#x2F;vocab.txt –bert_config_file&#x3D;D:&#x2F;code&#x2F;BERT-open&#x2F;GLUE&#x2F;BERT_BASE_DIR&#x2F;chinese_L-12_H-768_A-12&#x2F;bert_config.json –init_checkpoint&#x3D;D:&#x2F;code&#x2F;BERT-open&#x2F;GLUE&#x2F;BERT_BASE_DIR&#x2F;chinese_L-12_H-768_A-12&#x2F;bert_model.ckpt –max_seq_length&#x3D;128 –train_batch_size&#x3D;2  –learning_rate&#x3D;2e-5 –num_train_epochs&#x3D;3.0 –output_dir&#x3D;D:&#x2F;code&#x2F;BERT-open&#x2F;GLUE&#x2F;output2</p><p> –do_predict&#x3D;True –data_dir&#x3D;NERdata –vocab_file&#x3D;checkpoint&#x2F;vocab.txt –bert_config_file&#x3D;checkpoint&#x2F;bert_config.json –init_checkpoint&#x3D;checkpoint&#x2F;bert_model.ckpt –max_seq_length&#x3D;128 –train_batch_size&#x3D;32 –learning_rate&#x3D;2e-5 –num_train_epochs&#x3D;3.0 –output_dir&#x3D;.&#x2F;output&#x2F;result_dir&#x2F; </p><p>完成了bert fake news detection代码中的run_classifier和</p><h2 id="3-云中运行代码（Geogle-colab）"><a href="#3-云中运行代码（Geogle-colab）" class="headerlink" title="3.云中运行代码（Geogle-colab）"></a><strong>3.云中运行代码（Geogle-colab）</strong></h2><p><a href="https://blog.csdn.net/qq_24594197/article/details/109667227">(1条消息) Tensorflow调参报错：Resource exhausted OOM when allocating tensor with shape_仗剑-行天下的博客-CSDN博客_type float on</a></p><p>在查阅了一些CSDN和stackflow的一些资料以后，我了解到这个OMM内存溢出，需要修改batch-size，遂采取使用geogle-colab这个免费的云平台。</p><p><a href="https://blog.csdn.net/u014108004/article/details/84325301?spm=1001.2101.3001.6661.1&utm_medium=distribute.pc_relevant_t0.none-task-blog-2~default~CTRLIST~Rate-1-84325301-blog-90237285.pc_relevant_multi_platform_whitelistv3&depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2~default~CTRLIST~Rate-1-84325301-blog-90237285.pc_relevant_multi_platform_whitelistv3&utm_relevant_index=1">(1条消息) 使用Google免费GPU进行BERT模型fine-tuning_LeoWood 的博客-CSDN博客</a></p><p>如果TensorFlow版本为2.xx则可以参考这篇博客当中提出的使用方法，但是无奈的是我的代码需要的环境可以说是与这个colab基本上无缘了，因为其geogle云硬盘官网已经是弃用了TensorFlow1.xx，以前的话是可以进行TensorFlow版本的切换的，现在看来是不行的。</p><h2 id="4-云中运行代码（矩池云GPU）"><a href="#4-云中运行代码（矩池云GPU）" class="headerlink" title="4.云中运行代码（矩池云GPU）"></a><strong>4.云中运行代码（矩池云GPU）</strong></h2><p>后来用到的就是我们的矩阵云这个云 GPU加速器平台。</p><p> 我租用的是一块NVIDIA RTX A2000 12GB显存 30GB内存的显卡。</p><p><a href="https://blog.csdn.net/AAGHJJSJBJSHJ/article/details/122652323?spm=1001.2014.3001.5501">(1条消息) Pycharm连接矩池云平台租的GPU训练神经网络_ZFour_X的博客-CSDN博客_pycharm远程链接矩池云</a></p><p><a href="https://blog.csdn.net/AAGHJJSJBJSHJ/article/details/124610667">(1条消息) Xshell连接矩池云进行BERT-BiLSTM-CRF模型训练_ZFour_X的博客-CSDN博客</a></p><p>在鱼肚我在完成了基本的pycharm对于基本的shell云端部署以后，就开始进行训练了。</p><p><a href="https://www.matpool.com/">矩池云 - 专注于人工智能领域的云服务商 (matpool.com)</a></p><p>对于Bert-base模型的MRPC模型训练过程中，我全程训练使用的主要是这个CPU，GPU基本上没有怎么满载运行，可能只用了其中的1%不到。</p><p><img src="https://alan-obs.obs.cn-south-1.myhuaweicloud.com/picgo/27-167621704646420.png?AccessKeyId=8OZWHBFPIQNNBK85CIY5&Expires=1676306737&Signature=IJ4knOfRZSkCQ8D7U9LUBvRJmRM="></p><p>在下面的过程中我还踩了一次坑，主要就是在训练了很久以后竟然突然报错了，然后我仔细检查了一下，原来是这个bert&#x2F;uncased_L-24_H-1024_A-16被我换成了bert&#x2F;Chinese_L-24_H-1024_A-16这样的话，基本是训练的一个错误的匹配，因为其训练过程中，是要</p><p> !python run_classifier.py –task_name&#x3D;MRPC  –do_train&#x3D;true –do_eval&#x3D;true –data_dir&#x3D;data –vocab_file&#x3D;gs:&#x2F;&#x2F;cloud-tpu-checkpoints&#x2F;bert&#x2F;uncased_L-24_H-1024_A-16&#x2F;vocab.txt –bert_config_file&#x3D;gs:&#x2F;&#x2F;cloud-tpu-checkpoints&#x2F;bert&#x2F;uncased_L-24_H-1024_A-16&#x2F;bert_config.json –init_checkpoint&#x3D;gs:&#x2F;&#x2F;cloud-tpu-checkpoints&#x2F;bert&#x2F;uncased_L-24_H-1024_A-16&#x2F;bert_model.ckpt –max_seq_length&#x3D;128 –train_batch_size&#x3D;4 –learning_rate&#x3D;2e-5 –num_train_epochs&#x3D;3.0 –output_dir&#x3D;output </p><p> 报错参考这个：<a href="https://blog.csdn.net/jacke121/article/details/119644684">(1条消息) tensorflow.python.framework.errors_impl.DataLossError:_AI视觉网奇的博客-CSDN博客</a></p><p>内容大致是：tensorflow.python.framework.errors_impl.DataLossError: not an sstable(bad magic number)</p><p>这个错误是因为–input_checkpoint的名字搞错了，后面的.data-00000-of-00001不应该写。</p><p>tf1.13版本以后，加载预训练，需要index，模型权重和checkpoint，文件，缺少则会报错。</p><p>tf版加载模型报错：</p><p><a href="https://github.com/YadiraF/PRNet">https://github.com/YadiraF/PRNet</a></p><p>PRNet是Joint 3D Face Reconstruction and Dense Alignment with Position Map Regression Network中提出的3D人脸重建算法模型，文章发表于ECCV2018，利用神经网络直接预测了3D landmark，取得了不错的效果。</p><p>TensorFlow1.15 报错：</p><p>ValueError: The passed save_path is not a valid checkpoint: Data\net-data\256_256_resfcn256_weight</p><p>经过测试，发现如果模型文件不存在，也会报这个错。</p><p>开源网络提供的预训练缺少模型的index文件。</p><p>找到补上就可以了。</p><p>在我们进行训练命令当中不用加上这个后缀.data-00000-of-00001。但是这个文件不能删除。</p><p>关于我在矩阵云中训练代码的命令如下。batch-size设置为32即可。</p><p>python run_classifier.py –task_name&#x3D;MRPC –do_train&#x3D;true –do_eval&#x3D;true –do_predict –data_dir&#x3D;&#x2F;mnt&#x2F;PyCharm_Project_1&#x2F;GLUE&#x2F;glue_data&#x2F;MRPC –vocab_file&#x3D;&#x2F;mnt&#x2F;PyCharm_Project_1&#x2F;GLUE&#x2F;BERT_BASE_DIR&#x2F;uncased_L-12_H-768_A-12&#x2F;vocab.txt –bert_config_file&#x3D;&#x2F;mnt&#x2F;PyCharm_Project_1&#x2F;GLUE&#x2F;BERT_BASE_DIR&#x2F;uncased_L-12_H-768_A-12&#x2F;bert_config.json –init_checkpoint&#x3D;&#x2F;mnt&#x2F;PyCharm_Project_1&#x2F;GLUE&#x2F;BERT_BASE_DIR&#x2F;uncased_L-12_H-768_A-12&#x2F;bert_model.ckpt –max_seq_length&#x3D;128 –train_batch_size&#x3D;32 –learning_rate&#x3D;2e-5 –num_train_epochs&#x3D;1.0 –output_dir&#x3D;&#x2F;mnt&#x2F;PyCharm_Project_1&#x2F;GLUE&#x2F;output2</p><p>我们参考bert的github源码中的一段话来查看我们的MRPC的训练效果吧。</p><p><strong>Sentence (and sentence-pair) classification tasks</strong></p><p>Before running this example you must download the <a href="https://gluebenchmark.com/tasks">GLUE data</a> by running <a href="https://gist.github.com/W4ngatang/60c2bdb54d156a41194446737ce03e2e">this script</a> and unpack it to some directory <code>$GLUE_DIR</code>. Next, download the <code>BERT-Base</code> checkpoint and unzip it to some directory <code>$BERT_BASE_DIR</code>.</p><p>This example code fine-tunes <code>BERT-Base</code> on the Microsoft Research Paraphrase Corpus (MRPC) corpus, which only contains 3,600 examples and can fine-tune in a few minutes on most GPUs.</p><p>译文:在运行此示例之前，您必须下载<a href="https://gluebenchmark.com/tasks">GLUE数据</a>通过运行<a href="https://gist.github.com/W4ngatang/60c2bdb54d156a41194446737ce03e2e">此脚本</a>并将其解压缩到某个目录“$GLUE_DIR”。接下来，下载“BERT Base”检查点并将其解压缩到某个目录“$BERT_Base_DIR”。<br>此示例代码在Microsoft Research Paraphrase语料库（MRPC）上对“BERT Base”进行微调，该语料库仅包含3600个示例，在大多数GPU上可以在几分钟内进行微调。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">export BERT_BASE_DIR=/path/to/bert/uncased_L-12_H-768_A-12</span><br><span class="line">export GLUE_DIR=/path/to/glue</span><br><span class="line"></span><br><span class="line">python run_classifier.py \</span><br><span class="line">  --task_name=MRPC \</span><br><span class="line">  --do_train=true \</span><br><span class="line">  --do_eval=true \</span><br><span class="line">  --data_dir=$GLUE_DIR/MRPC \</span><br><span class="line">  --vocab_file=$BERT_BASE_DIR/vocab.txt \</span><br><span class="line">  --bert_config_file=$BERT_BASE_DIR/bert_config.json \</span><br><span class="line">  --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \</span><br><span class="line">  --max_seq_length=128 \</span><br><span class="line">  --train_batch_size=32 \</span><br><span class="line">  --learning_rate=2e-5 \</span><br><span class="line">  --num_train_epochs=3.0 \</span><br><span class="line">  --output_dir=/tmp/mrpc_output/</span><br></pre></td></tr></table></figure><p>You should see output like this:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">***** Eval results *****</span><br><span class="line">  eval_accuracy = 0.845588</span><br><span class="line">  eval_loss = 0.505248</span><br><span class="line">  global_step = 343</span><br><span class="line">  loss = 0.505248</span><br></pre></td></tr></table></figure><p>This means that the Dev set accuracy was 84.55%. Small sets like MRPC have a high variance in the Dev set accuracy, even when starting from the same pre-training checkpoint. If you re-run multiple times (making sure to point to different <code>output_dir</code>), you should see results between 84% and 88%.</p><p>A few other pre-trained models are implemented off-the-shelf in <code>run_classifier.py</code>, so it should be straightforward to follow those examples to use BERT for any single-sentence or sentence-pair classification task.</p><p>Note: You might see a message <code>Running train on CPU</code>. This really just means that it’s running on something other than a Cloud TPU, which includes a GPU.</p><p>译文:这意味着Dev集合的准确率为84.55%。像MRPC这样的小集合在Dev集合准确率上具有很高的差异，即使从同一个训练前检查点开始也是如此。如果您多次运行（请确保指向不同的“output_dir”），结果应该在84%到88%之间。<br>其他一些预先训练的模型在“run_classifier.py”中已现成实现，因此，遵循这些示例将BERT用于任何单个句子或句子对分类任务应该很简单。<br>注意：您可能会看到消息“正在CPU上运行训练”。这真的意味着它运行的不是云TPU，而是包括GPU。</p><p>而我的模型在云中历经差不多1小时多一点时间的训练以后，其效果如下图所示。</p><p><img src="https://alan-obs.obs.cn-south-1.myhuaweicloud.com/picgo/25-167621704646422.png?AccessKeyId=8OZWHBFPIQNNBK85CIY5&Expires=1676306756&Signature=2OfgdbY+P3W0IhLb5mRygeWjtUs="></p><p><img src="https://alan-obs.obs.cn-south-1.myhuaweicloud.com/picgo/26-167621704646421.png?AccessKeyId=8OZWHBFPIQNNBK85CIY5&Expires=1676306771&Signature=z6b0Vf0g0FprZWqUvapKLn1+pD0="></p><p>其中显而易见的是训练的准确率为81.37%这个比官方说明的这个结果应该在84%到88%之间，仅仅低了2.7%左右，这个可能还是有一些原因的。</p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 自然语言处理，Bert模型，MRPC训练，踩坑记录，encrypted </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Detectron2实战案例</title>
      <link href="/2022/05/15/Detectron2yolof/"/>
      <url>/2022/05/15/Detectron2yolof/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>最近几个月有在跑detectron2和yolof但是遇到了很多困难，下次出一期安装detectron2的教程，其实可以搬运来自我自己的csdn写的内容也可以的</p><h1 id="最近概况："><a href="#最近概况：" class="headerlink" title="最近概况："></a>最近概况：</h1><p>最近在学习一些视觉的内容detectron2还有yolof，遇到了很多的问题，在csdn上基本上都可以寻找到相关答案的</p><p><a href="https://blog.csdn.net/blink182007?type=blog">https://blog.csdn.net/blink182007?type=blog</a>    “我的csdn主页”<br><a href="https://blog.csdn.net/blink182007/article/details/124222921?spm=1001.2014.3001.5502">https://blog.csdn.net/blink182007/article/details/124222921?spm=1001.2014.3001.5502</a>    “Detectron2安装踩坑记录（比较详细版）”</p><p>基本格式示例：python demo&#x2F;demo.py –config-file configs&#x2F;COCO-Detection&#x2F;faster_rcnn_R_50_FPN_1x.yaml –input &#x2F;images&#x2F;1.jpg –output results –opt MODEL.WEIGHTS output&#x2F;model_0049999.pth</p><p>python demo&#x2F;demo.py –config-file configs&#x2F;COCO-Detection&#x2F;faster_rcnn_R_50_FPN_1x.yaml –input images&#x2F;1.jpg –output results 可以但是没有结果</p><p>我的detectron2模型本地存储地址”E:\anacondalib\detectron2-master\detectron2-master\configs\COCO-Detection\retinanet_R_50_FPN_3x.yaml”</p><h1 id="参考文献来源："><a href="#参考文献来源：" class="headerlink" title="参考文献来源："></a><strong>参考文献来源：</strong></h1><p>参考来自博客网站： <a href="https://blog.csdn.net/winorlose2000/article/details/112549795">https://blog.csdn.net/winorlose2000/article/details/112549795</a>    “(82条消息) Detectron2 快速开始，使用 WebCam 测试_GoCodingInMyWay的博客-CSDN博客”</p><p>模型137849600&#x2F;model_final_f10217.pkl下载来自Modelzoo网站：detectron2&#x2F;MODEL_ZOO.md at main · facebookresearch&#x2F;detectron2 (github.com) <a href="https://github.com/facebookresearch/detectron2/blob/main/MODEL_ZOO.md">https://github.com/facebookresearch/detectron2/blob/main/MODEL_ZOO.md</a>    “137849600&#x2F;model_final_f10217.pkl”</p><p>Python demo&#x2F;demo.py –config-file configs&#x2F;COCO-Detection&#x2F;faster_rcnn_R_50_FPN_3x.yaml –webcam –output results –opt MODEL.WEIGHTS models&#x2F;model_final_280758.pkl 实时检测失败<br>Python demo&#x2F;demo.py –config-file configs&#x2F;COCO-Detection&#x2F;faster_rcnn_R_50_FPN_3x.yaml –input images&#x2F;4.jpg –output results –opt MODEL.WEIGHTS models&#x2F;model_final_280758.pkl</p><p>Python demo&#x2F;demo.py –config-file configs&#x2F;LVISv1-InstanceSegmentation&#x2F;mask_rcnn_R_50_FPN_1x.yaml –input images&#x2F;5.jpg –output results –opt MODEL.WEIGHTS models&#x2F;model_final_571f7c.pkl 失败</p><p>“E:\anacondalib\detectron2-master\detectron2-master\configs\COCO-InstanceSegmentation\mask_rcnn_R_50_FPN_1x.yaml”</p><p>“E:\anacondalib\detectron2-master\detectron2-master\configs\LVISv1-InstanceSegmentation\mask_rcnn_R_50_FPN_1x.yaml”</p><h1 id="检测代码和实例"><a href="#检测代码和实例" class="headerlink" title="检测代码和实例"></a><strong>检测代码和实例</strong></h1><p><strong>1.1 普通目标检测：</strong><br>Python demo&#x2F;demo.py –config-file configs&#x2F;COCO-Detection&#x2F;faster_rcnn_R_50_FPN_3x.yaml –input images&#x2F;1.jpg –output results –opt MODEL.WEIGHTS models&#x2F;model_final_280758.pkl<br>成功语句 普通目标检测√<br><strong>2.1 关键点检测：</strong><br>Python demo&#x2F;demo.py –config-file configs&#x2F;COCO-Keypoints&#x2F;keypoint_rcnn_R_50_FPN_3x.yaml –input images&#x2F;7.jpg –output results –opt MODEL.WEIGHTS models&#x2F;model_final_a6e10b.pkl 关键点检测成功√（”E:\anacondalib\detectron2-master\detectron2-master\configs\COCO-Keypoints\keypoint_rcnn_R_50_FPN_3x.yaml”<br>）（model_final_a6e10b.pkl）<br><strong>3.1 语义分割实例：</strong><br>Python demo&#x2F;demo.py –config-file configs&#x2F;COCO-PanopticSegmentation&#x2F;panoptic_fpn_R_50_3x.yaml –input images&#x2F;8.jpg –output results –opt MODEL.WEIGHTS models&#x2F;model_final_c10459.pkl 语义分割实例成功√<br>（”E:\anacondalib\detectron2-master\detectron2-master\configs\COCO-PanopticSegmentation\panoptic_fpn_R_50_3x.yaml”）<br>（model_final_c10459.pkl）<br><strong>4.1城市街道平面图实例</strong><br>Python demo&#x2F;demo.py –config-file configs\Cityscapes\mask_rcnn_R_50_FPN.yaml –input images&#x2F;13.jpg –output results –opt MODEL.WEIGHTS models&#x2F;model_final_af9cf5.pkl</p><p><strong>5.1实时摄像头目标侦测</strong><br>Python demo&#x2F;demo.py –config-file configs&#x2F;COCO-Detection&#x2F;faster_rcnn_R_50_FPN_3x.yaml –webcam –output results –opt MODEL.WEIGHTS models&#x2F;model_final_280758.pkl<br>(成功)<br>对于报错ERROR——File “demo&#x2F;demo.py”, line 141, in <module><br>assert args.output is None, “output not yet supported with –webcam!”</p><p><img src="https://pica.zhimg.com/80/v2-a26cf9f2a62361c76efbebd8335c6664_r.jpg"></p><p>将这一行注释掉即可<br>参考来自：(82条消息) Detectron2 快速开始，使用 WebCam 测试_GoCodingInMyWay的博客-CSDN博客</p><p><strong>6.1实时语义分割检测</strong><br>Python demo&#x2F;demo.py –config-file configs&#x2F;COCO-PanopticSegmentation&#x2F;panoptic_fpn_R_50_3x.yaml –webcam –output results –opt MODEL.WEIGHTS models&#x2F;model_final_c10459.pkl<br>（成功）</p><h1 id="跑YOLOF的相关内容："><a href="#跑YOLOF的相关内容：" class="headerlink" title="跑YOLOF的相关内容："></a>跑YOLOF的相关内容：</h1><p>训练命令为：</p><p>***\Python .&#x2F;tools&#x2F;train_net.py –num-gpus 1 –config-file .&#x2F;configs&#x2F;yolof_R_50_C5_1x.yaml –eval-only MODEL.WEIGHTS output&#x2F;yolof&#x2F;R_50_C5_1x&#x2F;model_final.pth</p><p><img src="https://pica.zhimg.com/80/v2-3c23215c130871b12d8e02f3ffa686c2_r.jpg"></p><p>报错如下，需要注册coco2017数据集，自己标注数据，按照b站小鸡炖技术来训练文件，寻找出路和改进方向方法。标签平滑？</p><p><img src="https://pic4.zhimg.com/80/v2-76746e175d58736b67d0c556c9caf765_r.jpg"></p><p><strong>参考一下：一位b站深度学习up的训练命令如上图所示。</strong></p><p><strong>根据官方文档的内容——测试（test）命令如下：</strong></p><p>python .&#x2F;tools&#x2F;train_net.py –num-gpus 1 –config-file .&#x2F;configs&#x2F;yolof_R_50_C5_1x.yaml –eval-only MODEL.WEIGHTS &#x2F;path&#x2F;to&#x2F;checkpoint_file</p><p>运行过程如下（复制的比较多，可能有点长）<br>[04&#x2F;23 11:13:46 detectron2]: Environment info:</p><hr><p>sys.platform            win32<br>Python                  3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]<br>numpy                   1.21.5<br>detectron2              0.5 @e:\anacondalib\detectron2-master\detectron2-master\detectron2<br>Compiler                MSVC 192829923<br>CUDA compiler           CUDA 11.1<br>detectron2 arch flags   e:\anacondalib\detectron2-master\detectron2-master\detectron2_C.cp38-win_amd64.pyd; cannot find cuobjdump<br>DETECTRON2_ENV_MODULE   <not set><br>PyTorch                 1.9.0+cu111 @D:\Anaconda3\envs\detectron2\lib\site-packages\torch<br>PyTorch debug build     False<br>GPU available           Yes<br>GPU 0                   GeForce GTX 1650 (arch&#x3D;7.5)<br>Driver version          457.63<br>CUDA_HOME               C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.1<br>Pillow                  9.0.1<br>torchvision             0.10.0+cu111 @D:\Anaconda3\envs\detectron2\lib\site-packages\torchvision<br>torchvision arch flags  D:\Anaconda3\envs\detectron2\lib\site-packages\torchvision_C.pyd; cannot find cuobjdump<br>fvcore                  0.1.5.post20220414<br>iopath                  0.1.9<br>cv2                     4.5.5</p><hr><p>PyTorch built with:</p><ul><li>C++ Version: 199711</li><li>MSVC 192829337</li><li>Intel(R) Math Kernel Library Version 2020.0.2 Product Build 20200624 for Intel(R) 64 architecture applications</li><li>Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)</li><li>OpenMP 2019</li><li>CPU capability usage: AVX2</li><li>CUDA Runtime 11.1</li><li>NVCC architecture flags: -gencode;arch&#x3D;compute_37,code&#x3D;sm_37;-gencode;arch&#x3D;compute_50,code&#x3D;sm_50;-gencode;arch&#x3D;compute_60,code&#x3D;sm_60;-gencode;arch&#x3D;compute_61,code&#x3D;sm_61;-gencode;arch&#x3D;compute_70,code&#x3D;sm_70;-gencode;arch&#x3D;compute_75,code&#x3D;sm_75;-gencode;arch&#x3D;compute_80,code&#x3D;sm_80;-gencode;arch&#x3D;compute_86,code&#x3D;sm_86;-gencode;arch&#x3D;compute_37,code&#x3D;compute_37</li><li>CuDNN 8.0.5</li><li>Magma 2.5.4</li><li>Build settings: BLAS_INFO&#x3D;mkl, BUILD_TYPE&#x3D;Release, CUDA_VERSION&#x3D;11.1, CUDNN_VERSION&#x3D;8.0.5, CXX_COMPILER&#x3D;C:&#x2F;w&#x2F;b&#x2F;windows&#x2F;tmp_bin&#x2F;sccache-cl.exe, CXX_FLAGS&#x3D;&#x2F;DWIN32 &#x2F;D_WINDOWS &#x2F;GR &#x2F;EHs<br>c &#x2F;w &#x2F;bigobj -DUSE_PTHREADPOOL -openmp:experimental -IC:&#x2F;w&#x2F;b&#x2F;windows&#x2F;mkl&#x2F;include -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOCUPTI -DUSE_FBGEMM -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE,<br> LAPACK_INFO&#x3D;mkl, PERF_WITH_AVX&#x3D;1, PERF_WITH_AVX2&#x3D;1, PERF_WITH_AVX512&#x3D;1, TORCH_VERSION&#x3D;1.9.0, USE_CUDA&#x3D;ON, USE_CUDNN&#x3D;ON, USE_EXCEPTION_PTR&#x3D;1, USE_GFLAGS&#x3D;OFF, USE_GLOG&#x3D;OFF, USE_MKL&#x3D;ON, USE_MKLDNN&#x3D;ON, USE_MPI&#x3D;OFF, USE_NCCL&#x3D;OFF, USE_NNPACK&#x3D;OFF, USE_OPENMP&#x3D;ON,</li></ul><p>[04&#x2F;23 11:13:46 detectron2]: Command line arguments: Namespace(config_file&#x3D;’.&#x2F;configs&#x2F;yolof_R_50_C5_1x.yaml’, dist_url&#x3D;’tcp:&#x2F;&#x2F;127.0.0.1:49153’, eval_only&#x3D;True, machine_rank&#x3D;0, num_gpus&#x3D;1, num_machines&#x3D;1, opts&#x3D;[‘MODEL.WEIGHTS’, ‘&#x2F;path&#x2F;to&#x2F;checkpoint_file’], resume&#x3D;False)<br>[04&#x2F;23 11:13:47 detectron2]: Contents of args.config_file&#x3D;.&#x2F;configs&#x2F;yolof_R_50_C5_1x.yaml:<br><em>BASE</em>: “Base-YOLOF.yaml”<br>MODEL:<br>  WEIGHTS: “detectron2:&#x2F;&#x2F;ImageNetPretrained&#x2F;MSRA&#x2F;R-50.pkl”<br>  RESNETS:<br>    DEPTH: 50<br>OUTPUT_DIR: “output&#x2F;yolof&#x2F;R_50_C5_1x”</p><p>[04&#x2F;23 11:13:47 detectron2]: Running with full config:<br>CUDNN_BENCHMARK: false<br>DATALOADER:<br>  ASPECT_RATIO_GROUPING: true<br>  FILTER_EMPTY_ANNOTATIONS: true<br>  NUM_WORKERS: 2<br>  REPEAT_THRESHOLD: 0.0<br>  SAMPLER_TRAIN: TrainingSampler<br>DATASETS:<br>  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000<br>  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000<br>  PROPOSAL_FILES_TEST: []<br>  PROPOSAL_FILES_TRAIN: []<br>  TEST:</p><ul><li>coco_2017_val<br>TRAIN:</li><li>coco_2017_train<br>GLOBAL:<br>HACK: 1.0<br>INPUT:<br>CROP:<br>ENABLED: false<br>SIZE:<ul><li>0.9</li><li>0.9<br>TYPE: relative_range<br>DISTORTION:<br>ENABLED: false<br>EXPOSURE: 1.5<br>HUE: 0.1<br>SATURATION: 1.5<br>FORMAT: BGR<br>JITTER_CROP:<br>ENABLED: false<br>JITTER_RATIO: 0.3<br>MASK_FORMAT: polygon<br>MAX_SIZE_TEST: 1333<br>MAX_SIZE_TRAIN: 1333<br>MIN_SIZE_TEST: 800<br>MIN_SIZE_TRAIN:</li></ul></li><li>800<br>MIN_SIZE_TRAIN_SAMPLING: choice<br>MOSAIC:<br>ENABLED: false<br>MIN_OFFSET: 0.2<br>MOSAIC_HEIGHT: 640<br>MOSAIC_WIDTH: 640<br>NUM_IMAGES: 4<br>POOL_CAPACITY: 1000<br>RANDOM_FLIP: horizontal<br>RESIZE:<br>ENABLED: false<br>SCALE_JITTER:<ul><li>0.8</li><li>1.2<br>SHAPE:</li><li>640</li><li>640<br>TEST_SHAPE:</li><li>608</li><li>608<br>SHIFT:<br>SHIFT_PIXELS: 32<br>MODEL:<br>ANCHOR_GENERATOR:<br>ANGLES:</li><li><ul><li>-90</li><li>0</li><li>90<br>ASPECT_RATIOS:</li></ul></li><li><ul><li>1.0<br>NAME: DefaultAnchorGenerator<br>OFFSET: 0.0<br>SIZES:</li></ul></li><li><ul><li>32</li><li>64</li><li>128</li><li>256</li><li>512<br>BACKBONE:<br>FREEZE_AT: 2<br>NAME: build_resnet_backbone<br>DARKNET:<br>DEPTH: 53<br>NORM: BN<br>OUT_FEATURES:</li></ul></li><li>res5<br>RES5_DILATION: 1<br>WITH_CSP: true<br>DEVICE: cuda<br>FPN:<br>FUSE_TYPE: sum<br>IN_FEATURES: []<br>NORM: ‘’<br>OUT_CHANNELS: 256<br>KEYPOINT_ON: false<br>LOAD_PROPOSALS: false<br>MASK_ON: false<br>META_ARCHITECTURE: YOLOF<br>PANOPTIC_FPN:<br>COMBINE:<br>ENABLED: true<br>INSTANCES_CONFIDENCE_THRESH: 0.5<br>OVERLAP_THRESH: 0.5<br>STUFF_AREA_LIMIT: 4096<br>INSTANCE_LOSS_WEIGHT: 1.0<br>PIXEL_MEAN:</li></ul></li><li>103.53</li><li>116.28</li><li>123.675<br>PIXEL_STD:</li><li>1.0</li><li>1.0</li><li>1.0<br>PROPOSAL_GENERATOR:<br>MIN_SIZE: 0<br>NAME: RPN<br>RESNETS:<br>DEFORM_MODULATED: false<br>DEFORM_NUM_GROUPS: 1<br>DEFORM_ON_PER_STAGE:<ul><li>false</li><li>false</li><li>false</li><li>false<br>DEPTH: 50<br>NORM: FrozenBN<br>NUM_GROUPS: 1<br>OUT_FEATURES:</li><li>res5<br>RES2_OUT_CHANNELS: 256<br>RES5_DILATION: 1<br>STEM_OUT_CHANNELS: 64<br>STRIDE_IN_1X1: true<br>WIDTH_PER_GROUP: 64<br>RETINANET:<br>BBOX_REG_LOSS_TYPE: smooth_l1<br>BBOX_REG_WEIGHTS: &amp;id002</li><li>1.0</li><li>1.0</li><li>1.0</li><li>1.0<br>FOCAL_LOSS_ALPHA: 0.25<br>FOCAL_LOSS_GAMMA: 2.0<br>IN_FEATURES:</li><li>p3</li><li>p4</li><li>p5</li><li>p6</li><li>p7<br>IOU_LABELS:</li><li>0</li><li>-1</li><li>1<br>IOU_THRESHOLDS:</li><li>0.4</li><li>0.5<br>NMS_THRESH_TEST: 0.5<br>NORM: ‘’<br>NUM_CLASSES: 80<br>NUM_CONVS: 4<br>PRIOR_PROB: 0.01<br>SCORE_THRESH_TEST: 0.05<br>SMOOTH_L1_LOSS_BETA: 0.1<br>TOPK_CANDIDATES_TEST: 1000<br>ROI_BOX_CASCADE_HEAD:<br>BBOX_REG_WEIGHTS:</li><li>&amp;id001<ul><li>10.0</li><li>10.0</li><li>5.0</li><li>5.0</li></ul></li><li><ul><li>20.0</li><li>20.0</li><li>10.0</li><li>10.0</li></ul></li><li><ul><li>30.0</li><li>30.0</li><li>15.0</li><li>15.0<br>IOUS:</li></ul></li><li>0.5</li><li>0.6</li><li>0.7<br>ROI_BOX_HEAD:<br>BBOX_REG_LOSS_TYPE: smooth_l1<br>BBOX_REG_LOSS_WEIGHT: 1.0<br>BBOX_REG_WEIGHTS: *id001<br>CLS_AGNOSTIC_BBOX_REG: false<br>CONV_DIM: 256<br>FC_DIM: 1024<br>NAME: ‘’<br>NORM: ‘’<br>NUM_CONV: 0<br>NUM_FC: 0<br>POOLER_RESOLUTION: 14<br>POOLER_SAMPLING_RATIO: 0<br>POOLER_TYPE: ROIAlignV2<br>SMOOTH_L1_BETA: 0.0<br>TRAIN_ON_PRED_BOXES: false<br>ROI_HEADS:<br>BATCH_SIZE_PER_IMAGE: 512<br>IN_FEATURES:</li><li>res4<br>IOU_LABELS:</li><li>0</li><li>1<br>IOU_THRESHOLDS:</li><li>0.5<br>NAME: Res5ROIHeads<br>NMS_THRESH_TEST: 0.5<br>NUM_CLASSES: 80<br>POSITIVE_FRACTION: 0.25<br>PROPOSAL_APPEND_GT: true<br>SCORE_THRESH_TEST: 0.05<br>ROI_KEYPOINT_HEAD:<br>CONV_DIMS:</li><li>512</li><li>512</li><li>512</li><li>512</li><li>512</li><li>512</li><li>512</li><li>512<br>LOSS_WEIGHT: 1.0<br>MIN_KEYPOINTS_PER_IMAGE: 1<br>NAME: KRCNNConvDeconvUpsampleHead<br>NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true<br>NUM_KEYPOINTS: 17<br>POOLER_RESOLUTION: 14<br>POOLER_SAMPLING_RATIO: 0<br>POOLER_TYPE: ROIAlignV2<br>ROI_MASK_HEAD:<br>CLS_AGNOSTIC_MASK: false<br>CONV_DIM: 256<br>NAME: MaskRCNNConvUpsampleHead<br>NORM: ‘’<br>NUM_CONV: 0<br>POOLER_RESOLUTION: 14<br>POOLER_SAMPLING_RATIO: 0<br>POOLER_TYPE: ROIAlignV2<br>RPN:<br>BATCH_SIZE_PER_IMAGE: 256<br>BBOX_REG_LOSS_TYPE: smooth_l1<br>BBOX_REG_LOSS_WEIGHT: 1.0<br>BBOX_REG_WEIGHTS: *id002<br>BOUNDARY_THRESH: -1<br>CONV_DIMS:</li><li>-1<br>HEAD_NAME: StandardRPNHead<br>IN_FEATURES:</li><li>res4<br>IOU_LABELS:</li><li>0</li><li>-1</li><li>1<br>IOU_THRESHOLDS:</li><li>0.3</li><li>0.7<br>LOSS_WEIGHT: 1.0<br>NMS_THRESH: 0.7<br>POSITIVE_FRACTION: 0.5<br>POST_NMS_TOPK_TEST: 1000<br>POST_NMS_TOPK_TRAIN: 2000<br>PRE_NMS_TOPK_TEST: 6000<br>PRE_NMS_TOPK_TRAIN: 12000<br>SMOOTH_L1_BETA: 0.0<br>SEM_SEG_HEAD:<br>COMMON_STRIDE: 4<br>CONVS_DIM: 128<br>IGNORE_VALUE: 255<br>IN_FEATURES:</li><li>p2</li><li>p3</li><li>p4</li><li>p5<br>LOSS_WEIGHT: 1.0<br>NAME: SemSegFPNHead<br>NORM: GN<br>NUM_CLASSES: 54<br>WEIGHTS: &#x2F;path&#x2F;to&#x2F;checkpoint_file<br>YOLOF:<br>BOX_TRANSFORM:<br>ADD_CTR_CLAMP: true<br>BBOX_REG_WEIGHTS:<ul><li>1.0</li><li>1.0</li><li>1.0</li><li>1.0<br>CTR_CLAMP: 32<br>DECODER:<br>ACTIVATION: ReLU<br>CLS_NUM_CONVS: 2<br>IN_CHANNELS: 512<br>NORM: BN<br>NUM_ANCHORS: 5<br>NUM_CLASSES: 80<br>PRIOR_PROB: 0.01<br>REG_NUM_CONVS: 4<br>DETECTIONS_PER_IMAGE: 100<br>ENCODER:<br>ACTIVATION: ReLU<br>BACKBONE_LEVEL: res5<br>BLOCK_DILATIONS:</li><li>2</li><li>4</li><li>6</li><li>8<br>BLOCK_MID_CHANNELS: 128<br>IN_CHANNELS: 2048<br>NORM: BN<br>NUM_CHANNELS: 512<br>NUM_RESIDUAL_BLOCKS: 4<br>LOSSES:<br>BBOX_REG_LOSS_TYPE: giou<br>FOCAL_LOSS_ALPHA: 0.25<br>FOCAL_LOSS_GAMMA: 2.0<br>MATCHER:<br>TOPK: 4<br>NEG_IGNORE_THRESHOLD: 0.7<br>NMS_THRESH_TEST: 0.6<br>POS_IGNORE_THRESHOLD: 0.15<br>SCORE_THRESH_TEST: 0.05<br>TOPK_CANDIDATES_TEST: 1000<br>OUTPUT_DIR: output&#x2F;yolof&#x2F;R_50_C5_1x<br>SEED: -1<br>SOLVER:<br>AMP:<br>ENABLED: false<br>BACKBONE_MULTIPLIER: 0.334<br>BASE_LR: 0.12<br>BIAS_LR_FACTOR: 1.0<br>CHECKPOINT_PERIOD: 2500<br>CLIP_GRADIENTS:<br>CLIP_TYPE: value<br>CLIP_VALUE: 1.0<br>ENABLED: false<br>NORM_TYPE: 2.0<br>GAMMA: 0.1<br>IMS_PER_BATCH: 64<br>LR_SCHEDULER_NAME: WarmupMultiStepLR<br>MAX_ITER: 22500<br>MOMENTUM: 0.9<br>NESTEROV: false<br>REFERENCE_WORLD_SIZE: 0<br>STEPS:</li></ul></li></ul></li><li>15000</li><li>20000<br>WARMUP_FACTOR: 0.00066667<br>WARMUP_ITERS: 1500<br>WARMUP_METHOD: linear<br>WEIGHT_DECAY: 0.0001<br>WEIGHT_DECAY_BIAS: 0.0001<br>WEIGHT_DECAY_NORM: 0.0<br>TEST:<br>AUG:<br>ENABLED: false<br>FLIP: true<br>MAX_SIZE: 4000<br>MIN_SIZES:<ul><li>400</li><li>500</li><li>600</li><li>700</li><li>800</li><li>900</li><li>1000</li><li>1100</li><li>1200<br>DETECTIONS_PER_IMAGE: 100<br>EVAL_PERIOD: 0<br>EXPECTED_RESULTS: []<br>KEYPOINT_OKS_SIGMAS: []<br>PRECISE_BN:<br>ENABLED: false<br>NUM_ITER: 200<br>VERSION: 2<br>VIS_PERIOD: 0</li></ul></li></ul><p>[04&#x2F;23 11:13:47 detectron2]: Full config saved to output&#x2F;yolof&#x2F;R_50_C5_1x\config.yaml<br>[04&#x2F;23 11:13:47 d2.utils.env]: Using a generated random seed 47610603<br>[04&#x2F;23 11:13:53 d2.engine.defaults]: Model:<br>YOLOF(<br>  (backbone): ResNet(<br>    (stem): BasicStem(<br>      (conv1): Conv2d(<br>        3, 64, kernel_size&#x3D;(7, 7), stride&#x3D;(2, 2), padding&#x3D;(3, 3), bias&#x3D;False<br>        (norm): FrozenBatchNorm2d(num_features&#x3D;64, eps&#x3D;1e-05)<br>      )<br>    )<br>    (res2): Sequential(<br>      (0): BottleneckBlock(<br>        (shortcut): Conv2d(<br>          64, 256, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False<br>          (norm): FrozenBatchNorm2d(num_features&#x3D;256, eps&#x3D;1e-05)<br>        )<br>        (conv1): Conv2d(<br>          64, 64, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False<br>          (norm): FrozenBatchNorm2d(num_features&#x3D;64, eps&#x3D;1e-05)<br>        )<br>        (conv2): Conv2d(<br>          64, 64, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False<br>          (norm): FrozenBatchNorm2d(num_features&#x3D;64, eps&#x3D;1e-05)<br>        )<br>        (conv3): Conv2d(<br>          64, 256, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False<br>          (norm): FrozenBatchNorm2d(num_features&#x3D;256, eps&#x3D;1e-05)<br>        )<br>      )<br>      (1): BottleneckBlock(<br>        (conv1): Conv2d(<br>          256, 64, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False<br>          (norm): FrozenBatchNorm2d(num_features&#x3D;64, eps&#x3D;1e-05)<br>        )<br>        (conv2): Conv2d(<br>          64, 64, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False<br>          (norm): FrozenBatchNorm2d(num_features&#x3D;64, eps&#x3D;1e-05)<br>        )<br>        (conv3): Conv2d(<br>          64, 256, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False<br>          (norm): FrozenBatchNorm2d(num_features&#x3D;256, eps&#x3D;1e-05)<br>        )<br>      )<br>      (2): BottleneckBlock(<br>        (conv1): Conv2d(<br>          256, 64, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False<br>          (norm): FrozenBatchNorm2d(num_features&#x3D;64, eps&#x3D;1e-05)<br>        )<br>        (conv2): Conv2d(<br>          64, 64, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False<br>          (norm): FrozenBatchNorm2d(num_features&#x3D;64, eps&#x3D;1e-05)<br>        )<br>        (conv3): Conv2d(<br>          64, 256, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False<br>          (norm): FrozenBatchNorm2d(num_features&#x3D;256, eps&#x3D;1e-05)<br>        )<br>      )<br>    )<br>    (res3): Sequential(<br>      (0): BottleneckBlock(<br>        (shortcut): Conv2d(<br>          256, 512, kernel_size&#x3D;(1, 1), stride&#x3D;(2, 2), bias&#x3D;False<br>          (norm): FrozenBatchNorm2d(num_features&#x3D;512, eps&#x3D;1e-05)<br>        )<br>        (conv1): Conv2d(<br>          256, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(2, 2), bias&#x3D;False<br>          (norm): FrozenBatchNorm2d(num_features&#x3D;128, eps&#x3D;1e-05)<br>        )<br>        (conv2): Conv2d(<br>          128, 128, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False<br>          (norm): FrozenBatchNorm2d(num_features&#x3D;128, eps&#x3D;1e-05)<br>        )<br>        (conv3): Conv2d(<br>          128, 512, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False<br>          (norm): FrozenBatchNorm2d(num_features&#x3D;512, eps&#x3D;1e-05)<br>        )<br>      )<br>      (1): BottleneckBlock(<br>        (conv1): Conv2d(<br>          512, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False<br>          (norm): FrozenBatchNorm2d(num_features&#x3D;128, eps&#x3D;1e-05)<br>        )<br>        (conv2): Conv2d(<br>          128, 128, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False<br>          (norm): FrozenBatchNorm2d(num_features&#x3D;128, eps&#x3D;1e-05)<br>        )<br>        (conv3): Conv2d(<br>          128, 512, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False<br>          (norm): FrozenBatchNorm2d(num_features&#x3D;512, eps&#x3D;1e-05)<br>        )<br>      )<br>      (2): BottleneckBlock(<br>        (conv1): Conv2d(<br>          512, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False<br>          (norm): FrozenBatchNorm2d(num_features&#x3D;128, eps&#x3D;1e-05)<br>        )<br>        (conv2): Conv2d(<br>          128, 128, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False<br>          (norm): FrozenBatchNorm2d(num_features&#x3D;128, eps&#x3D;1e-05)<br>        )<br>        (conv3): Conv2d(<br>          128, 512, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False<br>          (norm): FrozenBatchNorm2d(num_features&#x3D;512, eps&#x3D;1e-05)<br>        )<br>      )<br>      (3): BottleneckBlock(<br>        (conv1): Conv2d(<br>          512, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False<br>          (norm): FrozenBatchNorm2d(num_features&#x3D;128, eps&#x3D;1e-05)<br>        )<br>        (conv2): Conv2d(<br>          128, 128, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False<br>          (norm): FrozenBatchNorm2d(num_features&#x3D;128, eps&#x3D;1e-05)<br>        )<br>        (conv3): Conv2d(<br>          128, 512, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False<br>          (norm): FrozenBatchNorm2d(num_features&#x3D;512, eps&#x3D;1e-05)<br>        )<br>      )<br>    )<br>    (res4): Sequential(<br>      (0): BottleneckBlock(<br>        (shortcut): Conv2d(<br>          512, 1024, kernel_size&#x3D;(1, 1), stride&#x3D;(2, 2), bias&#x3D;False<br>          (norm): FrozenBatchNorm2d(num_features&#x3D;1024, eps&#x3D;1e-05)<br>        )<br>        (conv1): Conv2d(<br>          512, 256, kernel_size&#x3D;(1, 1), stride&#x3D;(2, 2), bias&#x3D;False<br>          (norm): FrozenBatchNorm2d(num_features&#x3D;256, eps&#x3D;1e-05)<br>        )<br>        (conv2): Conv2d(<br>          256, 256, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False<br>          (norm): FrozenBatchNorm2d(num_features&#x3D;256, eps&#x3D;1e-05)<br>        )<br>        (conv3): Conv2d(<br>          256, 1024, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False<br>          (norm): FrozenBatchNorm2d(num_features&#x3D;1024, eps&#x3D;1e-05)<br>        )<br>      )<br>      (1): BottleneckBlock(<br>        (conv1): Conv2d(<br>          1024, 256, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False<br>          (norm): FrozenBatchNorm2d(num_features&#x3D;256, eps&#x3D;1e-05)<br>        )<br>        (conv2): Conv2d(<br>          256, 256, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False<br>          (norm): FrozenBatchNorm2d(num_features&#x3D;256, eps&#x3D;1e-05)<br>        )<br>        (conv3): Conv2d(<br>          256, 1024, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False<br>          (norm): FrozenBatchNorm2d(num_features&#x3D;1024, eps&#x3D;1e-05)<br>        )<br>      )<br>      (2): BottleneckBlock(<br>        (conv1): Conv2d(<br>          1024, 256, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False<br>          (norm): FrozenBatchNorm2d(num_features&#x3D;256, eps&#x3D;1e-05)<br>        )<br>        (conv2): Conv2d(<br>          256, 256, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False<br>          (norm): FrozenBatchNorm2d(num_features&#x3D;256, eps&#x3D;1e-05)<br>        )<br>        (conv3): Conv2d(<br>          256, 1024, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False<br>          (norm): FrozenBatchNorm2d(num_features&#x3D;1024, eps&#x3D;1e-05)<br>        )<br>      )<br>      (3): BottleneckBlock(<br>        (conv1): Conv2d(<br>          1024, 256, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False<br>          (norm): FrozenBatchNorm2d(num_features&#x3D;256, eps&#x3D;1e-05)<br>        )<br>        (conv2): Conv2d(<br>          256, 256, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False<br>          (norm): FrozenBatchNorm2d(num_features&#x3D;256, eps&#x3D;1e-05)<br>        )<br>        (conv3): Conv2d(<br>          256, 1024, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False<br>          (norm): FrozenBatchNorm2d(num_features&#x3D;1024, eps&#x3D;1e-05)<br>        )<br>      )<br>      (4): BottleneckBlock(<br>        (conv1): Conv2d(<br>          1024, 256, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False<br>          (norm): FrozenBatchNorm2d(num_features&#x3D;256, eps&#x3D;1e-05)<br>        )<br>        (conv2): Conv2d(<br>          256, 256, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False<br>          (norm): FrozenBatchNorm2d(num_features&#x3D;256, eps&#x3D;1e-05)<br>        )<br>        (conv3): Conv2d(<br>          256, 1024, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False<br>          (norm): FrozenBatchNorm2d(num_features&#x3D;1024, eps&#x3D;1e-05)<br>        )<br>      )<br>      (5): BottleneckBlock(<br>        (conv1): Conv2d(<br>          1024, 256, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False<br>          (norm): FrozenBatchNorm2d(num_features&#x3D;256, eps&#x3D;1e-05)<br>        )<br>        (conv2): Conv2d(<br>          256, 256, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False<br>          (norm): FrozenBatchNorm2d(num_features&#x3D;256, eps&#x3D;1e-05)<br>        )<br>        (conv3): Conv2d(<br>          256, 1024, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False<br>          (norm): FrozenBatchNorm2d(num_features&#x3D;1024, eps&#x3D;1e-05)<br>        )<br>      )<br>    )<br>    (res5): Sequential(<br>      (0): BottleneckBlock(<br>        (shortcut): Conv2d(<br>          1024, 2048, kernel_size&#x3D;(1, 1), stride&#x3D;(2, 2), bias&#x3D;False<br>          (norm): FrozenBatchNorm2d(num_features&#x3D;2048, eps&#x3D;1e-05)<br>        )<br>        (conv1): Conv2d(<br>          1024, 512, kernel_size&#x3D;(1, 1), stride&#x3D;(2, 2), bias&#x3D;False<br>          (norm): FrozenBatchNorm2d(num_features&#x3D;512, eps&#x3D;1e-05)<br>        )<br>        (conv2): Conv2d(<br>          512, 512, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False<br>          (norm): FrozenBatchNorm2d(num_features&#x3D;512, eps&#x3D;1e-05)<br>        )<br>        (conv3): Conv2d(<br>          512, 2048, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False<br>          (norm): FrozenBatchNorm2d(num_features&#x3D;2048, eps&#x3D;1e-05)<br>        )<br>      )<br>      (1): BottleneckBlock(<br>        (conv1): Conv2d(<br>          2048, 512, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False<br>          (norm): FrozenBatchNorm2d(num_features&#x3D;512, eps&#x3D;1e-05)<br>        )<br>        (conv2): Conv2d(<br>          512, 512, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False<br>          (norm): FrozenBatchNorm2d(num_features&#x3D;512, eps&#x3D;1e-05)<br>        )<br>        (conv3): Conv2d(<br>          512, 2048, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False<br>          (norm): FrozenBatchNorm2d(num_features&#x3D;2048, eps&#x3D;1e-05)<br>        )<br>      )<br>      (2): BottleneckBlock(<br>        (conv1): Conv2d(<br>          2048, 512, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False<br>          (norm): FrozenBatchNorm2d(num_features&#x3D;512, eps&#x3D;1e-05)<br>        )<br>        (conv2): Conv2d(<br>          512, 512, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False<br>          (norm): FrozenBatchNorm2d(num_features&#x3D;512, eps&#x3D;1e-05)<br>        )<br>        (conv3): Conv2d(<br>          512, 2048, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False<br>          (norm): FrozenBatchNorm2d(num_features&#x3D;2048, eps&#x3D;1e-05)<br>        )<br>      )<br>    )<br>  )<br>  (encoder): DilatedEncoder(<br>    (lateral_conv): Conv2d(2048, 512, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1))<br>    (lateral_norm): BatchNorm2d(512, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)<br>    (fpn_conv): Conv2d(512, 512, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1))<br>    (fpn_norm): BatchNorm2d(512, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)<br>    (dilated_encoder_blocks): Sequential(<br>      (0): Bottleneck(<br>        (conv1): Sequential(<br>          (0): Conv2d(512, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1))<br>          (1): BatchNorm2d(128, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)<br>          (2): ReLU(inplace&#x3D;True)<br>        )<br>        (conv2): Sequential(<br>          (0): Conv2d(128, 128, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(2, 2), dilation&#x3D;(2, 2))<br>          (1): BatchNorm2d(128, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)<br>          (2): ReLU(inplace&#x3D;True)<br>        )<br>        (conv3): Sequential(<br>          (0): Conv2d(128, 512, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1))<br>          (1): BatchNorm2d(512, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)<br>          (2): ReLU(inplace&#x3D;True)<br>        )<br>      )<br>      (1): Bottleneck(<br>        (conv1): Sequential(<br>          (0): Conv2d(512, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1))<br>          (1): BatchNorm2d(128, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)<br>          (2): ReLU(inplace&#x3D;True)<br>        )<br>        (conv2): Sequential(<br>          (0): Conv2d(128, 128, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(4, 4), dilation&#x3D;(4, 4))<br>          (1): BatchNorm2d(128, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)<br>          (2): ReLU(inplace&#x3D;True)<br>        )<br>        (conv3): Sequential(<br>          (0): Conv2d(128, 512, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1))<br>          (1): BatchNorm2d(512, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)<br>          (2): ReLU(inplace&#x3D;True)<br>        )<br>      )<br>      (2): Bottleneck(<br>        (conv1): Sequential(<br>          (0): Conv2d(512, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1))<br>          (1): BatchNorm2d(128, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)<br>          (2): ReLU(inplace&#x3D;True)<br>        )<br>        (conv2): Sequential(<br>          (0): Conv2d(128, 128, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(6, 6), dilation&#x3D;(6, 6))<br>          (1): BatchNorm2d(128, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)<br>          (2): ReLU(inplace&#x3D;True)<br>        )<br>        (conv3): Sequential(<br>          (0): Conv2d(128, 512, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1))<br>          (1): BatchNorm2d(512, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)<br>          (2): ReLU(inplace&#x3D;True)<br>        )<br>      )<br>      (3): Bottleneck(<br>        (conv1): Sequential(<br>          (0): Conv2d(512, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1))<br>          (1): BatchNorm2d(128, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)<br>          (2): ReLU(inplace&#x3D;True)<br>        )<br>        (conv2): Sequential(<br>          (0): Conv2d(128, 128, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(8, 8), dilation&#x3D;(8, 8))<br>          (1): BatchNorm2d(128, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)<br>          (2): ReLU(inplace&#x3D;True)<br>        )<br>        (conv3): Sequential(<br>          (0): Conv2d(128, 512, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1))<br>          (1): BatchNorm2d(512, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)<br>          (2): ReLU(inplace&#x3D;True)<br>        )<br>      )<br>    )<br>  )<br>  (decoder): Decoder(<br>    (cls_subnet): Sequential(<br>      (0): Conv2d(512, 512, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1))<br>      (1): BatchNorm2d(512, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)<br>      (2): ReLU(inplace&#x3D;True)<br>      (3): Conv2d(512, 512, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1))<br>      (4): BatchNorm2d(512, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)<br>      (5): ReLU(inplace&#x3D;True)<br>    )<br>    (bbox_subnet): Sequential(<br>      (0): Conv2d(512, 512, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1))<br>      (1): BatchNorm2d(512, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)<br>      (2): ReLU(inplace&#x3D;True)<br>      (3): Conv2d(512, 512, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1))<br>      (4): BatchNorm2d(512, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)<br>      (5): ReLU(inplace&#x3D;True)<br>      (6): Conv2d(512, 512, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1))<br>      (7): BatchNorm2d(512, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)<br>      (8): ReLU(inplace&#x3D;True)<br>      (9): Conv2d(512, 512, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1))<br>      (10): BatchNorm2d(512, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)<br>      (11): ReLU(inplace&#x3D;True)<br>    )<br>    (cls_score): Conv2d(512, 400, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1))<br>    (bbox_pred): Conv2d(512, 20, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1))<br>    (object_pred): Conv2d(512, 5, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1))<br>  )<br>  (anchor_generator): DefaultAnchorGenerator(<br>    return self.load(path, checkpointables&#x3D;[])<br>  File “e:\anacondalib\detectron2-master\detectron2-master\detectron2\checkpoint\detection_checkpoint.py”, line 53, in load<br>    ret &#x3D; super().load(path, *args, **kwargs)<br>  File “D:\Anaconda3\envs\detectron2\lib\site-packages\fvcore\common\checkpoint.py”, line 153, in load<br>    assert os.path.isfile(path), “Checkpoint {} not found!”.format(path)<br>AssertionError: Checkpoint &#x2F;path&#x2F;to&#x2F;checkpoint_file not found!<br>接下来我们逐步实现yolof能够跑起来<br>首先我们安装labelme<br>这一部分参考来自(84条消息) Labelme安装及使用教程_Marlowee的博客-CSDN博客_labelme安装 <a href="https://blog.csdn.net/weixin_43427721/article/details/107122775">https://blog.csdn.net/weixin_43427721/article/details/107122775</a>    “(84条消息) Labelme安装及使用教程_Marlowee的博客-CSDN博客_labelme安装”</p><h2 id="7-1报错与解决"><a href="#7-1报错与解决" class="headerlink" title="7.1报错与解决"></a>7.1报错与解决</h2><p>报错：KeyError: “Dataset ‘test’ is not registered! Available datasets are: coco_2014_train, coco_2014_val, coco_2014_minival, coco_2014_minival_100, coco_2014_valminusminival, coco_2017_tra<br>in, coco_2017_val, coco_2017_test, coco_2017_test-dev, coco_2017_val_100, keypoints_coco_2014_train, keypoints_coco_2014_val, keypoints_coco_2014_minival, keypoints_coco_2014_valminus<br>minival, keypoints_coco_2014_minival_100, keypoints_coco_2017_train, keypoints_coco_2017_val, keypoints_coco_2017_val_100, coco_2017_train_panoptic_separated, coco_2017_train_panoptic<br>_stuffonly, coco_2017_train_panoptic, coco_2017_val_panoptic_separated, coco_2017_val_panoptic_stuffonly, coco_2017_val_panoptic, coco_2017_val_100_panoptic_separated, coco_2017_val_1<br>00_panoptic_stuffonly, coco_2017_val_100_panoptic, lvis_v1_train, lvis_v1_val, lvis_v1_test_dev, lvis_v1_test_challenge, lvis_v0.5_train, lvis_v0.5_val, lvis_v0.5_val_rand_100, lvis_v<br>0.5_test, lvis_v0.5_train_cocofied, lvis_v0.5_val_cocofied, cityscapes_fine_instance_seg_train, cityscapes_fine_sem_seg_train, cityscapes_fine_instance_seg_val, cityscapes_fine_sem_se<br>g_val, cityscapes_fine_instance_seg_test, cityscapes_fine_sem_seg_test, cityscapes_fine_panoptic_train, cityscapes_fine_panoptic_val, voc_2007_trainval, voc_2007_train, voc_2007_val, voc_2007_test, voc_2012_trainval, voc_2012_train, voc_2012_val, ade20k_sem_seg_train, ade20k_sem_seg_val”</p><p><strong>报错OS Error</strong><br>OSError: [WinError 1455] 页面文件太小，无法完成操作。</p><p><strong>解决方法：</strong><br>dataloader.test &#x3D; L(build_data_loader)(<br>    dataset&#x3D;L(torchvision.datasets.ImageNet)(<br>        root&#x3D;”${…train.dataset.root}”,<br>        split&#x3D;”val”,<br>        transform&#x3D;L(T.Compose)(<br>            transforms&#x3D;[<br>                L(T.Resize)(size&#x3D;256),<br>                L(T.CenterCrop)(size&#x3D;224),<br>                T.ToTensor(),<br>                L(T.Normalize)(mean&#x3D;(0.485, 0.456, 0.406), std&#x3D;(0.229, 0.224, 0.225)),<br>            ]<br>        ),<br>    ),<br>    batch_size&#x3D;256 &#x2F;&#x2F; 8,<br>    num_workers&#x3D;4, #修改此处的num_workers&#x3D;0即可<br>    training&#x3D;False,<br>)</p><p><strong>其他报错：</strong><br>[04&#x2F;25 16:25:24 d2.engine.train_loop]: Starting training from iteration 0<br>Install mish-cuda to speed up training and inference. More importantly, replace the naive Mish with MishCuda will give a ~1.5G memory saving during training.<br>Traceback (most recent call last):<br>  File “<string>“, line 1, in <module><br>  File “D:\Anaconda3\envs\detectron2\lib\multiprocessing\spawn.py”, line 116, in spawn_main<br>    exitcode &#x3D; _main(fd, parent_sentinel)<br>  File “D:\Anaconda3\envs\detectron2\lib\multiprocessing\spawn.py”, line 125, in _main<br>    prepare(preparation_data)<br>  File “D:\Anaconda3\envs\detectron2\lib\multiprocessing\spawn.py”, line 236, in prepare<br>    _fixup_main_from_path(data[‘init_main_from_path’])<br>  File “D:\Anaconda3\envs\detectron2\lib\multiprocessing\spawn.py”, line 287, in _fixup_main_from_path<br>    main_content &#x3D; runpy.run_path(main_path,<br>  File “D:\Anaconda3\envs\detectron2\lib\runpy.py”, line 265, in run_path<br>    return _run_module_code(code, init_globals, run_name,<br>  File “D:\Anaconda3\envs\detectron2\lib\runpy.py”, line 97, in _run_module_code<br>    _run_code(code, mod_globals, init_globals,<br>  File “D:\Anaconda3\envs\detectron2\lib\runpy.py”, line 87, in _run_code<br>    exec(code, run_globals)<br>  File “E:\anacondalib\detectron2\YOLOF-master\YOLOF-master\tools\train_net.py”, line 47, in <module><br>    from torch.contrib._tensorboard_vis import visualize<br>  File “D:\Anaconda3\envs\detectron2\lib\site-packages\torch\contrib_tensorboard_vis.py”, line 17, in <module><br>    raise ImportError(“TensorBoard visualization of GraphExecutors requires having “<br><strong>ImportError: TensorBoard visualization of GraphExecutors requires having TensorFlow installed</strong></p><h1 id="现阶段-当前-遇到的问题："><a href="#现阶段-当前-遇到的问题：" class="headerlink" title="现阶段(当前)遇到的问题："></a>现阶段(当前)遇到的问题：</h1><p>yolof可以跑起来，但是目前的问题在于识别结果框的visualize可视化出了一些问题，yolof这一块跑起来以后目前无法做出成功的可视化框。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>索尼微单编年史</title>
      <link href="/2022/04/06/sony1/"/>
      <url>/2022/04/06/sony1/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>作为资深的索粉，在每天顶礼索尼大法好的同时，是否知道索尼品牌是如何一路高歌，最终登顶销量榜首的呢？ </p><p>今天我们就盘点下，从第一台微单“奶昔”开始，索尼是如何一步一步成为行业巨头的。 </p><p><strong>前传</strong> </p><p>2006年开始，索尼接手了柯尼卡美能达品牌 镜头卡口系统，开始研发数码单反相机，并沿用了美能达经典的“α”（阿尔法）品牌，并于同年发布了首款数码单反相机，α100。 </p><p><img src="https://pic4.zhimg.com/80/v2-d19d9066b84e31d97237614c549b4c6f_r.jpg"></p><p>索尼数码单反相机的开端α100 </p><p>从2006年到2010年，索尼先后推出了数十款数码单反相机，α700，α200，α900，α850等等，几乎把好看的三位数字都用上了，期间还推出了惊艳的蔡司 Vario Sonnar T*镜头和机身防抖技术，为日后机身防抖功能的普及埋下了伏笔，同时也开启了欧洲老牌镜头和先进技术厂商的嫁接之路（可惜后来这些老家伙都去玩手机了）。 </p><p>但索尼的单反之路并不顺利，相机的生产需要大量行业经验的积累和对摄影师充分的了解，作为一个年轻的相机品牌，索尼很显然是缺乏这些的。 </p><p><strong>开端</strong> </p><p><strong>2010年</strong> ， 索尼正式出品了第一款无反相机——NEX-5，也就是历史闻名的“奶昔”系列，主打“将数码可更换镜头相机装入口袋”的理念，同时开辟了无反相机的赛道，为日后雄踞榜首打下了坚实基础。 </p><p><img src="https://pica.zhimg.com/80/v2-ae19cea0a4fb4f776483da7c8a184edf_r.jpg"></p><p>索尼NEX-5，老骚本人有一台，现在依然是可以用的 </p><p><strong>2011年</strong> ，索尼发布了NEX-5的后续版本，NEX-5N以及主打入门级的NEX-3N，同时，索尼推出了旗舰级无反NEX-7，坚定了索尼力推无反相机、和佳尼错位竞争的决心。 </p><p><img src="https://pic2.zhimg.com/80/v2-764e58afc6c477386f5bddfec1aef679_r.jpg"></p><p>索尼NEX-7（相对于5而言，像素多了10mp，升级了传感器，机身变为金属，增加了很多专业的操控模式） </p><p><strong>2012年</strong> ，索尼发布了NEX-5的第三代版本NEX-5R和入门级3N的后续版本F3，并推出中端无反NEX-6，NEX-6相比于NEX-7，其对焦速度和对焦精度有所提升。 </p><p>此时，索尼仍没有引起佳能和尼康的注意，他们在这一年都推出了自己的准旗舰级的数码单反产品佳能5D3，尼康D800，索尼还显得比较孱弱。 </p><p><strong>发展</strong> </p><p><strong>2013年，索尼正式推出了世界上第一台全画幅无反——α7，标志着索尼无反正式进入了“7”时代。</strong> </p><p><strong>同年发布高像素版本的α7R，主打风光、商业。</strong> </p><p><img src="https://pic2.zhimg.com/80/v2-61e44fb72f047ae29b545c101e638f4b_r.jpg"></p><p>索尼“7”时代的开山之作，α7一代。但相机哪那么好做呢？这个机器的对焦和操控被各种吐槽，并不算有竞争力的产品 </p><p><strong>2014年</strong> ，发布奶昔后时代的继承者α5000、α5100、α6000，首次采用了4D对焦技术，大幅提高了对焦精度和对焦速度。 </p><p>同年也发布了低像素、高ISO版本的全画幅无反α7S，主打视频拍摄，至此，索尼全画幅无反正式建立了三条不同的产品线——主打均衡的α7系列、主打高像素高画质的α7R系列以及主打视频的α7S系列。同时推出了α7的二代版本——α7Ⅱ，加入了五轴防抖技术。 </p><p><strong>索尼大法，獠牙已现。</strong> </p><p><img src="https://pic1.zhimg.com/80/v2-a5802f85c3170a0c429599bece223299_r.jpg"></p><p>索尼的α7II才算是大法系真正的初代大魔王，新的传感器，5轴防抖，加上当时来看很强大的视频功能等等，都是专业级的呈现 </p><p><strong>2015年</strong> ，索尼发布α7R的二代版本——α7RⅡ，4200万像素刷新了当年相机市场的像素记录，4D对焦技术、五轴防抖技术、4K视频、三防等等——索尼集所有技术于大成打造了当年全新的旗舰α7RⅡ，无反相机正式开始挑战单反相机的地位。 </p><p>时至今日α7RⅡ仍然是高像素摄影的选择之一。 </p><p><img src="https://pic2.zhimg.com/80/v2-b9834d03256271bb1224ef1649ef6c99_r.jpg"></p><p>索尼二代大魔王A7RII，4200万的像素，相位对焦，眼部对焦，4K视频，现在看起来技术都毫不过时，毕竟现在某些厂商连相位对焦都没有 </p><p><strong>2016年</strong> ，发布了APS-C次旗舰α6300、旗舰α6500，首次将五轴防抖技术下放到APS-C画幅相机上。 </p><p><img src="https://pic3.zhimg.com/80/v2-5c1b5574fe157a9ba536e729ead3b5cb_r.jpg"></p><p>索尼α6xxx系列的相机，vlog神器 </p><p><strong>2017年</strong> ，发布了α7R的第三代版本——α7RⅢ，改进了连拍、续航。同年，索尼发布了全新的全画幅相机系列——α9，主打体育、生态类摄影， 首次采用了堆栈式传感器 ，连拍速度达到20张&#x2F;秒，各项参数均对飙同期的佳能1DXmarkⅡ和尼康D5。 </p><p><strong>成熟</strong> </p><p><strong>2018年</strong> ，发布了α7的第三代版本——α7Ⅲ，重新定义了均衡机型，大幅改进的693相位+435对比对焦系统、10张&#x2F;秒连拍等等，索尼下放了大部分α9的技术用于该机型。 </p><p><strong>索尼大法，功成圆满，直接可以与佳能尼康成三足鼎立之势</strong> 。 </p><p><img src="https://pica.zhimg.com/80/v2-96909c2b79258098bd102625af7bae40_r.jpg"></p><p>索尼α7进化到了三代目，算是正式追平了佳能尼康等一众大厂，索尼大法，功德圆满，就是后脑勺稍微丑点 </p><p>以后就是大家熟悉的剧情， <strong>强大的索尼α7四代目横空出世！</strong> </p><p><strong>2019年</strong> ，发布了α7R的第四代版本——α7RⅣ，采用了全新的6100万像素传感器，加入了像素转换合成拍摄。 </p><p><img src="https://pic3.zhimg.com/80/v2-d18260b32ee246ad27464127c8adc16a_r.jpg"></p><p>索尼a7RIV，6100w像素，相机里的柳岩，太性感了 </p><p>同年发布了α9的第二版本——α9Ⅱ，加入了机械快门连拍，速度达到10张&#x2F;秒，增加了电池续航以及连拍张数。 </p><p><img src="https://pic1.zhimg.com/80/v2-06ad5b17010e87a76da2e1063f3e7d24_r.jpg"></p><p>定位为“体育机”的索尼α9II </p><p><strong>2020年</strong> ，发布了α7S的第三代版本——α7SⅢ，加入了4K120P视频录制、全新的S-Cinetone*色彩模式， </p><p>由索尼FX9电影机下放以及专业电影级别色彩配置S-Gamut、S-Gamut3和S-Gamut3.Cine。 </p><p>同年，索尼发布了入门级别全画幅无反——α7C，采用了类似α6000系列的外观造型，但内核为全画幅传感器，轻便小巧，功能强大。 </p><p><img src="https://pic4.zhimg.com/80/v2-96619bb9df0903b4db1056c9076c1fbe_r.jpg"></p><p>索尼α7SIII是一个为视频而生的机器 </p><p><strong>2021年</strong> ，发布了全新的旗舰级无反——α1，融合了当今索尼最顶级的技术——5000万像素传感器，全像素约30张&#x2F;秒的无黑屏连拍以及以约120次&#x2F;秒的连续对焦和曝光计算、759个相位对焦点、8K30P拍摄等等，各类摄影一网打尽，真正的旗舰！ </p><p><img src="https://pica.zhimg.com/80/v2-6f0fec14cecd9e21ea6ab04ad3397409_r.jpg"></p><p>索尼α1，大法的光辉！ </p><p>同年，发布了α7的第四代版本——α7Ⅳ，采用了全新的3300万像素传感器、759个相位对焦点、首次加入了4K60P视频拍摄功能。 </p><p><img src="https://pic4.zhimg.com/80/v2-8e7af4f408d9c26412b892e1d0ed12f2_r.jpg"></p><p>索尼α7四代目，精壮的汉子人人爱 </p><p>目前，索尼相机拿到了北美销冠，今年二月份日本零售端报告依然是销冠。 </p><p>今后，索尼依然会面对佳能、尼康等强大对手的挑战，但他前进的步伐不会被阻挡。 </p><p>索尼象征的是一股年轻的力量，是不断挑战强者的决心！ </p><p>在将来，索尼大法依然会熠熠闪耀在影像科技的巅峰！ </p>]]></content>
      
      
      <categories>
          
          <category> sony </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 索尼，相机，微单，encrypted </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>初学入门YOLOv5手势识别之制作并训练自己的数据集</title>
      <link href="/2022/04/01/yolome/"/>
      <url>/2022/04/01/yolome/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p><img src="https://img-blog.csdnimg.cn/9fb64f5821fd4e30bac2ca88e2b57c84.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAd2NjbGxsbGxsbGxsbA==,size_20,color_FFFFFF,t_70,g_se,x_16"></p><p>随着短视频vlog时代的到来，自动驾驶技术、人脸识别门禁系统、智慧视频监控、AI机器人等贴近人们日常生活的视频信息量的暴增，视频目标检测的研究具有无比的现实研究意义与未来行业潜力。视频是由一系列具有时间连续性和内容相关性的图像组成，所以关于视频目标检测的研究自兴起以来就是在经典的图像目标检测算法的基础上进行改进与创新的。软硬件设备的迭代更新，使得视频的流畅度也越来越高，几秒钟的视频画面便可包含高达两三百甚至上千张图像，而视频比单纯的图像包含更多的时间和空间信息，若直接用图像目标检测的方法对视频文件的内容逐帧检测，不仅忽视了视频的时空信息还会拖慢检测速度，难以达到实时的需求。如何利用视频提供的时空上下文信息提升检测的准确率、速度等性能，成为了各国研究人员的工作重点。目标检测 (Object Detection) 是计算机视觉和图像处理的一项分支技术，其主要任务是在一幅数字图像中正确识别出目标物体的位置并判断类别。目标检测算法需要框选出图片中的物体，并判断出框选出的物体是什么以及它是否可信。</p><p>得益于近年来 GPU 加速技术和深度学习技术的发展，使得如今基于深度学习的人脸检测能够达到高精度和较好的实时性，有效的改善传统方法的效率低下问题，能够使该项技术广泛应用于医学军事领域等。</p><p>YOLOv5是一种单阶段目标检测算法，该算法在YOLOv4的基础上添加了一些新的改进思路，使其速度与精度都得到了较大的性能提升。YOLO 是一种卷积神经网络，相较于传统神经网络，卷积神经网络能够更好的提取特征，同时还能减少模型参数。</p><p>YOLOv5并不是一个单独的模型，而是一个模型家族，包括了YOLOv5s、YOLOv5m、YOLOv5l、YOLOv5x、YOLOv5x+TTA，这点有点儿像EfficientDet。由于没有找到V5的论文，只能从代码去学习它。总体上它和YOLOV4差不多，可以认为是YOLOV4的加强版。</p><p>对于yolov4而言yolov5的主要的改进思路如下所示：</p><p>输入端：在模型训练阶段，提出了一些改进思路，主要包括Mosaic（马赛克）数据增强、自适应锚框计算、自适应图片缩放；</p><p>基准网络：融合其它检测算法中的一些新思路，主要包括：Focus结构与CSP结构；</p><p>Neck网络：目标检测网络在BackBone与最后的Head输出层之间往往会插入一些层，Yolov5中添加了FPN+PAN结构；</p><p>Head输出层：输出层的锚框机制与YOLOv4相同，主要改进的是训练时的损失函数GIOU_Loss，以及预测框筛选的DIOU_nms。</p><p>   以下篇幅主要描述了我自己手动标注数据集并将其训练的过程。</p><p>一. 环境搭建（不能含有中文路径）<br>开始之前<br>克隆存储库和安装要求.txt在 Python&gt;&#x3D;3.7.0 环境中，包括 PyTorch&gt;&#x3D;1.7。模型和数据集从最新的 YOLOv5 版本自动下载。</p><p>git clone <a href="https://github.com/ultralytics/yolov5">https://github.com/ultralytics/yolov5</a>  # clone<br>cd yolov5<br>pip install -r requirements.txt  # install</p><p><img src="https://img-blog.csdnimg.cn/d79c75e0931442689c774da73ba18e33.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAd2NjbGxsbGxsbGxsbA==,size_20,color_FFFFFF,t_70,g_se,x_16"></p><p>创建自定义模型来检测对象是一个迭代过程，包括收集和组织图像、标记感兴趣的对象、训练模型、将其部署到野外以进行预测，然后使用该部署的模型收集边缘情况的示例以进行重复和改进</p><p>YOLOv5 模型必须在标记数据上进行训练，以便学习该数据中对象的类别。在开始训练之前，有两个选项可用于创建数据集：</p><p>使用Roboflow以YOLO格式自动标记，准备和托管您的自定义数据</p><p>或手动准备数据集<br> 本次例程用到的环境：</p><p>安装yolov5配置环境，我用的是anaconda和pycharm进行yolov5的环境搭建</p><p>pytorch: 1.8.0   python: 3.8.12</p><p>tips：如若使用GPU，cuda version &gt;&#x3D;10.1</p><p>首先要下载并安装yolov5：</p><p>yolov5官方要求 Python&gt;&#x3D;3.6 and PyTorch&gt;&#x3D;1.7</p><p>yolov5源码下载：<a href="https://github.com/ultralytics/yolov5">https://github.com/ultralytics/yolov5</a></p><p>下载后，进入pytorch环境进入yolov5文件夹，使用换源的方法安装依赖。</p><p>如若前面安装时没有换源，这里强烈建议你使用换源的方法再进行安装</p><p>安装过的模块不会在安装，以防缺少模块，影响后续程序运行以及模型训练。</p><p>使用清华镜像源：</p><p>pip install -r requirements.txt -i  <a href="https://pypi.tuna.tsinghua.edu.cn/simple">https://pypi.tuna.tsinghua.edu.cn/simple</a></p><p>本文数据集部分内容参考一位大佬的文章：<br>(62条消息) Yolov5训练自己的数据集（详细完整版）_缔宇的博客-CSDN博客_yolov5训练自己的数据集<br><a href="https://blog.csdn.net/qq_45945548/article/details/121701492">https://blog.csdn.net/qq_45945548/article/details/121701492</a></p><p>环境搭建可以参考 ：<br>(62条消息) 史上最详细yolov5环境配置搭建+配置所需文件_想到好名再改的博客-CSDN博客_yolov5环境配置<br><a href="https://blog.csdn.net/qq_44697805/article/details/107702939?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522164757209816782246424327%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fall.%2522%257D&amp;request_id=164757209816782246424327&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v1~rank_v31_ecpm-1-107702939.142%5Ev2%5Epc_search_result_cache,143%5Ev4%5Econtrol&amp;utm_term=yolov5%E7%8E%AF%E5%A2%83&amp;spm=1018.2226.3001.4187">https://blog.csdn.net/qq_44697805/article/details/107702939?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522164757209816782246424327%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fall.%2522%257D&amp;request_id=164757209816782246424327&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v1~rank_v31_ecpm-1-107702939.142%5Ev2%5Epc_search_result_cache,143%5Ev4%5Econtrol&amp;utm_term=yolov5%E7%8E%AF%E5%A2%83&amp;spm=1018.2226.3001.4187</a></p><p>二、环境配置所需资源<br>我选用的是pycharm+anaconda搭建的环境，使用Pip命令安装所需功能包,第一次配置时，我的环境如下：python3.8.12+cuda10.2+cudnn-10.2-windows11-x64-v22000.556Windows 功能体验包 1000.22000.556.0+pytorch1.8.0+cpu，后面因为cpu跑模型训练太慢了，所以我换成了torch 1.9.0+cu111 CUDA:0 (GeForce GTX 1650, 4096.0MB)也就是GPU独立显卡来跑yolov5。</p><p><img src="https://img-blog.csdnimg.cn/7fdf95330d024be0995ffa03c5427a98.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAd2NjbGxsbGxsbGxsbA==,size_20,color_FFFFFF,t_70,g_se,x_16"></p><p><img src="https://img-blog.csdnimg.cn/02f1bdd6ad8846b98f564c3cca77ab86.png"></p><p>最开始的训练环境CPU，发现太慢了，于是乎我换了GPU</p><p>安装环境：</p><p>git clone <a href="https://github.com/ultralytics/yolov5">https://github.com/ultralytics/yolov5</a> # 下载 yolov5 项目</p><p>python3 -c “from yolov5.utils.google_utils import gdrive_download; gdrive_download(‘1n_oKgR81BJtqk75b00eAjdv03qVCQn2f’,’coco128.zip’)” # 下载官方数据集</p><p>cd yolov5 #进入yolov5 项目文件</p><p>pip install -U -r requirements.txt #安装需求</p><p>如遇pip install -U -r requirements.txt 这一步报错，则依次手动执行pip install</p><p>pip install numpy&#x3D;&#x3D;1.17</p><p>pip install python&#x3D;&#x3D;3.8</p><p>###下面所有的需要全部装一遍，可以参照 yolov5下的 requirements.txt 文件</p><p>Python&gt;&#x3D;3.7 PyTorch&gt;&#x3D;1.5 Cython numpy&#x3D;&#x3D;1.17 opencv-python torch&gt;&#x3D;1.5 matplotlib</p><p>Pillow tensorboard PyYAML&gt;&#x3D;5.3 torchvision scipy tqdm</p><p>git+<a href="https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI">https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI</a></p><p>开始进行训练模型的准备</p><p>1.在 yolov5目录下 新建文件夹 VOCData</p><p><img src="https://img-blog.csdnimg.cn/68732cb29e7247e5b11cc34cf80199a7.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAd2NjbGxsbGxsbGxsbA==,size_20,color_FFFFFF,t_70,g_se,x_16"></p><p>2.在VOCData下新建两个文件夹 Annotations 以及 images</p><p><img src="https://img-blog.csdnimg.cn/de0b3ebfd91349b1b6e7063a55b146c7.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAd2NjbGxsbGxsbGxsbA==,size_20,color_FFFFFF,t_70,g_se,x_16"></p><p>其作用为：</p><p>images：用于存放要标注的图片（jpg格式）</p><p>Annotations ：用于存放标注图片后产生的内容（这里采用XML格式）</p><p> 三. 使用labelImg标注图片<br>1.安装labellmg<br>下载labelImg：<a href="https://github.com/tzutalin/labelImg">https://github.com/tzutalin/labelImg</a></p><p><img src="https://img-blog.csdnimg.cn/856acfb5de0b4ca8b94de298c055d62c.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAd2NjbGxsbGxsbGxsbA==,size_20,color_FFFFFF,t_70,g_se,x_16"></p><p><img src="https://img-blog.csdnimg.cn/71912a80961a4c2d861bccba09c55161.png"></p><p><img src="https://img-blog.csdnimg.cn/b813e5e579fa4291a4bc105229f6df17.png"></p><p>下载后存放目录到yolov5同级下面 </p><p>进入开始页面打开anaconda prompt（anaconda3） </p><p>cd d:</p><p>cd D:\Testcode\labelImg<br>执行命令前，建议更新一下conda<br>conda update -n base -c defaults conda</p><p>然后执行以下命令</p><p>conda install pyqt&#x3D;5<br>conda install -c anaconda lxml<br>pyrcc5 -o libs&#x2F;resources.py resources.qrc</p><ol start="2"><li>使用labellmg<br>开始使用labellmg进行图像的标注<br>运行软件前可以更改下要标注的类别。</li></ol><p>这里建议先更改类别因为进入软件后再添加的话，以后每次进入软件都要添加，会比较繁复</p><p><img src="https://img-blog.csdnimg.cn/be10114541204e8eb54db5fba6adf4be.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAd2NjbGxsbGxsbGxsbA==,size_20,color_FFFFFF,t_70,g_se,x_16"></p><p>打开labellmg（要进入labellmg文件夹运行，这里使用pycharm打开labellmg文件夹转到目录下再运行）</p><p><img src="https://img-blog.csdnimg.cn/4edb8b34a7f541b4ba2f1c870f0839bb.png"></p><p>python labelImg.py   #运行软件<br> 打开labelimg的自动保存模式（auto save mode）</p><p><img src="https://img-blog.csdnimg.cn/4ec86aa352c04a35a21de53e16866543.png"></p><p>保存位置为D:\Testcode\yolov5-5.0\VOCData\Annotations<br>要标注的图片文件夹D:\Testcode\yolov5-5.0\VOCData\images</p><p><img src="https://img-blog.csdnimg.cn/4271a26360df4ed1ac46d8d6b1e04621.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAd2NjbGxsbGxsbGxsbA==,size_20,color_FFFFFF,t_70,g_se,x_16"></p><p>格式默认为XML格式即可，可以更改成yolo，这里就使用默认的XML格式</p><p>点击左方边栏或者屏幕右键选择 Create RectBox 即可进行标注。</p><p>尽可能的完全拟合标注物体，建议放大标注</p><p>其它看个人标准。</p><p><img src="https://img-blog.csdnimg.cn/27d0a6beb0ee44c1a4a17bd3611cacf7.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAd2NjbGxsbGxsbGxsbA==,size_20,color_FFFFFF,t_70,g_se,x_16"></p><p>四. 划分数据集以及配置文件修改<br>运行代码方式</p><p>第一种：使用pycharm、vscode、python自带的IDLE。如果出现缺少模块的情况（no module named），你可以安装模块，也可以使用后一种方法。</p><p>第二种：进入pytorch环境，进入代码所在目录，使用命令行形式运行（python + 程序名）</p><ol><li>划分训练集、验证集、测试集<br>在VOCData目录下创建程序 split_train_val.py 并运行，程序如下：</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># coding:utf-8</span><br><span class="line"></span><br><span class="line">import os</span><br><span class="line">import random</span><br><span class="line">import argparse</span><br></pre></td></tr></table></figure><p>parser &#x3D; argparse.ArgumentParser()<br>#xml文件的地址，根据自己的数据进行修改 xml一般存放在Annotations下<br>parser.add_argument(‘–xml_path’, default&#x3D;’Annotations’, type&#x3D;str, help&#x3D;’input xml label path’)<br>#数据集的划分，地址选择自己数据下的ImageSets&#x2F;Main<br>parser.add_argument(‘–txt_path’, default&#x3D;’ImageSets&#x2F;Main’, type&#x3D;str, help&#x3D;’output txt label path’)<br>opt &#x3D; parser.parse_args()</p><p>trainval_percent &#x3D; 1.0  # 训练集和验证集所占比例。 这里没有划分测试集<br>train_percent &#x3D; 0.9     # 训练集所占比例，可自己进行调整<br>xmlfilepath &#x3D; opt.xml_path<br>txtsavepath &#x3D; opt.txt_path<br>total_xml &#x3D; os.listdir(xmlfilepath)<br>if not os.path.exists(txtsavepath):<br>    os.makedirs(txtsavepath)</p><p>num &#x3D; len(total_xml)<br>list_index &#x3D; range(num)<br>tv &#x3D; int(num * trainval_percent)<br>tr &#x3D; int(tv * train_percent)<br>trainval &#x3D; random.sample(list_index, tv)<br>train &#x3D; random.sample(trainval, tr)</p><p>file_trainval &#x3D; open(txtsavepath + ‘&#x2F;trainval.txt’, ‘w’)<br>file_test &#x3D; open(txtsavepath + ‘&#x2F;test.txt’, ‘w’)<br>file_train &#x3D; open(txtsavepath + ‘&#x2F;train.txt’, ‘w’)<br>file_val &#x3D; open(txtsavepath + ‘&#x2F;val.txt’, ‘w’)</p><p>for i in list_index:<br>    name &#x3D; total_xml[i][:-4] + ‘\n’<br>    if i in trainval:<br>        file_trainval.write(name)<br>        if i in train:<br>            file_train.write(name)<br>        else:<br>            file_val.write(name)<br>    else:<br>        file_test.write(name)</p><p>file_trainval.close()<br>file_train.close()<br>file_val.close()<br>file_test.close()</p><p>运行完毕后 会生成 ImagesSets\Main 文件夹，且在其下生成 测试集、训练集、验证集，存放图片的名字（无后缀.jpg）</p><p><img src="https://img-blog.csdnimg.cn/0ed21737690f4f65915ca60a25a1711b.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAd2NjbGxsbGxsbGxsbA==,size_20,color_FFFFFF,t_70,g_se,x_16"></p><p>由于没有分配测试集，所以测试集为空。</p><p>若要分配，更改第 14、15 行代码，更改所在比例即可。</p><ol start="2"><li>XML格式转yolo_txt格式<br>在VOCData目录下创建程序 text_to_yolo.py 并运行</li></ol><p><img src="https://img-blog.csdnimg.cn/ee93cd92e6184102a7c98cf6af5e0dd6.png"></p><p>程序如下：</p><p>注：需要将第 7 行改成要所标注的类别 以及 代码中各文件绝对路径</p><p>路径需为：d:\images 或者 d:&#x2F;images，双右斜线或者单左斜线</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line"></span><br><span class="line">import xml.etree.ElementTree as ET</span><br><span class="line">import os</span><br><span class="line">from os import getcwd</span><br><span class="line"></span><br><span class="line">sets = [&#x27;train&#x27;, &#x27;val&#x27;, &#x27;test&#x27;]</span><br><span class="line">classes = [&quot;victory&quot;]   # 改成自己的类别</span><br><span class="line">abs_path = os.getcwd()</span><br><span class="line">print(abs_path)</span><br><span class="line"></span><br><span class="line">def convert(size, box):</span><br><span class="line">    dw = 1. / (size[0])</span><br><span class="line">    dh = 1. / (size[1])</span><br><span class="line">    x = (box[0] + box[1]) / 2.0 - 1</span><br><span class="line">    y = (box[2] + box[3]) / 2.0 - 1</span><br><span class="line">    w = box[1] - box[0]</span><br><span class="line">    h = box[3] - box[2]</span><br><span class="line">    x = x * dw</span><br><span class="line">    w = w * dw</span><br><span class="line">    y = y * dh</span><br><span class="line">    h = h * dh</span><br><span class="line">    return x, y, w, h</span><br><span class="line"></span><br><span class="line">def convert_annotation(image_id):</span><br><span class="line">    in_file = open(&#x27;D:/Yolov5/yolov5/VOCData/Annotations/%s.xml&#x27; % (image_id), encoding=&#x27;UTF-8&#x27;)</span><br><span class="line">    out_file = open(&#x27;D:/Yolov5/yolov5/VOCData/labels/%s.txt&#x27; % (image_id), &#x27;w&#x27;)</span><br><span class="line">    tree = ET.parse(in_file)</span><br><span class="line">    root = tree.getroot()</span><br><span class="line">    size = root.find(&#x27;size&#x27;)</span><br><span class="line">    w = int(size.find(&#x27;width&#x27;).text)</span><br><span class="line">    h = int(size.find(&#x27;height&#x27;).text)</span><br><span class="line">    for obj in root.iter(&#x27;object&#x27;):</span><br><span class="line">        difficult = obj.find(&#x27;difficult&#x27;).text</span><br><span class="line">        #difficult = obj.find(&#x27;Difficult&#x27;).text</span><br><span class="line">        cls = obj.find(&#x27;name&#x27;).text</span><br><span class="line">        if cls not in classes or int(difficult) == 1:</span><br><span class="line">            continue</span><br><span class="line">        cls_id = classes.index(cls)</span><br><span class="line">        xmlbox = obj.find(&#x27;bndbox&#x27;)</span><br><span class="line">        b = (float(xmlbox.find(&#x27;xmin&#x27;).text), float(xmlbox.find(&#x27;xmax&#x27;).text), float(xmlbox.find(&#x27;ymin&#x27;).text),</span><br><span class="line">             float(xmlbox.find(&#x27;ymax&#x27;).text))</span><br><span class="line">        b1, b2, b3, b4 = b</span><br><span class="line">        # 标注越界修正</span><br><span class="line">        if b2 &gt; w:</span><br><span class="line">            b2 = w</span><br><span class="line">        if b4 &gt; h:</span><br><span class="line">            b4 = h</span><br><span class="line">        b = (b1, b2, b3, b4)</span><br><span class="line">        bb = convert((w, h), b)</span><br><span class="line">        out_file.write(str(cls_id) + &quot; &quot; + &quot; &quot;.join([str(a) for a in bb]) + &#x27;\n&#x27;)</span><br><span class="line"></span><br><span class="line">wd = getcwd()</span><br><span class="line">for image_set in sets:</span><br><span class="line">    if not os.path.exists(&#x27;D:/Yolov5/yolov5/VOCData/labels/&#x27;):</span><br><span class="line">        os.makedirs(&#x27;D:/Yolov5/yolov5/VOCData/labels/&#x27;)</span><br><span class="line">    image_ids = open(&#x27;D:/Yolov5/yolov5/VOCData/ImageSets/Main/%s.txt&#x27; % (image_set)).read().strip().split()</span><br></pre></td></tr></table></figure><pre><code>if not os.path.exists(&#39;D:/Yolov5/yolov5/VOCData/dataSet_path/&#39;):    os.makedirs(&#39;D:/Yolov5/yolov5/VOCData/dataSet_path/&#39;) list_file = open(&#39;dataSet_path/%s.txt&#39; % (image_set), &#39;w&#39;)for image_id in image_ids:    list_file.write(&#39;D:/Yolov5/yolov5/VOCData/images/%s.jpg\n&#39; % (image_id))    convert_annotation(image_id)list_file.close()</code></pre><p>运行后会生成如下 labels 文件夹和 dataSet_path 文件夹。</p><p><img src="https://img-blog.csdnimg.cn/1152977d70574f679cb1f9f878c5aa67.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAd2NjbGxsbGxsbGxsbA==,size_20,color_FFFFFF,t_70,g_se,x_16"></p><p>其中 labels 中为不同图像的标注文件。每个图像对应一个txt文件，文件每一行为一个目标的信息，包括class, x_center, y_center, width, height格式，这种即为 yolo_txt格式</p><p>dataSet_path文件夹包含三个数据集的txt文件，train.txt等txt文件为划分后图像所在位置的绝对路径，如train.txt就含有所有训练集图像的绝对路径。 </p><p><img src="https://img-blog.csdnimg.cn/4c6bec9d2d774ba1baeb5ad45dc14719.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAd2NjbGxsbGxsbGxsbA==,size_20,color_FFFFFF,t_70,g_se,x_16"></p><p><img src="https://img-blog.csdnimg.cn/5aff21fc48954fdda67761bbdd2d0198.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAd2NjbGxsbGxsbGxsbA==,size_18,color_FFFFFF,t_70,g_se,x_16"></p><ol start="3"><li>配置文件</li></ol><p>在 yolov5 目录下的 data 文件夹下 新建一个 myvoc.yaml文件（可以自定义命名），用记事本打开。<br>内容是：训练集以及验证集（train.txt和val.txt）绝对路径（通过 text_to_yolo.py 生成），然后是目标的类别数目和类别名称。</p><p><img src="https://img-blog.csdnimg.cn/3043ed0b57e544f6b3dd7fef9dfd8635.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAd2NjbGxsbGxsbGxsbA==,size_20,color_FFFFFF,t_70,g_se,x_16"></p><p> 我的是这样设置的</p><p>train: D:\Testcode\yolov5-5.0\VOCData\dataSet_path\train.txt<br>val: D:\Testcode\yolov5-5.0\VOCData\dataSet_path\val.txt</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># number of classes</span><br><span class="line"></span><br><span class="line">nc: 1</span><br><span class="line"></span><br><span class="line"># class names</span><br><span class="line"></span><br><span class="line">names: [&quot;victory&quot;]</span><br></pre></td></tr></table></figure><p>tips：冒号后面需要加空格</p><ol start="3"><li>聚类获得先验框</li></ol><p>3.1 生成anchors文件<br>在VOCData目录下创建程序两个程序 kmeans.py 以及 clauculate_anchors.py<br>不需要运行 kmeans.py，运行 clauculate_anchors.py 即可。</p><p>kmeans.py 程序如下：这不需要运行，也不需要更改，报错则查看第十三行内容。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">def iou(box, clusters):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Calculates the Intersection over Union (IoU) between a box and k clusters.</span><br><span class="line">    :param box: tuple or array, shifted to the origin (i. e. width and height)</span><br><span class="line">    :param clusters: numpy array of shape (k, 2) where k is the number of clusters</span><br><span class="line">    :return: numpy array of shape (k, 0) where k is the number of clusters</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    x = np.minimum(clusters[:, 0], box[0])</span><br><span class="line">    y = np.minimum(clusters[:, 1], box[1])</span><br><span class="line">    if np.count_nonzero(x == 0) &gt; 0 or np.count_nonzero(y == 0) &gt; 0:</span><br><span class="line">        raise ValueError(&quot;Box has no area&quot;)    # 如果报这个错，可以把这行改成pass即可</span><br></pre></td></tr></table></figure><pre><code>intersection = x * ybox_area = box[0] * box[1]cluster_area = clusters[:, 0] * clusters[:, 1] iou_ = intersection / (box_area + cluster_area - intersection) return iou_</code></pre><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">def avg_iou(boxes, clusters):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Calculates the average Intersection over Union (IoU) between a numpy array of boxes and k clusters.</span><br><span class="line">    :param boxes: numpy array of shape (r, 2), where r is the number of rows</span><br><span class="line">    :param clusters: numpy array of shape (k, 2) where k is the number of clusters</span><br><span class="line">    :return: average IoU as a single float</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    return np.mean([np.max(iou(boxes[i], clusters)) for i in range(boxes.shape[0])])</span><br><span class="line"></span><br><span class="line">def translate_boxes(boxes):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Translates all the boxes to the origin.</span><br><span class="line">    :param boxes: numpy array of shape (r, 4)</span><br><span class="line">    :return: numpy array of shape (r, 2)</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    new_boxes = boxes.copy()</span><br><span class="line">    for row in range(new_boxes.shape[0]):</span><br><span class="line">        new_boxes[row][2] = np.abs(new_boxes[row][2] - new_boxes[row][0])</span><br><span class="line">        new_boxes[row][3] = np.abs(new_boxes[row][3] - new_boxes[row][1])</span><br><span class="line">    return np.delete(new_boxes, [0, 1], axis=1)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def kmeans(boxes, k, dist=np.median):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Calculates k-means clustering with the Intersection over Union (IoU) metric.</span><br><span class="line">    :param boxes: numpy array of shape (r, 2), where r is the number of rows</span><br><span class="line">    :param k: number of clusters</span><br><span class="line">    :param dist: distance function</span><br><span class="line">    :return: numpy array of shape (k, 2)</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    rows = boxes.shape[0]</span><br></pre></td></tr></table></figure><pre><code>distances = np.empty((rows, k))last_clusters = np.zeros((rows,)) np.random.seed() # the Forgy method will fail if the whole array contains the same rowsclusters = boxes[np.random.choice(rows, k, replace=False)] while True:    for row in range(rows):        distances[row] = 1 - iou(boxes[row], clusters)     nearest_clusters = np.argmin(distances, axis=1)     if (last_clusters == nearest_clusters).all():        break     for cluster in range(k):        clusters[cluster] = dist(boxes[nearest_clusters == cluster], axis=0)     last_clusters = nearest_clusters return clusters</code></pre><p>if <strong>name</strong> &#x3D;&#x3D; ‘<strong>main</strong>‘:<br>    a &#x3D; np.array([[1, 2, 3, 4], [5, 7, 6, 8]])<br>    print(translate_boxes(a))</p><p>运行：clauculate_anchors.py</p><p>会调用 kmeans.py 聚类生成新anchors的文件</p><p>程序如下：</p><p>需要更改第 9 、13行文件路径 以及 第 16 行标注类别名称</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">FILE_ROOT = &quot;D:\Testcode\yolov5-5.0\VOCData/&quot;     # 根路径</span><br><span class="line">ANNOTATION_ROOT = &quot;Annotations&quot;   # 数据集标签文件夹路径</span><br><span class="line">ANNOTATION_PATH = FILE_ROOT + ANNOTATION_ROOT</span><br><span class="line"></span><br><span class="line">ANCHORS_TXT_PATH = &quot;D:\Testcode\yolov5-5.0\VOCData/anchors.txt&quot;   #anchors文件保存位置</span><br><span class="line"></span><br><span class="line">CLUSTERS = 9</span><br><span class="line">CLASS_NAMES = [&#x27;victory&#x27;]   #类别名称</span><br><span class="line"></span><br><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line"></span><br><span class="line"># 根据标签文件求先验框</span><br><span class="line"></span><br><span class="line">import os</span><br><span class="line">import numpy as np</span><br><span class="line">import xml.etree.cElementTree as et</span><br><span class="line">from kmeans import kmeans, avg_iou</span><br><span class="line"></span><br><span class="line">FILE_ROOT = &quot;D:/yolov5-5/VOCData/&quot;     # 根路径</span><br><span class="line">ANNOTATION_ROOT = &quot;Annotations&quot;   # 数据集标签文件夹路径</span><br><span class="line">ANNOTATION_PATH = FILE_ROOT + ANNOTATION_ROOT</span><br><span class="line"></span><br><span class="line">ANCHORS_TXT_PATH = &quot;D:/yolov5-5/VOCData/anchors.txt&quot;   #anchors文件保存位置</span><br><span class="line">CLUSTERS = 9</span><br><span class="line">CLASS_NAMES = [&#x27;victory&#x27;]   #类别名称</span><br><span class="line"></span><br><span class="line">def load_data(anno_dir, class_names):</span><br><span class="line">    xml_names = os.listdir(anno_dir)</span><br><span class="line">    boxes = []</span><br><span class="line">    for xml_name in xml_names:</span><br><span class="line">        xml_pth = os.path.join(anno_dir, xml_name)</span><br><span class="line">        tree = et.parse(xml_pth)</span><br></pre></td></tr></table></figure><pre><code>    width = float(tree.findtext(&quot;./size/width&quot;))    height = float(tree.findtext(&quot;./size/height&quot;))     for obj in tree.findall(&quot;./object&quot;):        cls_name = obj.findtext(&quot;name&quot;)        if cls_name in class_names:            xmin = float(obj.findtext(&quot;bndbox/xmin&quot;)) / width            ymin = float(obj.findtext(&quot;bndbox/ymin&quot;)) / height            xmax = float(obj.findtext(&quot;bndbox/xmax&quot;)) / width            ymax = float(obj.findtext(&quot;bndbox/ymax&quot;)) / height             box = [xmax - xmin, ymax - ymin]            boxes.append(box)        else:            continuereturn np.array(boxes)</code></pre><p>if <strong>name</strong> &#x3D;&#x3D; ‘<strong>main</strong>‘:</p><pre><code>anchors_txt = open(ANCHORS_TXT_PATH, &quot;w&quot;) train_boxes = load_data(ANNOTATION_PATH, CLASS_NAMES)count = 1best_accuracy = 0best_anchors = []best_ratios = [] for i in range(10):      ##### 可以修改，不要太大，否则时间很长    anchors_tmp = []    clusters = kmeans(train_boxes, k=CLUSTERS)    idx = clusters[:, 0].argsort()    clusters = clusters[idx]    # print(clusters)     for j in range(CLUSTERS):        anchor = [round(clusters[j][0] * 640, 2), round(clusters[j][1] * 640, 2)]        anchors_tmp.append(anchor)        print(f&quot;Anchors:&#123;anchor&#125;&quot;)     temp_accuracy = avg_iou(train_boxes, clusters) * 100    print(&quot;Train_Accuracy:&#123;:.2f&#125;%&quot;.format(temp_accuracy))     ratios = np.around(clusters[:, 0] / clusters[:, 1], decimals=2).tolist()    ratios.sort()    print(&quot;Ratios:&#123;&#125;&quot;.format(ratios))    print(20 * &quot;*&quot; + &quot; &#123;&#125; &quot;.format(count) + 20 * &quot;*&quot;)     count += 1     if temp_accuracy &gt; best_accuracy:        best_accuracy = temp_accuracy        best_anchors = anchors_tmp        best_ratios = ratios anchors_txt.write(&quot;Best Accuracy = &quot; + str(round(best_accuracy, 2)) + &#39;%&#39; + &quot;\r\n&quot;)anchors_txt.write(&quot;Best Anchors = &quot; + str(best_anchors) + &quot;\r\n&quot;)anchors_txt.write(&quot;Best Ratios = &quot; + str(best_ratios))anchors_txt.close()</code></pre><p>会生成anchors文件。如果生成文件为空，重新运行即可。</p><p><img src="https://img-blog.csdnimg.cn/5dfe0a74113d4aed83de4afdb022d374.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAd2NjbGxsbGxsbGxsbA==,size_20,color_FFFFFF,t_70,g_se,x_16"></p><p>第二行 Best Anchors 后面需要用到。</p><p>3.2 修改模型配置文件</p><p>选择一个模型，在yolov5目录下的model文件夹下是模型的配置文件，有n、s、m、l、x版本，逐渐增大（随着架构的增大，训练时间也是逐渐增大）。</p><p>这里放一些官方数据：GitHub - ultralytics&#x2F;yolov5</p><p><img src="https://img-blog.csdnimg.cn/b0c40f5cfa1d4cdab9fdc86d24b3ddb4.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAd2NjbGxsbGxsbGxsbA==,size_20,color_FFFFFF,t_70,g_se,x_16"></p><p><img src="https://img-blog.csdnimg.cn/467675904cc04718b91f1dcef5bad5b2.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAd2NjbGxsbGxsbGxsbA==,size_20,color_FFFFFF,t_70,g_se,x_16"></p><p>这里选用 yolov5m.yaml</p><p>使用记事本打开 yolov5m.yaml。</p><p><img src="https://img-blog.csdnimg.cn/5c6e18c9d96d43b7854b265b3c9052e7.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAd2NjbGxsbGxsbGxsbA==,size_20,color_FFFFFF,t_70,g_se,x_16"></p><p><img src="https://img-blog.csdnimg.cn/5e4d63e0a08f4de7b8d9f1bdaecd882a.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAd2NjbGxsbGxsbGxsbA==,size_20,color_FFFFFF,t_70,g_se,x_16"></p><p>这里我们需要修改两个参数。<br>把 nc：后面改成自己的标注类别数<br>修改anchors，根据 anchors.txt 中的 Best Anchors 修改，需要取整（四舍五入、向上、向下都可以）。<br>保持yaml中的anchors格式不变，按顺序一对一即可，比如我这里的anchors.txt紫色部分对应</p><p>着yolov5m.yaml的紫色部分，anchors.txt蓝色部分对应着yolov5m.yaml的蓝色部分。</p><p>五. 模型训练</p><ol><li>开始训练<br>打开yolov5 目录下的 train.py 程序，一般有七个参数可以留意：weights：权重文件路径 cfg：存储模型结构的配置文件 data：存储训练、测试数据的文件 epochs：指的就是训练过程中整个数据集将被迭代多少次 batch-size：一次看完多少张图片才进行权重更新，梯度下降的mini-batch  img-size：输入图片宽高 device：cuda device, i.e. 0 or 0,1,2,3 or cpu选择使用GPU还是CPU（这里我选择使用device0即为GPU），</li></ol><p>其它参数解释：rect：进行矩形训练、 resume：恢复最近保存的模型开始训练、 nosave：仅保存最终checkpoint、 notest：仅测试最后的epoch、evolve：进化超参数、  cache-images：缓存图像以加快训练速度、 adam：使用adam优化、 multi-scale：多尺度训练、 single-cls：单类别的训练集。</p><p>步骤11——训练命令如下：首先进入pytorch环境，进入yolov5文件夹，运行指令：python train.py –weights weights&#x2F;yolov5s.pt  –cfg models&#x2F;yolov5s.yaml  –data data&#x2F;myvoc.yaml –epoch 233 –batch-size 8 –img 640   –device 0 ，其中device 0：意为使用GPU进行训练 epoch 200 ：意为训练200次，batch-size 8：意为训练8张图片后进行权重更新 device cpu：意为使用CPU训练。</p><p>python train.py –weights weights&#x2F;yolov5s.pt  –cfg models&#x2F;yolov5s.yaml  –data data&#x2F;myvoc.yaml –epoch 233 –batch-size 8 –img 640   –device 0<br>2. 训练过程</p><p><img src="https://img-blog.csdnimg.cn/c919984c848142399c0f570c1a2aa78c.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAd2NjbGxsbGxsbGxsbA==,size_20,color_FFFFFF,t_70,g_se,x_16"></p><p> <img src="https://img-blog.csdnimg.cn/0912dc6446bf47979581d94fc646e0c6.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAd2NjbGxsbGxsbGxsbA==,size_20,color_FFFFFF,t_70,g_se,x_16"></p><p>训练好的模型会被保存在 yolov5 目录下的 runs&#x2F;train&#x2F;weights&#x2F;expxx下。</p><ol start="3"><li>训练时间</li></ol><p>本次训练对象为标注的118张手势图片，分别使用GPU训练了233次，耗时约为1.7小时。</p><p>这里打开了我的电脑的增强模式，风扇疾速运转中————-呼呼吹</p><p><img src="https://img-blog.csdnimg.cn/1bd3ff1be49849dca5ff09b2544583e4.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAd2NjbGxsbGxsbGxsbA==,size_19,color_FFFFFF,t_70,g_se,x_16"></p><p><img src="https://img-blog.csdnimg.cn/04cd605de59d434d964364d98b501251.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAd2NjbGxsbGxsbGxsbA==,size_20,color_FFFFFF,t_70,g_se,x_16"></p><p>训练过程pycharm可视化</p><ol start="4"><li>相关问题</li></ol><p>报错1：页面太小，无法完成操作——解决方案：由于虚拟内存不足，我设置了一下电脑的虚拟内存然后就可以了，或者降低线程 –workers (默认是8) ，调小 –batch-size，降低 –epoch。</p><p>报错2：训练过程中出现 cuda out of memory error ——解决方案：由于内存满了，减小batch-size ，同时降低 –epochs。然后降低线程 –workers (默认是8)同上一步。</p><p>报错3：运行detect.py时报错AttributeError: ‘NoneType’ object has no attribute ‘find’ ——解决方案；由于图片的指定路径出现错误或者路径是正确的，但是路径设置中的”\”无法被识别，需要”&#x2F;”，如\data\images无法被正确识别，而data&#x2F;images则可以被正确识别。</p><p>五. 训练可视化</p><p>由于我使用的是pycharm，在训练的时候终端可以可视化训练过程</p><p><img src="https://img-blog.csdnimg.cn/88d3be59430b46908d8b620f9c5556f3.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAd2NjbGxsbGxsbGxsbA==,size_20,color_FFFFFF,t_70,g_se,x_16"></p><p>六. 测试效果</p><p>这里刚刚训练出的最好的模型 best.pt 来测试，在yolov5目录下的runs&#x2F;train&#x2F;exp44&#x2F;weights&#x2F;best.pt（训练的人脸模型）和runs&#x2F;train&#x2F;exp62&#x2F;weights&#x2F;best.pt（训练的手势模型）如要进行图片或者视频数据流的识别在terminal输入python detect.py –weights runs&#x2F;train&#x2F;exp44&#x2F;weights&#x2F;best.pt –source ..&#x2F;data&#x2F;images（或videos）&#x2F;123.jpg（123.mp4）即可，如要进行实时摄像头的人脸或者手势识别，则需要修改detect.py第154行的代码为default&#x3D;’0’</p><p><img src="https://img-blog.csdnimg.cn/9dcc2de87d62437cb2637275986bfbdf.png"></p><p>识别结果如下：在测试过程中我留意到由于自己标注的数据集数量比较少以及显卡性能有限的原因batchsize（单次输入神经网络图片的数量即训练x张图片后进行权重更新）不能设置为8以上的大数值，这会导致报错ERROR：cuda out of memory（显卡内存不足）</p><p><img src="https://img-blog.csdnimg.cn/bcaa2ba8e3704585925ffea3209c9eb0.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAd2NjbGxsbGxsbGxsbA==,size_17,color_FFFFFF,t_70,g_se,x_16"></p><p>实时检测识别效果窗口</p><p>结果分析：本次识别中，进行图片和数据流的人脸识别和手势识别效果还行，但在进行实时识别的时候我发现，因为数据集样本数量不够多，少于推荐训练的1500张图片epochs（训练）次数不是特别多次，以及由于显卡内存不足导致batchsize的阈值设置的受限，在识别手势的过程中，比V形手势的时候置信度较高（高于0.85），其余手势也会被识别到，但是其置信度比较低。故下一步我将对yolov5的边框的准确度、标签平滑等方向——做出进一步的优化，以实现高准确率边框，高置信度的人脸识别和手势检测。</p><p>七、感谢以下文章提供的参考<br>(63条消息) 手把手教你使用Yolov5制作并训练自己的数据集_是七叔呀的博客-CSDN博客_yolov5自己制作数据集<br><a href="https://blog.csdn.net/m0_46378271/article/details/120872132?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522164758521116780264033398%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fall.%2522%257D&amp;request_id=164758521116780264033398&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v1~rank_v31_ecpm-1-120872132.142%5Ev2%5Epc_search_result_cache,143%5Ev4%5Econtrol&amp;utm_term=%E3%80%90%E5%B0%8F%E7%99%BDCV%E3%80%91%E6%89%8B%E6%8A%8A%E6%89%8B%E6%95%99%E4%BD%A0%E7%94%A8YOLOv5%E8%AE%AD%E7%BB%83%E8%87%AA%E5%B7%B1%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86%EF%BC%88%E4%BB%8EWindows%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%E5%88%B0%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%EF%BC%89+(icode9.com)&amp;spm=1018.2226.3001.4187">https://blog.csdn.net/m0_46378271/article/details/120872132?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522164758521116780264033398%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fall.%2522%257D&amp;request_id=164758521116780264033398&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v1~rank_v31_ecpm-1-120872132.142%5Ev2%5Epc_search_result_cache,143%5Ev4%5Econtrol&amp;utm_term=%E3%80%90%E5%B0%8F%E7%99%BDCV%E3%80%91%E6%89%8B%E6%8A%8A%E6%89%8B%E6%95%99%E4%BD%A0%E7%94%A8YOLOv5%E8%AE%AD%E7%BB%83%E8%87%AA%E5%B7%B1%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86%EF%BC%88%E4%BB%8EWindows%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%E5%88%B0%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%EF%BC%89+%28icode9.com%29&amp;spm=1018.2226.3001.4187</a></p><p>YOLOv5超详细的入门级教程（训练篇）——训练自己的数据集 - 知乎 (zhihu.com)<br><a href="https://zhuanlan.zhihu.com/p/448525505">https://zhuanlan.zhihu.com/p/448525505</a><br>训练自定义数据 ·ultralytics&#x2F;yolov5 Wiki (github.com)<br><a href="https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data">https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data</a><br>————————————————<br>版权声明：本文为CSDN博主「wccllllllllll」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。<br>原文链接：<a href="https://blog.csdn.net/blink182007/article/details/123569332">https://blog.csdn.net/blink182007/article/details/123569332</a></p>]]></content>
      
      
      <categories>
          
          <category> yolov5 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 视觉检测 tensorflow ai 计算机视觉，encrypted </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo常用命令</title>
      <link href="/2022/04/01/hexo%20command/"/>
      <url>/2022/04/01/hexo%20command/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>npm install hexo -g   &#x2F;&#x2F;安装<br>npm update hexo -g       &#x2F;&#x2F;升级<br>hexo version            &#x2F;&#x2F;查看hexo的版本<br>hexo init nodejs-hexo   &#x2F;&#x2F;创建nodejs-hexo 名字的本地文件<br>hexo init nodejs-hexo    &#x2F;&#x2F;创建博客<br>hexo init blog          &#x2F;&#x2F;初始化，生成文件夹为blog<br>cd blog                   &#x2F;&#x2F;进入blog文件夹<br>npm install            &#x2F;&#x2F;安装依赖库<br>hexo generate           &#x2F;&#x2F;生成一套静态网页<br>hexo server         &#x2F;&#x2F;运行测试,浏览器打开地址，<a href="http://localhost:4000/">http://localhost:4000/</a><br>hexo deploy         &#x2F;&#x2F;进行部署</p><p>hexo new “new article”  &#x2F;&#x2F;新建文章‘new article’<br>hexo new page “about”  &#x2F;&#x2F;新建页面 ‘about’</p><p>hexo n “我的博客”<code>==</code>hexo new<code> &quot;我的博客&quot;    //新建文章 hexo g == hexo generate        //生成</code><br>hexo s &#x3D;&#x3D; hexo server          &#x2F;&#x2F;启动服务预览<br>hexo d &#x3D;&#x3D; hexo deploy          &#x2F;&#x2F;部署</p><span id="more"></span><p>新建的笔记文件路径：source&#x2F;_posts 目录下, new-article.md 的文件, 打开文件, 文件开头采用的是yaml格式，用三条短横线分隔。下面是文章正文，采用用的是markdown格式。</p><p>新建、删除或修改文章后，不需要重启hexo server，刷新一下即可预览。</p><p>Hexo起服务<br>hexo server #Hexo 会监视文件变动并自动更新，您无须重启服务器。 hexo server -s #静态模式 hexo server -p 5000 #更改端口 hexo server -i 192.168.1.1 #自定义 IP<br>hexo clean #清除缓存 网页正常情况下可以忽略此条命令 hexo g #生成静态网页 hexo d #开始部署&#96;</p><p>Hexo写博客常用方法<br>Hexo文章属性设置<br>设置 - - - - 描述 - - - - Default<br>layout - - - - Layout - - - - post或page<br>title - - - - 文章的标题 - - - -<br>date - - - - 创建日期 - - - - 文件的创建日期<br>updated - - - - 修改日期 - - - - 文件的修改日期<br>comments - - - - 是否开启评论 - - - - true<br>tags - - - - 标签<br>categories - - - - 分类<br>permalink - - - - url中的名字 - - - - 文件名</p><p>Hexo摘要<br>这里是文章的摘要部分<!--more-->后面才是文章的正文内容</p><blockquote>在 markdown 文件顶部添加 description 属性，可以在首页文章列表显示文章的描述，如果没有这个属性，会自动截取文章内容作为文章的描述。</blockquote><p>hexo更换皮肤<br>hexo官网选择皮肤: git clone<br>放到themes目录下<br>编辑文件_config.yml，找到theme一行，改成 theme: pacman<br>本地启动hexo服务器，打开浏览器 <a href="http://localhost:4000/">http://localhost:4000</a><br>找不到git部署<br>ERROR Deployer not found: git<br>解决方法：npm install hexo-deployer-git –save</p><p>Hexo不常用的命令<br>render<br>渲染文件： hexo render <file1> [file2] …<br>参数 -o(–output) 设置输出路径</p><p>hexo render path1&#x2F;xxx.md -o path2&#x2F;xxx.yyy<br>1<br>hexo-render命令其实就是把我们的文章通过模板渲染成html代码，当然渲染完成后是可以输出到另一个文件的</p><p>list<br>列出网站资料：hexo list <type></p><p>hexo list page<br>1<br>Hexo草稿<br>草稿相当于很多博客都有的“私密文章”功能。</p><p>$ hexo new draft “new draft”<br>1<br>草稿会在source&#x2F;_drafts目录下生成一个new-draft.md文件。但是这个文件不被显示在页面上，链接也访问不到。也就是说如果你想把某一篇文章移除显示，又不舍得删除，可以把它移动到_drafts目录之中。</p><p>如果你希望强行预览草稿，更改配置文件：</p><p>render_drafts: true<br>1<br>或者，如下方式启动预览：</p><p>$ hexo server –drafts<br>1<br>下面这条命令可以把草稿变成文章，或者页面：</p><p>$ hexo publish [layout] <filename><br>1<br>github上hexo搭建博客-绑定域名<br>这里选择阿里云域名</p><p>登陆阿里云控制台，选择域名，选择要绑定的域名，点解析<br>解析页，添加记录，<br>记录类型 A<br>主机记录 WWW, @<br>记录值 填写对应的github的ip地址<br>Github的ip地址获取<br>clone 你创建的仓库, 用户名.github.io<br>输入 ping www.用户名.github.io<br>Ping sni.github.map.fastly.net [151.202.68.147], IP地址<br>在source目录下创建这个CNAME文件，输入域名<br>提交代码，点击域名，一般很快就生效。<br>标签插件<br>Hexo 的标签插件是独立于 Markdown 的渲染引擎的，标签插件采用独有的方式渲染,虽然有的时候可能和 Markdown 渲染效果一样，在这里我就不说 Markdown 的渲染方式了，如果想要了解 Markdown 的请参考这篇文章 MarkDown语法指南</p><p>引用块<br>-在文章中插入引言，可包含作者、来源和标题。</p><p>语法格式</p><blockquote><p>content</p><footer><strong>[author[</strong><cite>source]] [link] [source_link_title]</cite></footer></blockquote><p>例子1: 引用书上的句子</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;% blockquote David Levithan, Wide Awake %&#125;</span><br><span class="line">Do not just seek happiness for yourself. Seek happiness for all. Through kindness. Through mercy.</span><br><span class="line">&#123;% endblockquote %&#125;</span><br></pre></td></tr></table></figure><p>例子2: 引用Twitter</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;% blockquote @DevDocs https://twitter.com/devdocs/status/356095192085962752 %&#125;</span><br><span class="line">NEW: DevDocs now comes with syntax highlighting. http://devdocs.io</span><br><span class="line">&#123;% endblockquote %&#125;</span><br></pre></td></tr></table></figure><p>例子3: 引用网络上的文章</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;% blockquote Seth Godin http://sethgodin.typepad.com/seths_blog/2009/07/welcome-to-island-marketing.html Welcome to Island Marketing %&#125;</span><br><span class="line">Every interaction is both precious and an opportunity to delight.</span><br><span class="line">&#123;% endblockquote %&#125;</span><br></pre></td></tr></table></figure><p>代码块<br>语法格式</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;% codeblock [title] [lang:language] [url] [link text] %&#125;</span><br><span class="line">code snippet</span><br><span class="line">&#123;% endcodeblock %&#125;</span><br></pre></td></tr></table></figure><p>例子1: 普通的代码块</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;% codeblock %&#125;</span><br><span class="line">alert(&#x27;Hello World!&#x27;);</span><br><span class="line">&#123;% endcodeblock %&#125;</span><br></pre></td></tr></table></figure><p>例子2: 指定语言</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;% codeblock lang:objc %&#125;</span><br><span class="line">[rectangle setX: 10 y: 10 width: 20 height: 20];</span><br><span class="line">&#123;% endcodeblock %&#125;</span><br></pre></td></tr></table></figure><p>例子3: 附加说明和网址</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#123;% codeblock _.compact http://underscorejs.org/#compact Underscore.js %&#125;</span><br><span class="line">_.compact([0, 1, false, 2, &#x27;&#x27;, 3]);</span><br><span class="line">=&gt; [1, 2, 3]</span><br><span class="line">&#123;% endcodeblock %&#125;</span><br></pre></td></tr></table></figure><p>iframe<br>在文章中插入 iframe</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#123;% iframe url [width] [height] %&#125; //语法</span><br><span class="line">&#123;% iframe https://www.xgqq.com/ 960 500 %&#125;  //实例</span><br></pre></td></tr></table></figure><p>Image<br>在文章中插入指定大小的图片。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#123;% img [class names] /path/to/image [width] [height] [title text [alt text]] %&#125; //语法</span><br><span class="line">&#123;% img [class names] /yn.jpeg [300] [height] [云南风景 [风景图片]] %&#125; //实例</span><br></pre></td></tr></table></figure><p>Link<br>在文章中插入链接，并自动给外部链接添加 target&#x3D;”_blank” 属性。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#123;% link 百度一下 http://www.baidu.com [external] [超链接] %&#125;//语法</span><br><span class="line">&#123;% link 百度一下 http://www.baidu.com [这是什么] [超链接] %&#125;//实例</span><br></pre></td></tr></table></figure><p>include Code<br>插入 source 文件夹内的代码文件。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#123;% include_code [title] [lang:language] path/to/file %&#125; //语法</span><br><span class="line">&#123;% include_code appjs lang:javascript /app.js %&#125;//实例</span><br></pre></td></tr></table></figure><p>Youtube<br>在文章中插入 Youtube 视频。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#123;% youtube video_id %&#125; //语法</span><br><span class="line">&#123;% youtube ICkxRE_GdgI %&#125;  //id在Url中</span><br></pre></td></tr></table></figure><p>引用文章<br>引用其他文章的链接。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#123;% post_path slug %&#125;</span><br><span class="line">&#123;% post_link slug [title] %&#125;</span><br></pre></td></tr></table></figure><p>引用资源<br>引用文章的资源。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;% asset_path slug %&#125;</span><br><span class="line">&#123;% asset_img slug [title] %&#125;</span><br><span class="line">&#123;% asset_link slug [title] %&#125;</span><br></pre></td></tr></table></figure><p>RSS不显示<br>安装RSS插件</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-generator-feed --save  //安装RSS插件</span><br></pre></td></tr></table></figure><p>开启RSS功能<br>编辑hexo&#x2F;_config.yml<br>rss: &#x2F;atom.xml #rss地址   &#x2F;&#x2F;默认即可<br>开启评论<br>1.我使用多说代替自带的评论，在多说 网站注册 &gt; 后台管理 &gt; 添加新站点 &gt; 工具 &#x3D;&#x3D;&#x3D; 复制通用代码 里面有 short_name</p><p>在根目录 _config.yml 添加一行 disqus_shortname: jslite 是在多说注册时产生的<br>复制到 themes\landscape\layout_partial\article.ejs把</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&lt;% if (!index &amp;&amp; post.comments &amp;&amp; config.disqus_shortname)&#123; %&gt;</span><br><span class="line"></span><br><span class="line">&lt;section id=&quot;comments&quot;&gt;</span><br><span class="line">&lt;div id=&quot;disqus_thread&quot;&gt;</span><br><span class="line">  &lt;noscript&gt;Please enable JavaScript to view the &lt;a href=&quot;//disqus.com/?ref_noscript&quot;&gt;comments powered by Disqus.&lt;/a&gt;&lt;/noscript&gt;</span><br><span class="line">&lt;/div&gt;</span><br><span class="line">&lt;/section&gt;</span><br><span class="line"></span><br><span class="line">&lt;% &#125; %&gt;</span><br></pre></td></tr></table></figure><p>改为</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">&lt;% if (!index &amp;&amp; post.comments &amp;&amp; config.disqus_shortname)&#123; %&gt;</span><br><span class="line"></span><br><span class="line">  &lt;section id=&quot;comments&quot;&gt;</span><br><span class="line">    &lt;!-- 多说评论框 start --&gt;</span><br><span class="line">    &lt;div class=&quot;ds-thread&quot; data-thread-key=&quot;&lt;%= post.layout %&gt;-&lt;%= post.slug %&gt;&quot; data-title=&quot;&lt;%= post.title %&gt;&quot; data-url=&quot;&lt;%= page.permalink %&gt;&quot;&gt;&lt;/div&gt;</span><br><span class="line">    &lt;!-- 多说评论框 end --&gt;</span><br><span class="line">    &lt;!-- 多说公共JS代码 start (一个网页只需插入一次) --&gt;</span><br><span class="line">    &lt;script type=&quot;text/javascript&quot;&gt;</span><br><span class="line">    var duoshuoQuery = &#123;short_name:&#x27;&lt;%= config.disqus_shortname %&gt;&#x27;&#125;;</span><br><span class="line">      (function() &#123;</span><br><span class="line">        var ds = document.createElement(&#x27;script&#x27;);</span><br><span class="line">        ds.type = &#x27;text/javascript&#x27;;ds.async = true;</span><br><span class="line">        ds.src = (document.location.protocol == &#x27;https:&#x27; ? &#x27;https:&#x27; : &#x27;http:&#x27;) + &#x27;//static.duoshuo.com/embed.js&#x27;;</span><br><span class="line">        ds.charset = &#x27;UTF-8&#x27;;</span><br><span class="line">        (document.getElementsByTagName(&#x27;head&#x27;)[0] </span><br><span class="line">         || document.getElementsByTagName(&#x27;body&#x27;)[0]).appendChild(ds);</span><br><span class="line">      &#125;)();</span><br><span class="line">      &lt;/script&gt;</span><br><span class="line">    &lt;!-- 多说公共JS代码 end --&gt;</span><br><span class="line">  &lt;/section&gt;</span><br><span class="line"></span><br><span class="line">&lt;% &#125; %&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">layout</span><br></pre></td></tr></table></figure><p>如果你修改了layout，在scaffolds文件夹里一定要有名字对应的模版文件，否则会采用默认模版。<br>参考资料：<br>Hexo官方网站 : <a href="https://hexo.io/">https://hexo.io/</a></p>]]></content>
      
      
      <categories>
          
          <category> hexo </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 学习，小白，encrypted </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>YoloV5实战：手把手教物体检测——YoloV5</title>
      <link href="/2022/03/26/yoloweihua/"/>
      <url>/2022/03/26/yoloweihua/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>【摘要】 目录摘要训练1、下载代码2、配置环境3、准备数据集4、生成数据集5、修改配置参数6、修改train.py的参数7、查看训练结果测试摘要YOLOV5严格意义上说并不是YOLO的第五个版本，因为它并没有得到YOLO之父Joe Redmon的认可，但是给出的测试数据总体表现还是不错。详细数据如下：YOLOv5并不是一个单独的模型，而是一个模型家族，包括了YOLOv5s、YOLOv5m、YOLO…</p><span id="more"></span><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>YOLOV5严格意义上说并不是YOLO的第五个版本，因为它并没有得到YOLO之父Joe Redmon的认可，但是给出的测试数据总体表现还是不错。详细数据如下：</p><p><img src="https://img-blog.csdnimg.cn/20201023232701871.png"></p><p>点击并拖拽以移动</p><p>YOLOv5并不是一个单独的模型，而是一个模型家族，包括了YOLOv5s、YOLOv5m、YOLOv5l、YOLOv5x、YOLOv5x+TTA，这点有点儿像EfficientDet。由于没有找到V5的论文，我们也只能从代码去学习它。总体上和YOLOV4差不多，可以认为是YOLOV5的加强版。</p><p>项目地址：GitHub - ultralytics&#x2F;yolov5: YOLOv5 🚀 in PyTorch &gt; ONNX &gt; CoreML &gt; TFLite</p><h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><h3 id="1、下载代码"><a href="#1、下载代码" class="headerlink" title="1、下载代码"></a>1、下载代码</h3><p>项目地址：<a href="https://github.com/ultralytics/YOLOv5%EF%BC%8C%E6%9C%80%E8%BF%91%E4%BD%9C%E8%80%85%E5%8F%88%E6%9B%B4%E6%96%B0%E4%BA%86%E4%B8%80%E4%BA%9B%E4%BB%A3%E7%A0%81%E3%80%82">https://github.com/ultralytics/YOLOv5，最近作者又更新了一些代码。</a></p><p><img src="https://img-blog.csdnimg.cn/2020102323275322.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2hoaGhoaGhoaGh3d3d3d3d3d3d3,size_16,color_FFFFFF,t_70"></p><p>点击并拖拽以移动</p><h3 id="2、配置环境"><a href="#2、配置环境" class="headerlink" title="2、配置环境"></a>2、配置环境</h3><p>matplotlib&gt;&#x3D;3.2.2</p><p>numpy&gt;&#x3D;1.18.5</p><p>opencv-python&gt;&#x3D;4.1.2</p><p>pillow</p><p>PyYAML&gt;&#x3D;5.3</p><p>scipy&gt;&#x3D;1.4.1</p><p>tensorboard&gt;&#x3D;2.2</p><p>torch&gt;&#x3D;1.6.0</p><p>torchvision&gt;&#x3D;0.7.0</p><p>tqdm&gt;&#x3D;4.41.0<br>点击并拖拽以移动</p><h3 id="3、准备数据集"><a href="#3、准备数据集" class="headerlink" title="3、准备数据集"></a>3、准备数据集</h3><p>数据集采用Labelme标注的数据格式，数据集从RSOD数据集中获取了飞机和油桶两类数据集，并将其转为Labelme标注的数据集。</p><p>数据集的地址： <a href="https://pan.baidu.com/s/1iTUpvA9_cwx1qiH8zbRmDg">https://pan.baidu.com/s/1iTUpvA9_cwx1qiH8zbRmDg</a></p><p>提取码：gr6g</p><p>或者：LabelmeData.zip_yolov5实战-深度学习文档类资源-CSDN下载</p><p>将下载的数据集解压后放到工程的根目录。为下一步生成测试用的数据集做准备。如下图：</p><p><img src="https://img-blog.csdnimg.cn/2020102323280512.png"></p><p>点击并拖拽以移动</p><h3 id="4、生成数据集"><a href="#4、生成数据集" class="headerlink" title="4、生成数据集"></a>4、生成数据集</h3><p>YoloV5的数据集和以前版本的数据集并不相同，我们先看一下转换后的数据集。</p><p>数据结构如下图：</p><p><img src="https://img-blog.csdnimg.cn/20200924200709826.png"></p><p>点击并拖拽以移动</p><p>images文件夹存放train和val的图片</p><p>labels里面存放train和val的物体数据，里面的每个txt文件和images里面的图片是一一对应的。</p><p>txt文件的内容如下：</p><p><img src="https://img-blog.csdnimg.cn/2020092420072036.png"></p><p>点击并拖拽以移动</p><p>格式：物体类别 x y w h  </p><p>坐标是不是真实的坐标，是将坐标除以宽高后的计算出来的，是相对于宽和高的比例。</p><p>下面我们编写生成数据集的代码，新建LabelmeToYoloV5.py，然后写入下面的代码。</p><p>import os</p><p>import numpy as np</p><p>import json</p><p>from glob import glob</p><p>import cv2</p><p>from sklearn.model_selection import train_test_split</p><p>from os import getcwd<br>classes &#x3D; [“aircraft”, “oiltank”]</p><h1 id="1-标签路径"><a href="#1-标签路径" class="headerlink" title="1.标签路径"></a>1.标签路径</h1><p>labelme_path &#x3D; “LabelmeData&#x2F;“</p><p>isUseTest &#x3D; True  # 是否创建test集</p><h1 id="3-获取待处理文件"><a href="#3-获取待处理文件" class="headerlink" title="3.获取待处理文件"></a>3.获取待处理文件</h1><p>files &#x3D; glob(labelme_path + “*.json”)</p><p>files &#x3D; [i.replace(“\“, “&#x2F;“).split(“&#x2F;“)[-1].split(“.json”)[0] for i in files]</p><p>print(files)</p><p>if isUseTest:</p><pre><code>trainval_files, test_files = train_test_split(files, test_size=0.1, random_state=55)</code></pre><p>else:</p><pre><code>trainval_files = files</code></pre><h1 id="split"><a href="#split" class="headerlink" title="split"></a>split</h1><p>train_files, val_files &#x3D; train_test_split(trainval_files, test_size&#x3D;0.1, random_state&#x3D;55)<br>def convert(size, box):</p><pre><code>dw = 1. / (size[0])dh = 1. / (size[1])x = (box[0] + box[1]) / 2.0 - 1y = (box[2] + box[3]) / 2.0 - 1w = box[1] - box[0]h = box[3] - box[2]x = x * dww = w * dwy = y * dhh = h * dhreturn (x, y, w, h)</code></pre><p>wd &#x3D; getcwd()</p><p>print(wd)<br>def ChangeToYolo5(files, txt_Name):</p><pre><code>if not os.path.exists(&#39;tmp/&#39;):    os.makedirs(&#39;tmp/&#39;)list_file = open(&#39;tmp/%s.txt&#39; % (txt_Name), &#39;w&#39;)for json_file_ in files:    json_filename = labelme_path + json_file_ + &quot;.json&quot;    imagePath = labelme_path + json_file_ + &quot;.jpg&quot;    list_file.write(&#39;%s/%s\n&#39; % (wd, imagePath))    out_file = open(&#39;%s/%s.txt&#39; % (labelme_path, json_file_), &#39;w&#39;)    json_file = json.load(open(json_filename, &quot;r&quot;, encoding=&quot;utf-8&quot;))    height, width, channels = cv2.imread(labelme_path + json_file_ + &quot;.jpg&quot;).shape    for multi in json_file[&quot;shapes&quot;]:        points = np.array(multi[&quot;points&quot;])        xmin = min(points[:, 0]) if min(points[:, 0]) &gt; 0 else 0        xmax = max(points[:, 0]) if max(points[:, 0]) &gt; 0 else 0        ymin = min(points[:, 1]) if min(points[:, 1]) &gt; 0 else 0        ymax = max(points[:, 1]) if max(points[:, 1]) &gt; 0 else 0        label = multi[&quot;label&quot;]        if xmax &lt;= xmin:            pass        elif ymax &lt;= ymin:            pass        else:            cls_id = classes.index(label)            b = (float(xmin), float(xmax), float(ymin), float(ymax))            bb = convert((width, height), b)            out_file.write(str(cls_id) + &quot; &quot; + &quot; &quot;.join([str(a) for a in bb]) + &#39;\n&#39;)            print(json_filename, xmin, ymin, xmax, ymax, cls_id)</code></pre><p>ChangeToYolo5(train_files, “train”)</p><p>ChangeToYolo5(val_files, “val”)</p><p>ChangeToYolo5(test_files, “test”)<br>点击并拖拽以移动<br>这段代码执行完成会在LabelmeData生成每个图片的txt标注数据，同时在tmp文件夹下面生成训练集、验证集和测试集的txt，txt记录的是图片的路径，为下一步生成YoloV5训练和测试用的数据集做准备。在tmp文件夹下面新建MakeData.py文件，生成最终的结果，目录结构如下图：</p><p><img src="https://img-blog.csdnimg.cn/20201023232827265.png"></p><p>点击并拖拽以移动</p><p>打开MakeData.py,写入下面的代码。</p><p>import shutil<br>import os</p><p>file_List &#x3D; [“train”, “val”, “test”]<br>for file in file_List:<br>    if not os.path.exists(‘..&#x2F;VOC&#x2F;images&#x2F;%s’ % file):<br>        os.makedirs(‘..&#x2F;VOC&#x2F;images&#x2F;%s’ % file)<br>    if not os.path.exists(‘..&#x2F;VOC&#x2F;labels&#x2F;%s’ % file):<br>        os.makedirs(‘..&#x2F;VOC&#x2F;labels&#x2F;%s’ % file)<br>    print(os.path.exists(‘..&#x2F;tmp&#x2F;%s.txt’ % file))<br>    f &#x3D; open(‘..&#x2F;tmp&#x2F;%s.txt’ % file, ‘r’)<br>    lines &#x3D; f.readlines()<br>    for line in lines:<br>        print(line)<br>        line &#x3D; “&#x2F;“.join(line.split(‘&#x2F;‘)[-5:]).strip()<br>        shutil.copy(line, “..&#x2F;VOC&#x2F;images&#x2F;%s” % file)<br>        line &#x3D; line.replace(‘JPEGImages’, ‘labels’)<br>        line &#x3D; line.replace(‘jpg’, ‘txt’)<br>        shutil.copy(line, “..&#x2F;VOC&#x2F;labels&#x2F;%s&#x2F;“ % file)<br>点击并拖拽以移动<br>执行完成后就可以生成YoloV5训练使用的数据集了。结果如下：</p><p><img src="https://img-blog.csdnimg.cn/20201023232908956.png"></p><p>点击并拖拽以移动</p><p>5、修改配置参数<br>打开voc.yaml文件，修改里面的配置参数<br>点击并拖拽以移动<br>train: VOC&#x2F;images&#x2F;train&#x2F;  # 训练集图片的路径<br>点击并拖拽以移动<br>val: VOC&#x2F;images&#x2F;val&#x2F;  # 验证集图片的路径<br>点击并拖拽以移动</p><h1 id="number-of-classes"><a href="#number-of-classes" class="headerlink" title="number of classes"></a>number of classes</h1><p>nc: 2 #检测的类别，本次数据集有两个类别所以写2</p><h1 id="class-names"><a href="#class-names" class="headerlink" title="class names"></a>class names</h1><p>names: [“aircraft”, “oiltank”]#类别的名称，和转换数据集时的list对应<br>点击并拖拽以移动<br>6、修改train.py的参数<br>cfg参数是YoloV5 模型的配置文件，模型的文件存放在models文件夹下面，按照需求填写不同的文件。<br>点击并拖拽以移动<br>weights参数是YoloV5的预训练模型，和cfg对应，例：cfg配置的是yolov5s.yaml，weights就要配置yolov5s.pt<br>点击并拖拽以移动<br>data是配置数据集的配置文件，我们选用的是voc.yaml，所以配置data&#x2F;voc.yaml<br>点击并拖拽以移动<br>修改上面三个参数就可以开始训练了，其他的参数根据自己的需求修改。修改后的参数配置如下：<br>点击并拖拽以移动<br>parser.add_argument(‘–weights’, type&#x3D;str, default&#x3D;’yolov5s.pt’, help&#x3D;’initial weights path’)</p><p>parser.add_argument(‘–cfg’, type&#x3D;str, default&#x3D;’yolov5s.yaml’, help&#x3D;’model.yaml path’)</p><p>parser.add_argument(‘–data’, type&#x3D;str, default&#x3D;’data&#x2F;voc.yaml’, help&#x3D;’data.yaml path’)<br>点击并拖拽以移动<br> 修改完成后，就可以开始训练了。如下图所示：</p><p><img src="https://img-blog.csdnimg.cn/20201107122234540.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2hoaGhoaGhoaGh3d3d3d3d3d3d3,size_16,color_FFFFFF,t_70"></p><p>点击并拖拽以移动</p><p>点击并拖拽以移动</p><h3 id="7、查看训练结果"><a href="#7、查看训练结果" class="headerlink" title="7、查看训练结果"></a>7、查看训练结果</h3><p>在经历了300epoch训练之后，我们会在runs文件夹下面找到训练好的权重文件和训练过程的一些文件。如图：</p><p><img src="https://img-blog.csdnimg.cn/20201107122322539.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2hoaGhoaGhoaGh3d3d3d3d3d3d3,size_16,color_FFFFFF,t_70"></p><p><img src="https://img-blog.csdnimg.cn/20201107122331603.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2hoaGhoaGhoaGh3d3d3d3d3d3d3,size_16,color_FFFFFF,t_70"></p><p><img src="https://img-blog.csdnimg.cn/20201107122341529.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2hoaGhoaGhoaGh3d3d3d3d3d3d3,size_16,color_FFFFFF,t_70"></p><p><img src="https://img-blog.csdnimg.cn/20201107122348665.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2hoaGhoaGhoaGh3d3d3d3d3d3d3,size_16,color_FFFFFF,t_70"></p><p><img src="https://img-blog.csdnimg.cn/20201107122405745.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2hoaGhoaGhoaGh3d3d3d3d3d3d3,size_16,color_FFFFFF,t_70"></p><h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><p>首先需要在voc.yaml中增加测试集的路径，打开voc.yaml，在val字段后面增加test: tmp&#x2F;test.txt这行代码，如图：</p><p><img src="https://img-blog.csdnimg.cn/20201107122414380.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2hoaGhoaGhoaGh3d3d3d3d3d3d3,size_16,color_FFFFFF,t_70"></p><p>点击并拖拽以移动</p><p>修改test.py中的参数，下面的这几个参数要修改。</p><p>parser &#x3D; argparse.ArgumentParser(prog&#x3D;’test.py’)<br>parser.add_argument(‘–weights’, nargs&#x3D;’+’, type&#x3D;str, default&#x3D;’runs&#x2F;exp7&#x2F;weights&#x2F;best.pt’, help&#x3D;’model.pt path(s)’)<br>parser.add_argument(‘–data’, type&#x3D;str, default&#x3D;’data&#x2F;voc.yaml’, help&#x3D;’*.data path’)<br>parser.add_argument(‘–batch-size’, type&#x3D;int, default&#x3D;2, help&#x3D;’size of each image batch’)<br>parser.add_argument(‘–save-txt’, default&#x3D;’True’, action&#x3D;’store_true’, help&#x3D;’save results to *.txt’)</p><p>在275行 修改test的方法，增加保存测试结果的路径。这样测试完成后就可以在inference\images查看到测试的图片，在inference\output中查看到保存的测试结果。</p><p>如图：</p><p><img src="https://img-blog.csdnimg.cn/20201107122433991.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2hoaGhoaGhoaGh3d3d3d3d3d3d3,size_16,color_FFFFFF,t_70"></p><p>点击并拖拽以移动</p><p>下面是运行的结果：</p><p><img src="https://img-blog.csdnimg.cn/20201107122442924.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2hoaGhoaGhoaGh3d3d3d3d3d3d3,size_16,color_FFFFFF,t_70"></p><p>点击并拖拽以移动</p><p>代码和模型：手把手教物体检测yolov5-master.zip_yolov5数字识别-深度学习文档类资源-CSDN下载</p><p>点击并拖拽以移动</p><p>转载自华为云社区</p><p>[]: <a href="https://bbs.huaweicloud.com/blogs/330644">https://bbs.huaweicloud.com/blogs/330644</a></p>]]></content>
      
      
      <categories>
          
          <category> yolov5 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 学习，小白，encrypted </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>上海交大提出CDNet：基于改进YOLOv5的斑马线和汽车过线行为检测</title>
      <link href="/2022/03/25/shanghaidaxueyolo/"/>
      <url>/2022/03/25/shanghaidaxueyolo/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>CDNet: A Real-Time and Robust Crosswalk Detection Network on Jetson Nano Based on YOLOv5</p><p>CDNet: 一个基于YOLOv5的在Jetson Nano上实时、鲁棒的斑马线检测网络</p><p>作者：Zheng-De Zhang, Meng-Lu Tan, Zhi-Cai Lan, Hai-Chun Liu, Ling Pei, Wen-Xian Yu</p><p>时间：Feb, 2022</p><p>期刊：Neural Computing &amp; Applications, IF 5.6</p><p>一个神经网络在具体场景（斑马线检测）的应用，改进YOLOv5，提出多项tricks，数据集和复现代码开源！</p><span id="more"></span><p>摘要：</p><p><img src="https://pic2.zhimg.com/80/v2-0960db19bc893177adc3eeeff87a9985_720w.jpg"></p><p>图1 图形摘要</p><p>在复杂场景和有限计算能力下实现实时、鲁棒的斑马线（人行横道）检测是当前智能交通管理系统（ITMS）的重要难点之一。有限的边缘计算能力和多云、晴天、雨天、雾天和夜间等真实复杂的场景同时对这项任务提出了挑战。本研究提出基于改进YOLOv5的人行横道检测网络（CDNet），实现车载摄像头视觉下快速准确的人行横道检测，并在Jetson nano设备上实现实时检测。强大的卷积神经网络特征提取器用于处理复杂环境，网络中嵌入了squeeze-and-excitation（SE）注意力机制模块，使用负样本训练（NST）方法提高准确率，利用感兴趣区域（ROI）算法进一步提高检测速度，提出了一种新的滑动感受野短时向量记忆（SSVM）算法来提高车辆交叉行为检测精度，使用合成雾增强算法允许模型适应有雾的场景。最后，在 Jetson nano 上以 33.1 FPS 的检测速度，我们在上述复杂场景中获得了 94.83% 的平均 F1 分数。对于晴天和阴天等更好的天气条件，F1 分数超过 98%。该工作为人工神经网络算法优化方法在边缘计算设备上的具体应用提供了参考，发布的模型为智能交通管理系统提供了算法支持。</p><p>贡献：</p><ul><li>注意力机制网络改进网络，提升精度，略微降低速度：SENet (Squeeze-and-Excitation Network)</li><li><img src="https://pic2.zhimg.com/80/v2-1e5666756525bb9665445bd47939d735_720w.jpg"></li></ul><p>图2 改进，SENet嵌入到CSP中形成SE-CSP</p><ul><li>负样本训练，提升精度，速度不变: NST (Negative Samples Training)</li><li><img src="https://pic2.zhimg.com/80/v2-053ebc13c731af4192f246150d4f99e9_720w.jpg"></li></ul><p>图3 负样本训练</p><ul><li>感兴趣区域，提升速度，精度下降：ROI (Region Of Interest)</li><li><img src="https://pic1.zhimg.com/80/v2-0a1b7f29a483579536fd6889ba193440_720w.jpg"></li></ul><p>图4 ROI</p><ul><li>滑动感受野短时向量记忆算法，迁移斑马线检测任务到汽车过线行为检测任务，提升精度，速度不变：SSVM (Slide receptive field Short-term Vectors Memory)</li><li><img src="https://pic2.zhimg.com/80/v2-08703a6889a5daf9237b8f4edf82d171_720w.jpg"></li></ul><p>图5 SSVM</p><ul><li>合成雾增强算法，增强数据集，适应雾天，提升精度，速度不变：SFA (Synthetic Fog Augment)</li><li><img src="https://pic3.zhimg.com/80/v2-01a1f8ac0baa37892927ffa6ba11930e_720w.jpg"></li></ul><p>图6 SFA</p><ul><li><p>斑马数据集：标注好的，车载摄像头视角下的，共计6868张图像。</p></li><li><p>复现代码：见github项目主页</p></li></ul><p>结果</p><p><img src="https://pic1.zhimg.com/80/v2-344b24beda5098b71df845407ac7e740_720w.jpg"></p><p>CDNet最终在Jetson nano上实现了33.1FPS实时检测，多云、晴天、雨天、雾天和夜间多个场景平均检测F1分数94.72%。</p><p>与原生YOLOv5相比，检测尺寸为640时，CDNet 在 Jetson nano 上提高了5.13%的F1分数和10.7FPS的速度， 检测尺寸为288时，提升为13.38%的F1分数和13.1FPS。</p><p>相关链接</p><p>数据集、复现代码：<a href="https://github.com/zhangzhengde0225/CDNet">https://github.com/zhangzhengde0225/CDNet</a></p><p>视频样例：<a href="https://www.bilibili.com/video/BV1qf4y1B7BA">https://www.bilibili.com/video/BV1qf4y1B7BA</a></p><p>阅读全文：<a href="https://rdcu.be/cHuc8">https://rdcu.be/cHuc8</a></p><p>下载全文：<a href="https://doi.org/10.1007/s00521-022-07007-9">https://doi.org/10.1007/s00521-022-07007-9</a></p><p>[]: <a href="https://link.springer.com/epdf/10.1007/s00521-022-07007-9?sharing_token=DRB13ldb4pD1uh0V0yNPCPe4RwlQNchNByi7wbcMAY5KArlu0TX_ZFp7-m7RpG0Y4Bu0AQ2fiGCsxWTMz57dozRmmcIpAneboVR39q4l4hZnWYgxVpqVOT5VpGAX09KHIhzl3er39RZX7DksHxu9tOZ41x-Irl7N1bv7VW814ko=">https://link.springer.com/epdf/10.1007/s00521-022-07007-9?sharing_token=DRB13ldb4pD1uh0V0yNPCPe4RwlQNchNByi7wbcMAY5KArlu0TX_ZFp7-m7RpG0Y4Bu0AQ2fiGCsxWTMz57dozRmmcIpAneboVR39q4l4hZnWYgxVpqVOT5VpGAX09KHIhzl3er39RZX7DksHxu9tOZ41x-Irl7N1bv7VW814ko%3D</a></p>]]></content>
      
      
      <categories>
          
          <category> test </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 学习，小白，encrypted </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>又来!项目如何改进YOLOv5？这篇告诉你如何修改让检测更快、更稳！！！</title>
      <link href="/2022/03/24/possibleupforyolov5/"/>
      <url>/2022/03/24/possibleupforyolov5/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p><img src="http://mianbaoban-assets.oss-cn-shenzhen.aliyuncs.com/xinyu-images/MBXY-CR-160c10bce0fdfa84ad486ec364b243fe.png"></p><p><strong>交通标志检测对于无人驾驶系统来说是一项具有挑战性的任务，尤其是多尺度目标检测和检测的实时性问题。在交通标志检测过程中，目标的规模变化很大，会对检测精度产生一定的影响。特征金字塔是解决这一问题的常用方法，但它可能会破坏交通标志在不同尺度上的特征一致性。而且，在实际应用中，普通方法难以在保证实时检测的同时提高多尺度交通标志的检测精度。</strong></p><span id="more"></span><p><strong>本文提出了一种改进的特征金字塔模型AF-FPN，该模型利用自适应注意模块(adaptive attention module, AAM)和特征增强模块(feature enhancement module, FEM)来减少特征图生成过程中的信息丢失，进而提高特征金字塔的表示能力。将YOLOv5中原有的特征金字塔网络替换为AF-FPN，在保证实时检测的前提下，提高了YOLOv5网络对多尺度目标的检测性能。</strong></p><p><strong>此外，提出了一种新的自动学习数据增强方法，以丰富数据集，提高模型的鲁棒性，使其更适合于实际场景。在100K (TT100K)数据集上的大量实验结果表明，与几种先进方法相比，本文方法的有效性和优越性得到了验证。</strong></p><h2 id="1介绍"><a href="#1介绍" class="headerlink" title="1介绍"></a>1介绍</h2><p>交通标志识别系统是ITS和无人驾驶系统的重要组成部分。如何提高交通标志检测与识别技术的准确性和实时性，是该技术走向实际应用时需要解决的关键问题。</p><p>近年来，大多数先进的目标检测算法，如Faster R-CNN、R-FCN、SSD和YOLO，都使用了卷积神经网络，并在目标检测任务中取得了丰硕的成果。然而，将这些方法简单地应用到交通标志识别中很难取得满意的效果。车载移动终端的目标识别和检测对不同尺度的目标要求较高的精度，对识别速度要求较高，这意味着要满足准确性和实时性两个要求。</p><p>传统的CNN通常需要大量的参数和浮点运算(FLOPs)来达到令人满意的精度，例如ResNet-50有大约25.6万个参数，需要41亿个浮点运算来处理224×224大小的图像。然而，内存和计算资源有限的移动设备(如智能手机和自动驾驶汽车)无法用于更大网络的部署和推理。YOLOv5作为一种One-stage检测器，具有计算量小、识别速度快等优点。</p><p>本文提出了一种改进的YOLOv5网络，既保证模型尺寸满足部署在车辆侧的要求，又提高了多尺度目标的能力，满足实时性要求。</p><p>工作的主要贡献如下:</p><p>提出了一种新的特征金字塔网络。通过自适应特征融合和感受野增强，在特征传递过程中很大程度上保留通道信息，并自适应学习每个特征图中的不同感受野，增强特征金字塔的表示，有效地提高了多尺度目标识别的精度;</p><p>提出了一种新的自动学习数据增强策略。受AutoAugment的启发，添加了最新的数据增强操作。改进的数据增强方法有效地提高了模型训练效果和训练模型的鲁棒性，具有更大的现实意义;</p><p>与现有的YOLOv5网络不同，对当前版本进行了改进，以减少尺度不变性的影响。同时，它可以部署在车辆的移动终端上，对交通标志进行实时检测和识别。</p><h2 id="2相关工作"><a href="#2相关工作" class="headerlink" title="2相关工作"></a>2相关工作</h2><h3 id="2-1-基于CNN的交通标识检测"><a href="#2-1-基于CNN的交通标识检测" class="headerlink" title="2.1 基于CNN的交通标识检测"></a>2.1 基于CNN的交通标识检测</h3><p>目前，卷积神经网络在视觉目标检测方面取得了很大的成功。根据是否需要提出区域建议，基于深度学习的目标检测可分为两类:单阶段检测和两阶段检测。</p><p>Shao等人提出了一种区域建议算法来简化Gabor小波，提高Faster R-CNN用于交通标志检测。Zhang等人提出了一种改进的基于YOLOv2的交通标志检测器，修改了经典YOLOv2网络的卷积层数，使其适合中国交通标志数据集。Li等人开发了一种新的感知生成对抗网络，该网络通过生成小交通标志的超分辨率表示来提高检测性能。SADANet结合域自适应网络和多尺度预测网络来解决尺度变化问题。</p><p>上述网络大多采用单尺度的深度特征，难以提高复杂场景下的检测和识别性能。大型和小型交通标志具有完全不同的视觉特征，因此规模变化问题是交通标志检测与识别中的一个难题。对于目标检测，学习尺度不变表示对于识别和定位目标至关重要。目前的工作主要从两个方面来解决这一挑战，即网络架构和数据扩充。</p><p>目前，多尺度特征被广泛应用于高层目标识别中，以提高多尺度目标的识别性能。特征金字塔网络(Feature Pyramid Network, FPN)是一种常用的多层特征融合方法，利用其多尺度表达能力衍生出许多检测精度较高的网络，如Mask R-CNN和RetinaNet。</p><p>值得注意的是，由于功能通道的减少，特征图会出现信息丢失，并且在其他level的特征图中只包含一些不太相关的上下文信息。</p><p>此外，使用FPN会导致网络过分注重Low-level特征的优化，有时会导致对大规模目标的检测精度降低。针对这一问题，提出了一种简单而有效的方法，即感受野金字塔(RFP)，以增强特征金字塔的表示能力，并驱动网络学习最优的特征融合模式。</p><h3 id="2-2-数据增强"><a href="#2-2-数据增强" class="headerlink" title="2.2 数据增强"></a>2.2 数据增强</h3><p>数据增强已被广泛应用于网络优化，并被证明有利于视觉任务，可以提高CNN的性能，防止过拟合，且易于实现。</p><p>数据增强方法大致可以分为颜色操作(如亮度、对比度和颜色投射)和几何操作(如缩放、翻转、平移和缩放)。</p><p>这些增强操作通过数据扭曲或过采样人为地扩大了训练数据集的大小。</p><p>Lv等人提出了五种针对人脸图像的数据增强方法，包括landmark抖动和四种合成方法(发型、眼镜、姿势、照明)。</p><p>Nair等人对训练数据应用了两种形式的数据增强。一种是随机裁剪和水平反射，另一种是通过在颜色空间上应用PCA来改变RGB通道的强度。这些常用的方法只是做简单的转换，不能满足复杂情况的需求。</p><p>Dwibedi等人通过cut-paste策略提高了检测性能。</p><p>此外，InstaBoost使用带注释的实例mask和位置概率图来增强训练图像。</p><p>YOLOv4和Stitcher引入了包含重新缩放子图像的Mosaic输入，这也在YOLOv5中使用。然而，这些数据增强实现是手工设计的，最佳的增强策略是特定于数据集的。</p><p>为了避免数据增强的数据特定性质，最近的工作集中在直接从数据本身学习数据增强策略。Tran等人使用贝叶斯方法根据从训练集中学习到的分布生成增广数据。Cubuk等人提出了一种名为AutoAugment的数据增强新方法，用于自动搜索改进的数据增强策略。</p><h2 id="3本文方法"><a href="#3本文方法" class="headerlink" title="3本文方法"></a>3本文方法</h2><h3 id="3-1-Improved-YOLOv5s架构"><a href="#3-1-Improved-YOLOv5s架构" class="headerlink" title="3.1 Improved YOLOv5s架构"></a>3.1 Improved YOLOv5s架构</h3><p>作为目前YOLO系列的最新版本，YOLOv5优越的灵活性使得它可以方便地快速部署在车辆硬件侧。YOLOv5包含YOLOv5s、YOLOv5m、YOLOv5l, YOLOv5x。</p><p>YOLOv5s是YOLO系列中最小的版本，由于其内存大小为14.10M，更适合部署在车载移动硬件平台上，但其识别精度不能满足准确高效识别的要求，尤其是对小目标的识别。</p><p>YOLOv5的基本框架可以分为4个部分:Input、Backbone、Neck和Prediction。</p><p>Input部分通过拼接数据增强来丰富数据集，对硬件设备要求低，计算成本低。但是，这会导致数据集中原有的小目标变小，导致模型的泛化性能下降。</p><p>Backbone部分主要由CSP模块组成，通过CSPDarknet53进行特征提取。</p><p>在Neck中使用FPN和路径聚合网络(PANet)来聚合该阶段的图像特征。</p><p>最后，网络进行目标预测并通过预测输出。</p><p>本文引入AF-FPN和自动学习数据增强，解决模型大小与识别精度不兼容的问题，进一步提高模型的识别性能。用AF-FPN代替原来的FPN结构，提高了多尺度目标识别能力，在识别速度和精度之间进行了有效的权衡。此外，去除原网络中的Mosaic增强，根据自动学习数据增强策略使用最佳的数据增强方法来丰富数据集，提高训练效果。改进后的YOLOv5s网络结构如图1所示。</p><p><img src="http://mianbaoban-assets.oss-cn-shenzhen.aliyuncs.com/xinyu-images/MBXY-CR-2475c67134453dbc97d14d1e824172a8.png"></p><p>图1 改进后的YOLOv5<br>在预测中，使用Generalized IoU (GIoU) Loss作为BBox的损失函数，使用加权的非最大抑制(NMS)方法NMS。损失函数如下:</p><p><img src="http://mianbaoban-assets.oss-cn-shenzhen.aliyuncs.com/xinyu-images/MBXY-CR-16d5ef3a340a412bc0d57e421a705f9c.png"></p><p>其中是覆盖和的最小方框。为ground-truth box,为predicted box。</p><p>但是，当预测框在ground-truth 框内且预测框大小相同时，预测框与ground-truth框的相对位置无法区分。</p><p>本文用Complete IoU(CIoU) Loss代替GIoU Loss。CIoU损失在考虑GIoU损失的基础上，考虑了BBox的重叠面积、中心点距离以及BBox长宽比的一致性。损失函数可以定义为:</p><p><img src="http://mianbaoban-assets.oss-cn-shenzhen.aliyuncs.com/xinyu-images/MBXY-CR-28dae6e295685858b7c2449fa39bc6b8.png"></p><p>其中是惩罚项，通过最小化两个BBox中心点之间的归一化距离来定义。和表示和的中心点，为欧几里得距离，c为覆盖两个方框的最小封闭方框的对角线长度。是一个正的权衡参数，衡量纵横比的一致性。</p><p>权衡参数定义为:</p><p><img src="http://mianbaoban-assets.oss-cn-shenzhen.aliyuncs.com/xinyu-images/MBXY-CR-ecb23a759ecdad7a8941753adc4d0a6a.png"></p><p>其中重叠面积因子在回归中具有较高的优先级，特别是在非重叠情况下。</p><h3 id="3-2-架构改进"><a href="#3-2-架构改进" class="headerlink" title="3.2 架构改进"></a>3.2 架构改进</h3><p>1、AF-FPN<br>AF-FPN在传统特征金字塔网络的基础上，增加了自适应注意力模块(AAM)和特征增强模块(FEM)。前者减少了特征通道，减少了高层特征图中上下文信息的丢失。后一部分增强了特征金字塔的表示，提高了推理速度，同时实现了最先进的性能。AF-FPN结构如图2所示。</p><p><img src="http://mianbaoban-assets.oss-cn-shenzhen.aliyuncs.com/xinyu-images/MBXY-CR-62b4052f91fe4a7953e265f87d14c892.png"></p><p>图2 AF-FPN结构<br>输入图像通过多个卷积生成特征映射{C1, C2, C3, C4, C5}。C5通过AAM生成特征映射M6。M6与M5求和并通过自上而下的途径传播与较低层次的其他特征融合，通过扩展感受域每次融合后的有限元分析。PANet缩短了底层与顶层特征之间的信息路径。</p><p>自适应注意模块的操作可以分为2个步骤：</p><p>首先，通过自适应平均池化层获得不同尺度的多个上下文特征。池化系数为[0.1,0.5]，根据数据集的目标大小自适应变化。<br>其次，通过空间注意力机制，为每个特征图生成空间权值图。通过权重图融合上下文特征，生成包含多尺度上下文信息的新特征图。<br>2、AAM<br>AAM的具体结构如图3所示。</p><p><img src="http://mianbaoban-assets.oss-cn-shenzhen.aliyuncs.com/xinyu-images/MBXY-CR-ef25aace3bbf87784d249bafc0aa58b5.png"></p><p>图3　AAM结构<br>作为自适应注意力模块的输入，C5的大小为S&#x3D;h×w，</p><p>首先通过自适应池化层获得不同尺度的语义特征。<br>然后，每个上下文特征进行1×1卷积，以获得相同的通道维数256。利用双线性插值将它们上采样到S尺度，进行后续融合。空间注意力机制通过Concat层将3个上下文特征的通道进行合并；<br>然后特征图依次经过1×1卷积层、ReLU激活层、3×3卷积层和sigmoid激活层，为每个特征图生成相应的空间权值。生成的权值映射和合并通道后的特征映射经过Hadamard乘积操作，将其分离并添加到输入特征映射M5中，将上下文特征聚合为M6。<br>最终得到的特征图具有丰富的多尺度上下文信息，在一定程度上缓解了由于通道数量减少而造成的信息丢失。<br>3、FEM<br>FEM主要是根据检测到的交通标志尺度的不同，利用扩张卷积自适应地学习每个特征图中的不同感受野，从而提高多尺度目标检测识别的准确性。</p><p>图4　FEM结构<br>如图4所示，它可以分为两部分:</p><p><img src="http://mianbaoban-assets.oss-cn-shenzhen.aliyuncs.com/xinyu-images/MBXY-CR-d5c5f330d7f401c096d31d61e74c678e.png"></p><p>多分支卷积层</p><p>分支池化层</p><p>多分支卷积层通过扩张卷积为输入特征图提供不同大小的感受野。利用平均池化层融合来自三个支路感受野的交通信息，提高多尺度精度预测。</p><p>多分支卷积层包括扩张卷积、BN层和ReLU激活层。三个平行分支中的扩张卷积具有相同的内核大小，但扩张速率不同。具体来说，每个扩张卷积的核为3×3，不同分支的扩张速率d分别为1、3、5。</p><p>扩展卷积支持指数扩展的感受野，而不损失分辨率。而在扩张卷积的卷积运算中，卷积核的元素是间隔的，空间的大小取决于膨胀率，这与标准卷积运算中卷积核的元素都是相邻的不同。</p><p>卷积核由3×3更改为7×7，该层的感受野为7×7。扩张卷积的感受野公式为:</p><p><img src="http://mianbaoban-assets.oss-cn-shenzhen.aliyuncs.com/xinyu-images/MBXY-CR-a031f9a8824f46096688ec34a449b219.png"></p><p>其中k和分别表示kernel-size和膨胀率。d表示卷积的stride。</p><p>分支池化层用于融合来自不同并行分支的信息，避免引入额外参数。在训练过程中，利用平均操作来平衡不同平行分支的表示，使单个分支在测试过程中实现推理。表达式如下:</p><p><img src="http://mianbaoban-assets.oss-cn-shenzhen.aliyuncs.com/xinyu-images/MBXY-CR-2e89dddbd103d90f6b0bb0121c759972.png"></p><p>其中表示分支池化层的输出。B表示并行分支的数量，这里设B &#x3D; 3。</p><h3 id="3-3-数据增强"><a href="#3-3-数据增强" class="headerlink" title="3.3 数据增强"></a>3.3 数据增强</h3><p>扩展策略由搜索空间和搜索算法两部分组成。在搜索空间中，有S&#x3D;5个子策略，每个子策略由两个图像操作组成，依次应用。随机选择一个子策略并应用于当前图像。此外，每个操作还与两个超参数相关:应用操作的概率和操作的大小。</p><p>在实验中使用的操作包括最新的数据增强方法，如Mosaic、SnapMix、Earsing、CutMix、Mixup和Translate X&#x2F;Y。在搜索空间中总共有15个操作。每个操作也有一个默认的幅度范围。将幅度的范围离散为D&#x3D;11等间距值，以便可以使用离散搜索算法来找到它们。</p><p>类似地，还将对P&#x3D;10个值(均匀间距)进行操作的概率离散化。在个可能性的空间中找到每个子策略成为一个搜索问题。因此，包含5个子策略的搜索空间大约有可能性，需要一个高效的搜索算法来导航该空间。图5显示了搜索空间中包含5个子策略的策略。</p><p><img src="http://mianbaoban-assets.oss-cn-shenzhen.aliyuncs.com/xinyu-images/MBXY-CR-180ef093beb29cbe7ed749c5aa4bbc0c.png"></p><p>图5<br>通过搜索空间，将搜索学习到的增广策略问题转化为离散优化问题。采用强化学习作为搜索算法，它包含两个部分:控制器RNN和训练算法。控制器RNN为递归神经网络，训练算法为近端策略优化(PPO)，学习率为0.00035。控制器RNN在每一步预测softmax产生的决策，然后将预测作为搜索空间的嵌入，送入下一步。控制器总共有30个softmax预测来预测5个子策略，每个子策略有2个操作，每个操作需要操作类型、大小和概率。将自动学习数据增强方法应用于TT100K数据集，然后使用通过训练获得的最佳数据增强策略。</p><h2 id="4实验"><a href="#4实验" class="headerlink" title="4实验"></a>4实验</h2><p><img src="http://mianbaoban-assets.oss-cn-shenzhen.aliyuncs.com/xinyu-images/MBXY-CR-dfac4e6bd51bc3327df9586bbf3a5908.png"></p><p><img src="http://mianbaoban-assets.oss-cn-shenzhen.aliyuncs.com/xinyu-images/MBXY-CR-5a24668f19a5e349dbbc6f9b03f2ed31.png"></p><h2 id="5参考"><a href="#5参考" class="headerlink" title="5参考"></a>5参考</h2><p>[1].  Improved YOLOv5 network for real-time multi-scale traffic sign detection</p><p>[]: <a href="https://arxiv.org/abs/2112.08782v1">https://arxiv.org/abs/2112.08782v1</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Deep Fusion(深度融合)是什么？Deep Fusion有多优秀？</title>
      <link href="/2022/03/24/deepfusion/"/>
      <url>/2022/03/24/deepfusion/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>2019年10月29号的大半夜，苹果放出了iOS 13.2正式版的更新，顺手还更新了一波AirPods。之前发布会上提到的iPhone 11专属的Deep Fusion也正式上线，利用A13的神经网络引擎，可以在低光照环境下获得噪点更低的照片。</p><p>​    <img src="https://pic4.zhimg.com/80/v2-966f9a4ed12e6c8b6d115a009a06b7eb_720w.jpg"></p><p>更新介绍讲的超级简单，但是小口袋知道这一切并不是那么简单！这篇文章讲讲Deep Fusion到底是个啥？用来干嘛的。</p><span id="more"></span><h2 id="DeepFusion（深度融合）简介"><a href="#DeepFusion（深度融合）简介" class="headerlink" title="DeepFusion（深度融合）简介"></a>DeepFusion（深度融合）简介</h2><p>DeepFusion中文翻译应该叫深度融合，那么是怎么个融合法呢？看图：</p><p>​            <img src="https://pic1.zhimg.com/80/v2-1d34909a6503cfc39dbe71e7ce7e904c_720w.jpg"></p><p>简而言之，9图合成1图。DeepFusion采用的技术简单来说就是，当你按下快门，相机会先拍4张短时间曝光的照片，然后再以正常标准曝光拍4张，最后拍一张长曝光获取暗部细节，然后将这九张照片自动分析，选择其中解析力最高的部分进行合成。</p><p>在A13的加持下，合成速度只需要半秒到一秒半秒左右（其他处理器由于缺少对于ISP，无法实现该功能）。最终可以得到一张2400万像素的照片（2.6MB的HEIC格式，正常情况下的Smart HDR照片约为1.4MB HEIC格式）</p><p>不得不说，苹果这一次的技术革新让我们耳目一新，当一众厂商还在疯狂堆积像素时，苹果试图用1200万像素的镜头，诠释什么才是手机相机应该有的样子。也许从一开始，苹果就与其他手机不在一条赛道上。</p><h2 id="怎么开启DeepFusion？"><a href="#怎么开启DeepFusion？" class="headerlink" title="怎么开启DeepFusion？"></a>怎么开启DeepFusion？</h2><p>DeepFusion开启不需要任何调节，拍摄方式一如既往的简单：按下快门即可，其余交给苹果。只要光照在10-600lux就会自动打开，并会在打开后自动关闭超取景模式。</p><p>值得注意的是，DeepFusion由于需要大量的计算量，连拍模式下连A13都会吃不消，因此开启时连拍会变慢，并且拍摄到照片在相册中会暂时显示为预览图，需要过一段时间才能获得处理过的完整照片。</p><p>事实上苹果故意弱化了连拍功能，以前连拍长按拍照按钮就可以了，现在用iPhone 11你再试试？</p><p>​            <img src="https://pic4.zhimg.com/80/v2-9987b6fbd49eed7a1bc6ff4f529d8c23_720w.jpg"></p><p>是的，iPhone 11将长按拍照按钮变成了快速录像，而连拍功能则需要在按下快门后快速向左滑动。这也是为了有效避免用户不小心长按拍摄按钮，导致DeepFusion疯狂开启后大量消耗资源的妥协之策。</p><p>也正是因此，DeepFusion只能用于拍照，并不能用于视频拍摄，A13再强也扛不住，如果A12X能拍DeepFusion，估计也扛不住用来拍视频。</p><h4 id="照片对比"><a href="#照片对比" class="headerlink" title="照片对比"></a>照片对比</h4><p>​          <img src="https://pic4.zhimg.com/80/v2-e623e8eb3efa2a1c7fee150b5030e863_720w.jpg"></p><p>via：Instagram@tldtoday</p><p>可以很明显看出，开启DeepFusion后照片清晰度大幅度提升，主要原因之一是DeepFusion照片像素是2400万的，比正常拍摄像素提升了一倍！</p><p>​            <img src="https://pic1.zhimg.com/80/v2-a630fc45de4b8ea1a32590e034360fc8_720w.jpg"></p><p>via：Instagram@tldtoday</p><h2 id="其他细节一览"><a href="#其他细节一览" class="headerlink" title="其他细节一览"></a>其他细节一览</h2><p>小口袋特地做了一个图片，解释一下iPhone 11系列（iPhone11  iPhone 11 Pro和Pro Max）在拍照时到底是怎么一回事：</p><p>​       <img src="https://pic4.zhimg.com/80/v2-39038b6601915ea9f20762969d6f12bb_720w.jpg">     </p><p>由于DeepFusion的功能实在太过强大，甚至有人怀疑iPhone是不是采用了一枚4800万像素的镜头进行超采样输出，然后合成为2400万像素的照片。</p><p>这也是目前DxOMark至今不敢给出iPhone 11系列拍照得分的原因：iPhone 11相机的能力，可能还有待开发，每一次升级系统都可能带来巨大的提升，这让评分变成了一件很尴尬的事情。</p><p>这也让我们对苹果似乎有了些不一样的认识，Deep  Fusion似乎是对其他一众设备上的相机的嘲讽，在其对ISP芯片和NPU的深度挖掘下，几乎没有厂商目前有能力与其抗衡。而苹果将其如此强大的处理器和相机性能，用来做了一件极其平凡而又不平凡的事情，提升当前分辨率下照片的绝对清晰度，这让我们感觉到曾经的苹果似乎回来了。</p><p>我们对未来的iOS的deepfusion大更新也有了更多期待，iPhone 11上究竟还有什么神奇的黑科技有待开发呢？比如至今苹果只字未提的U1超宽频芯片，或者是Wi-Fi 6带来的强大能力？敬请期待吧！</p>]]></content>
      
      
      
        <tags>
            
            <tag> encrypted </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>目标检测：单阶段YOLOv5及其相关应用介绍</title>
      <link href="/2022/03/23/yolo/"/>
      <url>/2022/03/23/yolo/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p><img src="https://pic1.zhimg.com/80/v2-043bc718ba401c54f9f288d94fc075ec_720w.jpg"></p><p><img src="https://pic2.zhimg.com/80/v2-93a69bb261545aea8069f7c9a1108931_720w.jpg"></p><p>​                                                                                 (jk少女日常的yolov5检测识别)<br><strong>前言——背景简介（我和yolov5从此结缘）</strong><br>2021年底我和我们大学的班导师以及几位其他班级的同学一起开始了yolo系列视觉检测框架的学习，我们从项目的开始到现在也不过才几个月而已，其实我们虽然只是学习到了一些非常基础的视觉知识，比如我们大三选修的一门课程opencv视觉基础课程，虽然是计算机里边的视觉检测部分，但是对于我们这群视觉小白（好吧视觉小白其实是我自己）时过境迁，在AI方面我还是小学生，在社会的飞速发展中，视觉检测神经网络领域里边的Yolo早已经脱胎换骨从V1发展到了V5，并有各种衍生和优化版本譬如yolox和由中国科学院大学牵头搞出来的yolof。我们津津乐道它的名字是You only look once的简写，原意“你只看一遍”是为了区分看“两遍”的faster-rcnn等二阶段模型，其简写Yolo又寓意着另一句谚语”You only live once”，生命只有一次。人生的面，见一面少一面，好好把握当下比什么都重要。</p><span id="more"></span><p><img src="/yolo/u=3064032978,2763916980&fm=253&fmt=auto&app=138&f=JPEG.jpeg"><br>充满噱头的除了性能、名字外甚至还有作者本身，作者年初时宣布因为对技术滥用的担忧放弃视觉研究了，颇有高人之风。而且明明是个大胡子，个人网站却是满满粉色独角兽的萌妹风格。写的论文中，为了突出速度之快，甚至故意把图画到坐标轴外面去了。<br><img src="https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fimg-blog.csdnimg.cn%2F2020110315301217.png%3Fx-oss-process%3Dimage%2Fwatermark%2Ctype_ZmFuZ3poZW5naGVpdGk%2Cshadow_10%2Ctext_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d1emhpaGFvNzc4OA%3D%3D%2Csize_16%2Ccolor_FFFFFF%2Ct_70&refer=http%3A%2F%2Fimg-blog.csdnimg.cn&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=auto?sec=1650554578&t=4618cbf3b833e94cb7a2a8e0e51e95e1"><br>机器视觉任务，常见的有分类、检测、分割。而Yolo正是检测中的佼佼者，在工业界，Yolo兼顾精度和速度，往往是大家的首选。<br><img src="https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fpic1.zhimg.com%2Fv2-50b4a2b8e1529817bf6bb6de0fdf4c2d_1440w.jpg%3Fsource%3D172ae18b&refer=http%3A%2F%2Fpic1.zhimg.com&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=auto?sec=1650554578&t=e23da028b4e36ecf6001bdb744da65eb"><br><strong>实际效果</strong><br>V5的效果也是非常不错，这里给到官方模型的一些数据：<br><img src="/yolo/src=http%253A%252F%252Fcdn.zhuanzhi.ai%252Fimages%252Fwx%252F4bb7ea3df6427619e23cacf33cdb06f9&refer=http%253A%252F%252Fcdn.zhuanzhi.jpeg"><br><img src="https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fpic4.zhimg.com%2Fv2-ec8f70e8983cd0a121bc4b1917dc36d7_1440w.jpg%3Fsource%3D172ae18b&refer=http%3A%2F%2Fpic4.zhimg.com&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=auto?sec=1650554775&t=05a9ab0c62e63d57ea7c049f9c912084"><br><img src="https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fi0.hdslb.com%2Fbfs%2Farchive%2F6dbfc6415706f299f6163943c71fec91f027cd1d.jpg&refer=http%3A%2F%2Fi0.hdslb.com&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=auto?sec=1650554811&t=6209234606d5189b2a4ba79240f5af4e"></p><p><img src="/yolo/u=3807873174,2984490567&fm=253&fmt=auto&app=138&f=PNG.png"></p><p>其实，yolov4刚出来的时候，大家还对名字争议了一番。毕竟原作者已经弃坑，新作看上去又是一个trick集合。马上yolov5出现之后，似乎对v4的质疑就消失了，因为v5这个名字起的似乎更离谱了。这里就不讨论名字争议了。</p><p><strong>原理</strong><br>作为一阶段end2end的检测算法代表作，我们简单回顾一下Yolo从V1到V5的主要改进措施，当然backbone也一直在进化着。</p><p>v1: 划分网格负责检测，confidence loss<br>v2: 加入k-means的anchor，两阶段训练，全卷积网络<br>v3: FPN多尺度检测<br>v4: spp，mish激活函数，数据增强mosaic\mixup，giou损失函数<br>v5: 对模型大小灵活控制，hardswish激活函数，数据增强<br>v1&#x2F;v2因为对小目标检测效果不佳，在v3中加入了多尺度检测。v3大概可以比作iphone界的iphone6，是前后代广受好评的大成者，至今仍活跃在一线，“等等党永远不亏”。Yolov4把整个流程中可能进行的优化都梳理和尝试了了一遍，并找到了各个排列组合中的最好效果。v5可以灵活的控制从10+M到200+M的模型，其小模型非常惊艳。<br>v3到v5的整体网络图相差不多，可以清晰的看到：模型从三个不同尺度分别对大小不同的物体有重点的进行检测。<br><img src="/yolo/u=4109089220,341220966&fm=253&fmt=auto&app=138&f=PNG.png"></p><p><strong>细节：数据增强和预处理</strong><br>由于模型需要图片尺寸相同，因此可以采用resize、padding resize和letterbox的方法。letterbox训练的时候不用，只是在推理时候用。数据增强(data augmentation)是提升模型泛化能力的重要手段。</p><h3 id="传统数据增强方式有随机翻转、旋转、裁剪、变形缩放、添加噪声、颜色扰动等等。"><a href="#传统数据增强方式有随机翻转、旋转、裁剪、变形缩放、添加噪声、颜色扰动等等。" class="headerlink" title="传统数据增强方式有随机翻转、旋转、裁剪、变形缩放、添加噪声、颜色扰动等等。"></a><em>传统数据增强方式有随机翻转、旋转、裁剪、变形缩放、添加噪声、颜色扰动等等。</em></h3><p><img src="https://img-blog.csdnimg.cn/8692b5243cf84fbebd2733111803fd9e.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5paH54Gr5Yaw57OW55qE56GF5Z-65bel5Z2K,size_14,color_FFFFFF,t_70,g_se,x_16"></p><p><img src="https://img-blog.csdnimg.cn/b9dd35be99c74842bf736fc4d4829c59.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5paH54Gr5Yaw57OW55qE56GF5Z-65bel5Z2K,size_15,color_FFFFFF,t_70,g_se,x_16"></p><p> Random Erasing Data Augmentation（随机擦除数据增强）</p><p><img src="https://img-blog.csdnimg.cn/img_convert/a3545bae1db9486cb49dd10e9f7335ce.png"></p><p><img src="https://img-blog.csdnimg.cn/img_convert/8be8d885ed38a171d5c3022c860fdadb.png"></p><p>随机的删除图片中部分区域，增强模型的泛化能力。</p><p>RandAugment（随机增强）<br>随机增强的意思是提前列举出几种数据增强的方式，如下图所示：</p><p><img src="https://img-blog.csdnimg.cn/img_convert/391be61c63a3f253c56e8cf6d4808ce3.png"></p><p>混合数据增强Mixup、Cutout、CutMix </p><p><img src="https://img-blog.csdnimg.cn/img_convert/38061f0535871d1806a024be1a756c8f.png"></p><p>Mixup是将两张图像以一定的概率凑到了一起，例如图中猫和狗各占一半，会显示一半像猫一半像狗；</p><p>Cutout是只一张图，但是随机选择一块区域进行丢弃；</p><p>CutMix则整合两种方法，仍然是两种图像各自占一定概率，然后丢弃一块区域的像素，用其中一张图像进行填充。</p><p> <strong>Mosaic Data Augmentation（马赛克数据增强）</strong></p><p>那么问题来了——什么是马赛克数据增强呢？<br>该方法被应用到YOLO-V4当中扩充数据！</p><p>简言之马赛克数据增强就是把原来的四幅图组在一起——Mosaic借鉴了CutMix增强的方法，只不过本方法采用了4张图片，对其进行了随机裁剪、缩放、旋转等操作，合成1张图像，这种方式达到了如下的效果：</p><p><img src="https://img-blog.csdnimg.cn/img_convert/0d23637d4d18d3fd10d10fba72135e64.png"></p><p><img src="https://img-blog.csdnimg.cn/img_convert/c4deb0c356b72346bce0c7a14143f4de.png"></p><p><img src="https://img-blog.csdnimg.cn/img_convert/24e6bb7e9aa9ee21c539d9a2311d5a83.png"></p><p>基本原理</p><p><img src="https://img-blog.csdnimg.cn/e5e30c79d46841559c1f5afe84a9d9f2.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5paH54Gr5Yaw57OW55qE56GF5Z-65bel5Z2K,size_14,color_FFFFFF,t_70,g_se,x_16"></p><p><img src="https://img-blog.csdnimg.cn/217ac1df147b43fd9ea187e099882a05.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5paH54Gr5Yaw57OW55qE56GF5Z-65bel5Z2K,size_16,color_FFFFFF,t_70,g_se,x_16"></p><p>（1）扩充了数据集的数量</p><p>（2）增加了小样本的数量：把大样本随机放缩成了小样本，因此增加了小样本的数量。</p><p>（3）小样本更小：由于采用随机放缩，合并后，导致小样本的尺寸更小。</p><p> <strong>YOLO V5 - ultralytics代码解析</strong><br>使能马赛克数据增强<br>超参数：hyp[‘mosaic’]</p><p>load_mosaic(self, index)<br>def load_mosaic(self, index)</p><p>{undefined</p><pre><code>    # 该函数会4张图片，先进行随机增强，然后合并成一张图片。</code></pre><p>}</p><p>flip: 翻转<br><img src="https://pica.zhimg.com/v2-8ba96ab2aa5753d0ad851ed1fa6e0a2e_1440w.jpg?source=172ae18b"></p><p>perspective：仿射变换<br><img src="https://img-blog.csdnimg.cn/20210903154515647.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAbHp6enp6em0=,size_20,color_FFFFFF,t_70,g_se,x_16"></p><p>hsv augment：颜色变换<br><img src="https://img-blog.csdnimg.cn/03f0170479a94e39881de89e6d79e788.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3BlbnRpdW1DTQ==,size_16,color_FFFFFF,t_70"></p><p>当然后面可以试试albumentations包里的方法。</p><p>获取anchor：k-means<br>早就在机器学习中学过最基础的聚类算法k-means，没想到第一次用上是在目标检测里。果然没有免费的午餐啊，在合适的地方用上合适的算法就好。k-means的两个关键超参数是k的选择和距离的选择，所以在Yolo中的k等于n_scale（尺度，一般为3）* anchor_per_scale (每个尺度的anchor，一般也为3）。V5做了一点小小的改进：训练时自动获取。</p><p>匹配anchor和网格<br>虽然讨论的时候，经常看见的都是改了哪些fancy的模型，真正写程序时比较困难的反而是这种数据前处理的pipeline。yolo的关键思想通过标记物体的中心点落在哪个网格里，就由这个网格负责检测，就是通过设置目标来实现的。</p><p>1）计算这张图片所有ground truth与每一个anchor的IOU，从而得到对每一个ground truth匹配最好的anchor id。可以根据真实框和锚矿的IOU或者长宽比来选择，v4与v5中做的改进是根据宽高比来进行anchor的匹配。</p><p>2）对每一个ground truth循环。找到这个anchor对应的尺度，和这个anchor对应的哪一个anchor。<br>3）模型最终输出的是三个尺度，每个尺度下每个网格带下的三个anchor对应的box。所以将标记的box也要放置到对应的grid里，对应的是三个anchor里的哪一个。根据上面找到对应grid，对应anchor，将标记的box尺寸和class 信息设置到目标里，其他设置为0。<br>另外两个改进点，一个改进点是正样本增强。通过正样本的增强可以减轻正负样本的不均衡。另一个改进点是标注框对anchor的encoding(编码方式)。从v2中引入anchor以来，一直通过如下编码方式</p><p>为了解决网格的敏感性，v4的作者发现简单的编码方式就可以提高结果。</p><p>x &#x3D; (logistic(in) * 2 - 0.5 + grid_x) &#x2F; grid_width<br>y &#x3D; …<br>w &#x3D; pow( logistic(in)*2, 2) * anchor &#x2F; network_width<br>h &#x3D; …</p><p>模型主体</p><p>（来自dabai同学）</p><p>v5的模型通过类似EfficientNet的两个系数来控制模型的宽度和深度，在yaml文件的depth_multiple和width_multiple中调节。<br>Focus：是v5相对v4的改进。<br>CSPNet：跨阶段局部融合网络<br>PANet：之前FPN是把传统视觉任务的图像金字塔改进到了深度学习的特征金字塔，来自论文Path Aggregation Network for Instance Segmentation。</p><p>损失函数<br>Yolo的损失函数比较惊艳，通过损失函数的设定把目标检测任务转化为一个回归任务。</p><p>&#96;def build_targets(p, targets, model):</p><pre><code># `***Build targets for compute_loss(), input targets(image,class,x,y,w,h)***`</code></pre><p>​    &#96;&#96;det &#x3D; model.module.model[-1] if is_parallel(model) else model.model[-1]  # Detect() module<code>​   </code>na, nt &#x3D; det.na, targets.shape[0]  # number of anchors, targets<code>​   </code>tcls, tbox, indices, anch &#x3D; [], [], [], []<code>​   </code>gain &#x3D; torch.ones(7, device&#x3D;targets.device)  # normalized to gridspace gain<code>​   </code>ai &#x3D; torch.arange(na, device&#x3D;targets.device).float().view(na, 1).repeat(1, nt)  # same as .repeat_interleave(nt)<code>​   </code>#将targets复制3份，每份分配一个anchor编号，如0,1,2. 也就是每个anchor分配一份targets。<code> ​    targets = torch.cat((targets.repeat(na, 1, 1), ai[:, :, None]), 2)  # append anchor indices</code></p><p>重要的代码块在build_targets内。</p><pre><code>g = 0.5  # bias# 这里off表示了5个偏移，原点不动，往右、往下、往左、往上。# 其中坐标原点在图像的左上角，x轴往右（列），y轴往下（行）。off = torch.tensor([[0, 0],                    [1, 0], [0, 1], [-1, 0], [0, -1],  # j,k,l,m                    # [1, 1], [1, -1], [-1, 1], [-1, -1],  # jk,jm,lk,lm                    ], device=targets.device).float() * g  # offsetsfor i in range(det.nl):    #det.anchors在导入model的时候就除以了步长，因此此时anchor大小不是相对于原图，而是相对于对应特征层的尺寸    anchors = det.anchors[i]    gain[2:6] = torch.tensor(p[i].shape)[[3, 2, 3, 2]]  # xyxy gain    # Match targets to anchors    #这里主要是将gt的cx，cy，w,h换算到当前特征层对应的尺寸，以便和该层的anchor大小相对应    t = targets * gain    if nt:        # Matches        #这个部分是计算gt和anchor的匹配程度        #即w_gt/w_anchor  h_gt/h_anchor        r = t[:, :, 4:6] / anchors[:, None]  # wh ratio        #这里判断了r和1/r与model.hyp[&#39;anchor_t&#39;]的大小关系，即只有不大于这个数，也就是说gt与anchor的宽高差距不过大的时候，才认为匹配。代码中 model.hyp[&#39;anchor_t&#39;]=4        j = torch.max(r, 1. / r).max(2)[0] &lt; model.hyp[&#39;anchor_t&#39;]  # compare               # j = wh_iou(anchors, t[:, 4:6]) &gt; model.hyp[&#39;iou_t&#39;]  # iou(3,n)=wh_iou(anchors(3,2), gwh(n,2))        #将满足条件的targets筛选出来。                  t = t[j]  # filter                # Offsets        #这个部分就是扩充targets的数量，将比较targets附近的4个点，选取最近的2个点作为新targets中心，新targets的w、h使用与原targets一致，只是中心点坐标的不同。        gxy = t[:, 2:4]  # grid xy        gxi = gain[[2, 3]] - gxy  # inverse        j, k = ((gxy % 1. &lt; g) &amp; (gxy &gt; 1.)).T        l, m = ((gxi % 1. &lt; g) &amp; (gxi &gt; 1.)).T        j = torch.stack((torch.ones_like(j), j, k, l, m))        t = t.repeat((5, 1, 1))[j] #筛选后t的数量是原来t的3倍。        offsets = (torch.zeros_like(gxy)[None] + off[:, None])[j]    else:        t = targets[0]        offsets = 0    # Define    b, c = t[:, :2].long().T  # image, class    gxy = t[:, 2:4]  # grid xy    gwh = t[:, 4:6]  # grid wh    gij = (gxy - offsets) #自己加的代码，方便查看gij的分布。     plot_gxy(gxy=gij, scale_i=i, size=gain, flag=&#39;gij&#39;) #自己编的代码，用于查看gij的分布。    gij = (gxy - offsets).long() #将所有targets中心点坐标进行偏移。    gi, gj = gij.T  # grid xy indices    # Append    a = t[:, 6].long()  # anchor indices    indices.append((b, a, gj, gi))  # image, anchor, grid indices    tbox.append(torch.cat((gxy - gij, gwh), 1))  # box    anch.append(anchors[a])  # anchors    tcls.append(c)  # classreturn tcls, tbox, indices, anch`</code></pre><p>下图是20x20的特征图上的gij的分布示意图，从图中可以看出每个targets都扩充了2个临近的targets。关于为什么扩充，我还没理解，有知道的网友欢迎留言。另外，知乎网友Ancy贝贝的理解是：之前通过筛选，去掉了一些匹配不上anchor的gt，本来正样本就比负样本少很多，经过筛选，少得更多了，所以每个gt扩充2个出来，增加正样本比例。</p><p><img src="https://images2.freesion.com/444/75/75b593c73852ed76a1013bf5ef28b5c4.png"></p><pre><code># Regressionpxy = ps[:, :2].sigmoid() * 2. - 0.5pwh = (ps[:, 2:4].sigmoid() * 2) ** 2 * anchors[i]pbox = torch.cat((pxy, pwh), 1).to(device)  # predicted boxgiou = bbox_iou(pbox.T, tbox[i], x1y1x2y2=False, CIoU=True)  # giou(prediction, target)lbox += (1.0 - giou).mean()  # giou loss</code></pre><p>代码中的pxy对应bxy，ps[:, :2]对应txy。由此可知bxy的取值范围是[-0.5,1.5]。因此有可能偏移到临近的单元格内，但偏移不多，不知道作者是什么考虑的。</p><p><img src="https://images1.freesion.com/54/4c/4c332b4656f3ac2fb81f718225aebb46.png"></p><p>代码中的pwh对应bwh，anchors[i]对应Pwh。因此可知bwh的范围是[0,4]*Pwh。这和前面<br>j &#x3D; torch.max(r, 1. &#x2F; r).max(2)[0] &lt; model.hyp[‘anchor_t’] # model.hyp[‘anchor_t’]&#x3D;4 是一致的。</p><p>OBJECTNESS<br>tobj[b, a, gj, gi] &#x3D; (1.0 - model.gr) + model.gr * giou.detach().clamp(0).type(tobj.dtype) # giou ratio<br>此处 tobj[b, a, gj, gi]用giou（真实的是ciou）取代1，代表该点对应置信度。为什么要用giou来代替，我也没想明白，有知道的网友欢迎留言。</p><p>其余的部分比较好理解，在此不再赘述。</p><p>附：<br>plot_gxy的代码：</p><p><code>def plot_gxy(gxy, scale_i, size, flag):</code><br>    <code>s = int(size[2].cpu().numpy())</code><br>    <code>ax = plt.subplot(111)</code><br>    <code>ax.axis([0, s, 0, s])</code><br>    <code>lxx = np.arange(0, s + 1, 1)</code><br>    <code>lxx = np.repeat(lxx, s + 1, axis=0)</code><br>    <code>lxx = lxx.reshape(s + 1, s + 1)</code><br>    <code>lyy = np.arange(0, s + 1, 1)</code><br>    <code>lyy = np.repeat(lyy, s + 1, axis=0)</code><br>    <code>lyy = lyy.reshape(s + 1, s + 1)</code><br>    <code>lyy = lyy.T</code></p><pre><code>for i in range(len(lxx)):    plt.plot(lxx[i], lyy[i], color=&#39;k&#39;, linewidth=0.05, linestyle=&#39;-&#39;)    plt.plot(lyy[i], lxx[i], color=&#39;k&#39;, linewidth=0.05, linestyle=&#39;-&#39;)for i in range(len(gxy)):    x1, y1 = gxy.cpu().numpy().T    plt.scatter(x1, y1, s=0.02, color=&#39;k&#39;)ax = plt.gca()  # 获取到当前坐标轴信息ax.xaxis.set_ticks_position(&#39;top&#39;)  # 将X坐标轴移到上面ax.invert_yaxis()plt.savefig(&quot;gxy_&#123;&#125;_&#123;&#125;.png&quot;.format(scale_i, flag))plt.close()`</code></pre><p>第一部分为box的损失函数，可以用过smoth L1损失函数计算xywh，也可以通过iou、giou、ciou等。类似的giou、diou和ciou都是在交并比iou的基础上发展而来的优化方法，解决了iou在两个候选框完全没有交集时，损失为0不可优化的状况。第二部分为有无物体的损失：yolo独特的设置了一个有无物体的confidence，目标中有的自然设置为1。预测的值用sigmoid转化为（0，1）的概率，然后计算binary cross entropy。第三部分为分类损失部分，虽然是多分类，也采用binary corss entropy。<br>同时，为了平衡不同尺度的输出，v5中对以上三部分加了人工痕迹比较明显的系数。</p><p>后处理<br>极大值抑制(nms)。分三个尺度进行检测，且分配不同的anchor，那么同一个物体有可能被多个尺度、anchor所检测到。所以通过极大值抑制来从中选择最佳的候选框。也有soft-nms、matrix-nms等改进方式可以试一试。</p><p>训练<br>训练过程就比较平平无奇了，可以添加如下技巧：梯度累积、遗传算法、EMA滑动平均、算子融合。</p><p>展望<br>检测之外，之后还可以继续拓展和优化的方向：</p><p>tensorrt推理加速<br>deepsort物体追踪，可以稳定视频中检测框的抖动<br>针对小目标分割patch检测，例如卫星遥感图像的改进you only look twice<br>多尺度scalable yolov4</p><p>代码如下：<br>Yolov5-pytorch<br>Yolov5-tensorflow</p><p>date: 2022-3-22 20:22:25<br>tags: 标签<br>categories: 分类</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>机器学习，深度学习，神经网络，深度神经网络之间有何区别？</title>
      <link href="/2022/03/23/deeplearning/"/>
      <url>/2022/03/23/deeplearning/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>首先，有必要对神经网络、深度学习、机器学习的概念做个简单描述。<br>神经网络，该模型灵感来自动物的中枢神经系统，通常呈现为相互连接的“神经元°”，它可以对输入值通过反馈机制使得它们适应对应的输出。<br>深度学习是神经网络的进阶版，它的基本思路与神经网络类似，但往往比神经网络有着更复杂的结构以及优化算法，是神经网络的纵向延伸，常见的模型有 CNN , RNN , LSTM 等。<br>机器学习是一门多领域交又学科，渉及概率论°、统计学、逼近论、凸分析、算法复杂度理论等们学科。专们研究计算机怎样模拟或实现人类的学习行为，以获取新的识或技能，重新组织已有的<br>知识结构使之不断改善自身的性能。<br>接着，阐述神经网络、深度学习、机器学习的区别与联系。下面的图片很子地描述了这三者之间的关系：</p><p><img src="https://pic3.zhimg.com/80/v2-6d2584526f458fa512543d04ffb66c42_720w.jpg?source=1940ef5c"></p><p>在这之中，机器学习的涉及范围是最广的，神经网络次之，深度学习最小。<br>机器学习包含了神经网络，神经网络中又包含了深度学习。<br>机器学习专门研究计算机怎样模拟或实现人类的学习行为，而神经网络只是借助了动物的神经系统，只是用计算机实现人类行为的一种手段，因此，神经网络包含于机器学习。<br>深度学习是神经网络的进阶版，只是在模型结构及优化算法等方面有不同，因此，深度学习应包含<br>于神经网络内。</p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
